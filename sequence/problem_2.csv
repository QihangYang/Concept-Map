Problem Set ID,Problem_text,Related_Concept
lr_guren_ch1_p1,"Let us be given p 2: 3 independent normally distributed random variables
Yi rv N (J.Li, I ), i =1, . . . ,p. Let Y = (Y1, . .. ,Yp)' and J.t =(J.L1, . . . , J.Lp )' . Let
the loss of a decision rule 8(Y) for estimating the parameter vector J.t be
L(J.t,8(Y)) = (8(Y ) - J.t)'(8(Y) - J.t) =I:f=1(8(Y) i - J.L i) 2.
(a) Determine th e risk of the estimator 81(Y) = Y.
(b) Show that the risk of the estimator
82(Y) = (1 - (p; 2))y, Z =Y'y ,
is given as
p(J.t,( 2) = p - (p - 2)2E(ljZ) .
[Hint: Use the identity E[(Y - J.t)'YjZ] = (p - 2) E(ljZ).]
(c) Compare both risks. What can be said?","probability distribution, Hypothesis Testing"
lr_guren_ch1_p2,". Consider the sit uation from Problem 1.1 for th e case p = 10. Let
u = (1.9665, .5456, 2.2983, .6172, 1.9213, -2.1017, - .2727, 1.3510, 2.6029, 1.4598)'
be a given observation of Y .
(a) Compu te the two possible decisions 81(y) and 82(y) (estimat es of J.t)
corresponding to t he decision rules 81 (Y ) and 82 (Y) from Problem 1.1.
(b) Compute the observed losses of 81 (y) and 82(y) when J.L i = 1, i = 1, . .. , 10, are the true parameters.
(c) The i-th element of 81 (y ) and 82 (y ) can respectively be seen as an estimate of J.li . Check whether each element of 82 (y ) has a smaller observed
loss th an the corresponding element of 81 (y) when J.Li = 1 is the true
parameter.","probability distribution, Hypothesis Testing"
lr_guren_ch1_p3,"Consider the sit uation from Example 1.3 for an arbitrary sample size n .
Assume that the set contains the three rules
an d JO•5 (Y ) = 0.5.
(a) Determine the risks of J2 and JO.5 •
(b) Is JO.5(Y) admissible in = {J1(Y),J2(Y),JO.5 (Y)}? (P lot the three
risk functions for n = 10.) Is JO.5 (Y ) a reasonable decision rule?",Hypothesis Testing
lr_guren_ch1_p4,"Let Yi ,...., N(f-L, 1), i = 1, ... , n, be n independent distributed random
variables. Show that Y = L:~I Yi is admissible for f-L in the class
= {J(Y) : J(Y) =cY,c E IR}
with respect to the loss function L(f-L,J(Y)) = (J(Y) - f-L) 2.",Hypothesis Testing
lr_guren_ch1_p5,"Consider the situation from P roblem 1.2 and compute the observed losses
of the estimates J1 (y) and J2 (y) when f-L i = 1, i = 1, .. . , 10 are the true
parameters. Use the weighted squared error loss function
L (9,J(Y)) =(J(Y) - 9)'W(J(Y ) - 9) ,
where
W = diag(O.OI, 0.46, 0.01, 0.46, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01) .",Hypothesis Testing
lr_guren_ch1_p6,"Let W be a p x p symmetric nonnegati ve
definite matrix . Let >'max denote the largest eigenvalue of W .
(a) Show that all eigenvalues of F = (1/ >'max) W lie in the closed interval
[0,1].
(b) Show that F - F 2 is symmetric nonnegative definite.","Linear Algebra, Hypothesis Testing"
lr_guren_ch1_p7,"Explain why the condit ion
is necessary for the proof of Theorem 1.1. Explain in addit ion, why it is actually enough to demand the condition to be true only for symmet ric matrices
A with all eigenvalues in [0,1] .","Linear Algebra, Hypothesis Testing"
lr_guren_ch1_p8,"Is the condition
where A is an ar bitrary p x p matrix, satisfied for the following sets of decision
rules?
1.3 Problems 31
(i) = {8(Y) : 8(Y) = FY + f} , where F is a p x n matrix and f is a
p x 1 vector;
(ii) = {8(Y) : E[8(Y)] = O}.","Linear Algebra, Hypothesis Testing"
lr_guren_ch1_p9,"Let (e,L,Y ) be an estimation problem with a weighted squared error
loss function L and symmetric nonnegative definite weight matrix W. Let
W * denote an arbitra ry but fixed symmet ric positive definite matrix.
(a) Show that if 8o(Y) is admissible for 0 for the case W = W *, then
28 (Y is admissible for 20 for the case W = [ p. Note that
/ 2 is the (uniquely determined) symmet ric positive matrix satisfying
W 1/ 2W1/ 2 = W * * *.
(b) Use part (a) and Theorem 1.1 to derive the following claim: If 8o(Y) is
admissible for 0 for t he case W = W* , then 8o(Y) is admissible for 0
for th e case W = [p.","Estimation, Hypothesis Testing"
lr_guren_ch1_p10,"Show that
MSE (0 ,8 ) = Cov(8(Y)) + bias(8(Y)) bias(8(Y))' ,
where bias(8(Y)) = E(8(Y)) - O",Estimation 
lr_guren_ch1_p11,"Let 81 (Y) and 82 (Y ) be two decision rules such that the matri x-valued
inequality MSE(O, 8d MSE(0 , 82) is satisfied for all 0 E e. Show that
if for some given 0 E e the difference MSE( O, 82 ) - MSE( 0,8d is positive
definit e, then for this 0 the following two statements hold true:
(i) E [(81 (Y)i - 8i)2] < E [(82 (Y)i - 8i)2] for every i E {I , . . . ,p }.
(ii) E [(81 (Y) - 0)'W(81 (Y) - 0)] < E [(82(Y) - 0)'W(82(Y) - 0)] for every p x p symmetric positive definite matrix W .","Hypothesis Testing, Diagnostic"
lr_guren_ch1_p12,"Consider th e situation from Problem 1.1. Check whet her 8(Y ) = 0 is
admissible for JL in
= {8(Y) : 8(Y) = FY + f}
with respect to th e unweight ed squared error loss. Here F denotes a p x p
matrix and f denotes a p x 1 vector.","probability distribution, Hypothesis Testing"
lr_guren_ch2_p1,"Show that under the linear regression model with
assumptions (i) to (iv), the identity
is satisfied for any vector 13 * E IRP.","Linear Algebra, Multiple Linear Regression"
lr_guren_ch2_p2,"Consider the linear regression model
Yi =131 +132Xi +e.,
with n =8 observations
i = 1, ... ,n ,
i 1 2 3 4 5 6 7 8
Yi 1.764 1.672 1.796 1.548 -0.719 -0.635 0.484 3.133
Xi 15.00 15.89 23.39 6.07 7.63 8.57 14.63 18.79
(a) Plot the pairs (Xi, Yi), i =1, .. . ,8 .
2.6 Problems 81
(b) Determine the estimates 731 and 732 for 131and 132 , using the least squares
principle, and insert the regression line fj =731+732X into the picture.
(c) The true parameters are 131= 0 and 132 = 0.1. Insert the line y = 131 +132 X
into the picture.","Simple Linear Regression, Estimation"
lr_guren_ch2_p3,"Consider the linear regression model with assumption (i) to (iv) and
I n E C(X), where I n denotes t he n x 1 vector whose elements are all equal
to one. (Th e condition I n E C(X) is always satisfied in a linear regression
model with intercept .)
(a) Show that the condition I n E C(X) is equivalent to PIn = In, where
P = X(X'X)-1 X' .
(b) Show that the identities 2::7=1f}; = 2:: ~1 Yi and 2::7=1Ei =0 are sat isfied.
(Here f}; , Yi and Ei are th e elements of the vectors y = x13 , y and
g =y - y respectively.)","Linear Algebra, Multiple Linear Regression"
lr_guren_ch2_p4,"Consider the linear regression model with assumptions (i) to (iv) and
In E C(X). Under such a model often the corrected coefficie nt of determination
R2 = y'Py - ~(I~y)
c y'y _ ~(I ~y )2
is applied instead of th e usual coefficient of determination R2 . Show that
can be written as
R2 _ y'Cy c - y'Cy ,
where C =I n - ~I nI~ (centering matrix).","Linear Algebra, Multiple Linear Regression, Diagnostic"
lr_guren_ch2_p5,"Consider the sit uation from Problem 2.2.
(a) Compute the coefficient of determination R2 •
(b) Compute the unweighted squared error loss of the ordinary least squares
estimator 13 ,when j3 =(0,0.1)' is th e true parameter vector.
(c) Compute the unweighted squared error loss of the estimator 7!J = c13 , where
(n - p)R2
c = , 1 + (n - p - 1)R2
when j3 = (0,0.1)' is the true parameter. Compare the losses from (b)
and (c).
(d) Insert the line obt ained from the estimates 7!J into the picture from Problem 2.2. What can be said about th e estimation?","Multiple Linear Regression, Diagnostic, Estimation"
lr_guren_ch2_p7,". Show that und er the linear regr ession mod el with assumptions (i) to
(iv), the identity
&2 = _l_y' M y , M=In-X(X'X)- lX' , n- p
is true.","Linear Algebra, Multiple Linear Regression"
lr_guren_ch2_p8,"Consider th e linear regression model with
(a) Determine th e least squares estimator 13 of {3 as a function of Yi, i
1, .. . , 4.
(b) Find a further unbi ased estimator for {3 .
(c) Consider the estimator
13 = (Yl + Y2 + YIY3 + YIY4 - Y2Y3 - Y2Y4)
2 Y3 + Y4 + YIY3 + YIY4 - Y2Y3 - Y2Y4
for {3. Explain why 13 is a non-linear estimator and check if 13 is unbi ased
for {3.","Multiple Linear Regression, Estimation"
lr_guren_ch2_p10,"Show that und er th e linear regression model with assumptions (i) to
(iv) , an estimator Ay + a is unbiased for {3 if and only if AX = I p and
a = O","Multiple Linear Regression, Estimation"
lr_guren_ch2_p11,"Consider the linear regression model from Problem 2.8 and th e est imator
73 = (Yl + Y2 - Y3 - Y4)
4 Y3 + Y4 - Yl - Y2
for {3. Show that 13 has a strictly smaller risk than 13 with respect to th e
real-valued unweighted squared error loss, when {3 = 0 is th e true parameter
value. Is this a contradiction to th e Gauss-Markov Theorem?","Multiple Linear Regression, Estimation"
lr_guren_ch2_p12,"Consider the linear regression mod el from Problem 2.8 and the two
estimators
73 - ( 2Yl - Y3 + Y4 ) 1 - 2 -Yl + Y2 + Y3 + Y4 '
for {3 .
2.6 Problems 83
(a) Show that 73 1 is uniformly better than 732 with respect to the real-valued
unweighted squared error loss.
(b) Show that a similar statement does not hold with respect to the matrixvalued squared error loss.","Multiple Linear Regression, Estimation"
lr_guren_ch2_p13,". Let T be a symmetric positive definite matri x and let
fr({3 . ) = (y - X{3. )'T(y - X{3. ) .
(a) Show th at
minfr({3. ) = fr(fJT) fl.
holds true for fJT = (X'TX)-lX'Ty (weight ed least squares estimator).
[Hint: T =T 1
/
2T1
/
2 .]
(b) Explain why und er a linear regression model with assumptions (i) to(iv) ,
th e least squares estimator fJ is uniformly not worse than a weighted least
squares estimator fJT (with respect to the matrix-valued squared error
loss or th e real-valued unweighted squared erro r loss).","Linear Algebra, Multiple Linear Regression, Estimation"
lr_guren_ch2_p14,"Consider th e linear regression model with assumptions (i) to (iv). Show
that for Ay E U({3) and p({3,(3 ) = E[(73 - {3)'(73 - {3)] the identity
p({3 ,Ay) = p({3 ,fJ)
is satisfied if and only if .it =(X 'X) -l X'","Linear Algebra, Multiple Linear Regression"
lr_guren_ch2_p15,"Show that under the linear regression model the est imator fJ k
(X 'X + klp )-lX'y , where k is fixed, satisfies the two conditions
(i) E(fJk) = {3 - k(X'X + klp )-l{3 ;
(ii) COV(fJ k) = a 2X'X(X'X + kIp )- 2.","Multiple Linear Regression, Estimation"
lr_guren_ch2_p16,"Show that for a fixed k th e estimator fJk = (X 'X + klp)-lX'y is
unbiased for {3 if and only if k = O.","Multiple Linear Regression, Estimation"
lr_guren_ch2_p17,"Recall the situation from Example 2.6 and consider th e individual element s (fJ)i and (fJ1 /5)i, i = 1,2, of fJ and fJ1 /5 as estimato rs for (31 and (32.
Compute
(i) E[(fJ 1/5)i], i = 1,2 ;
(ii) Var[ (fJ)d and Var[(fJ1/5)i], i = 1,2.","Multiple Linear Regression, Estimation"
lr_guren_ch2_p18,"Consider th e linear regression model with assumptions (i) to (iv).
(a) Determine p({3 , fJ ) = Lf=l Var(fJi) as a function of the eigenvalues
of X' X . What effect does a small eigenvalue of X' X on the sum
Lf= l Var(fJi) has?
84 2 The Linear Regression Model
(b) Determine L:f=l Var((.8 k)i) as a function of the eigenvalues of XiX.
(c) Show that L:f=l Var((.8 k)i) L:f=l Var(.8 i) is always satisfied.
[Hint: spectral decomposition of X'X .J","Linear Algebra, Multiple Linear Regression"
lr_guren_ch2_p19,"Consider the linear regression model with n = 10 observations and
p = 4 independent variables
1 2 3 4 5 6 7 8 9 10
Xi,l 1.9 1.8 1.8 1.8 2.0 2.1 2.1 2.2 2.3 2.3
Xi ,2 2.2 2.2 2.4 2.4 2.5 2.6 2.6 2.6 2.8 2.7
Xi~ 1.9 2.0 2.1 2.2 2.3 2.4 2.6 2.6 2.8 2.8
Xi,4 3.73.83.63.83.83.73.84.03.73.8
(percentage of research and development expenses on the gross national product of four countries in ten different years).
(a) Compute the eigenvalues of XiX.
(b) ComputeL:f=l Var(.8 i).
(c) Compute v := L:f=l Var((.80.02)i)' What is the amount of reduction of v
compared to L:f=l Var(.8i)?","Linear Algebra, Multiple Linear Regression"
lr_guren_ch2_p20,"Show that if in the linear regression model the matrix X is not of full
column rank, i.e. assumption (ii) is violated, then U(f3) =0.","Linear Algebra, Multiple Linear Regression"
lr_guren_ch2_p21,"Bias of the positive square root of the least squares variance estimator:
(a) Let X '"" X7k) ' Show that
where r(·) is the Gamma function.
(b) Use Theorem 2.7 to show that under the linear regression model with
assumptions (i) to (iv) and (v)
_(n_ p )-1/2r(~) E(a) - 2 r(y) a ,
where ais the positive square root of the least squares variance estimator ~2
a .
(c) Show that for
( ) _ r(r + !)
c r - ,jTr(r) , r > 0 ,
it follows that c(r) < 1 for r < 00 and limr-too c(r ) = 1.
(d) Conclude that under the assumptions of (b) it follows that E(a) < a and
limn-too E(a) = a.","Estimation, Probability Distribution"
lr_guren_ch2_p22,"To see that unbiasedness and consistency must not come along toget her , consider the following:
(a) Let Y1 , . .. , Yn , ... be a series ofrandom variables such that P(Yn =e) =
1 - In and P(Yn =n) = In for any n, where eis not an integer. Show
that Yn converges in probability to ebut E(Yn ) :j:. e.
(b) Let Y1 , . . . , Yn , . .. be a series of random variables such that P(Yn = 2e)= = P(Yn = 0) for any n, where e > 1. Show that E(Yn ) = e, but Yn
does not converge in probability to e.","Estimation, Probability Distribution"
lr_guren_ch2_p23,"Consider a linear regression model with n = 9, p = 3, and normally
distributed errors, i.e, model assumptions (i) to (vi) are satisfied. The matrix
X is given as
(
1 1 1 0 0 0 0 0 0)
X' = 0 0 0 1 1 1 0 0 0 .
000000111
(a) Show that the ordinary least squares estimator :0 for {) is given by
1 (Yl + Y2 + Y3) {) = v'3 Y4 +Y5 +Y6
Y7 + Ys + Y9
in the reparameterized model.
(b) Show that the Stein estimator :Os for {) is given by
in the reparameterized model.","Multiple Linear Regression, Estimation"
lr_guren_ch2_p25,"Consider the linear regression model from Problem 2.23 and a realization
y = (-.8946, -.8882, - .0525, -.5289, .6568, .0842, 1.072, -.0459, .4267)' .
(a) Compute the estimates :0 and :Os for {) in the reparameterized model.
(b) Compute the estimates 13 and 13s for 13 in the original model.
(c) Compute the losses of the estimates :0 , :Os for {) and the losses of the
estimates 13, 13s for 13, each with respect to the unweighted squared error
loss. The true parameter in the original model is 13 = (0,0.1,0.2)'.","Multiple Linear Regression, Estimation"
lr_guren_ch2_p26,"Consider the reparameterized regression model. If u 2 is known, th en
( (p - 2)U2 ) fJs = 1 - ,~ fJ
fJfJ
can be used as an estimator for fJ (Stein estimation for known ( 2 ) . Show that
where A=fJ'fJ/2u2 .","Multiple Linear Regression, Estimation"
lr_guren_ch2_p29,"Consider th e linear regression model with assumptions (i) to (iv) and
(vi), where R2 > 0. Let
p- 2 y'My
/= 1 - n-p+2j!lx'x/3 '
i.e, /3s = //3. Show that th e equivalence
° p-2 R2 /~1 ¢:} --<
n
is satisfied.","Multiple Linear Regression, Estimation"
lr_guren_ch2_p30,"Consider the model from Problem 2.23 with a sample value
y = (.4900, .3102,-.5425, -.2422, - .6419, .1904, .9029, .2579, -.9080)' .
~ ~ ~ +
(a) Compute the estimates fJ, fJs, and fJs for fJ in the reparameterized model. ~ +
(b) Compute th e losses of the estimates fJ, fJs and fJs with respect to the
unweight ed squared error loss. The true parameter in th e original model
is {3 = (0,0.1,0.2)'.","Multiple Linear Regression, Estimation"
lr_guren_ch3_p1,"Show th at th e restricted least squares estimator f3R satisfies the identity
Rf3R = r .","Multiple Linear Regression, Estimation"
lr_guren_ch3_p2,Show that under th e linear regression model with assumptions (i) to (iv) is satisfied for every vector /3 . E lRP wit h R/3. = r.,"Linear Algebra, Multiple Linear Regression"
lr_guren_ch3_p3,"Show that under the linear regression model with assumptions (i) to (iv)
and (viii) an estimator Ay +a is unbiased for 13, if and only if the conditions
AX(Ip - R'(RR')-1 R) + R'(RR')-1 R = I p
and
a =(Ip - AX)R'(RR')-lr
are met. [Hint: The set of all 13 such that Rf3 =r coincides with the set of
all 13 satisfying 13 = R'(RR')-lr + (I p - R'(RR')-1 R)z, z E lRP.]","Linear Algebra, Multiple Linear Regression"
lr_guren_ch3_p4,"Consider a linear regression model with
and linear restrictions /31 + /32 = o.
(a) Derive the restricted least squares estimator /3R of 13 as a function of Yi,
i =1, . . . ,4.
(b) Check whether
= (2Y1 + Y2 + Y3 + Y4)
Yl + Y2 + Y3 + 2Y4
is unbiased for 13 when the given linear restrictions are satisfied (model
assumptions (i) to (iv) and (viii)) .","Multiple Linear Regression, Estimation"
lr_guren_ch3_p5,"Determine MSE(f3,/3R) and p(f3,/3R) for /3R from Problem 3.4. Suppose
that model assumptions (i) to (iv) but not necessarily (viii) are satisfied.","Multiple Linear Regression, Estimation"
lr_guren_ch3_p6,"Explain why under the linear regression model with assumptions (i) to
(iv) none of the two estimators /3 and /3R can be uniformly better than the
other with respect to MSE(f3, ~).","Multiple Linear Regression, Estimation"
lr_guren_ch3_p7,"Consider the regression model from Problem 3.4, where it is only known
that (/31 + /32)2 :s; (12 . Is it nonetheless reasonable to use the restricted least
squares estimator with corresponding restrictions /31 + /32 = O?","Multiple Linear Regression, Estimation"
lr_guren_ch3_p8,"Plot p(f3,/3R) and p(f3,/3) from Problem 3.4 as functions of >. = 8'(RS-1R')-18/2(12 with (12 =1.","Multiple Linear Regression, Estimation"
lr_guren_ch3_p9,"Consider a linear regression model with n = 9, p = 3 and normally
distributed errors, so that assumptions (i) to (vi) are satisfied. The model
matrix is given as
(
1 1 1 0 0 0 0 0 0)
X' = 0 0 0 1 1 1 0 0 0 .
000000111
Moreover, the observed vector y is given as
y = (-.8946, -.8882, - .0525, -.5289, .6568, .0842, 1.072, -.0459, .4267)'.
206 3 Alternative Estimators
(a) Compute the level a =0.05 pretest estimator j3p for {3 under the imposed
restriction /31 + /32+ /33 =0..
(b) Compute the observed loss of j3p when {3 = (0,0.1,0.2)' is the true parameter and the usual unweighted squared error loss function is used.
Compare the result with the loss of 13 (Problem 2.25 (c)).","Multiple Linear Regression, Estimation"
lr_guren_ch3_p10,"Under the situation of Problem 3.9 compute
A= 8'(RS- 1R')-18/2a2
when ({3 , a2 ) = ((0~1) , 0.5) are the true parameters.","Linear Algebra, Multiple Linear Regression"
lr_guren_ch3_p11,"Compute in the situation of Example 3.7 the risks p({3,13) and p({3,13(1»)
when ({3 , a2 ) = (G) , 1) is the true parameter. Compare the obtained values
with the observed average losses given in Example 3.7.","Multiple Linear Regression, Estimation"
lr_guren_ch3_p13,"Show that if {3 E C(U2), i.e. {3
MSE({3,O) :SL MSE({3,{3 ).
U 2b for some vector b, then","Multiple Linear Regression, Estimation"
lr_guren_ch3_p14,"Consider the linear regression model with assumptions (i) to (iv), where
X' X = Alp for some number A > O. Show that for any fixed k > 0 the
inequality p({3, 13k) < p({3,13) holds true for all parameters ({3 , a2) satisfying
the condition
{3'{3 2A + k
~<p~.
Here p(.) stands for the unweighted squared error risk.","Multiple Linear Regression, Estimation"
lr_guren_ch3_p15,"Show that for an n x p matrix X of full column rank the identity
X'X(X'X + kIp)-1 = (X'X + kIp)-1X'X
is satisfied for any k E JR","Linear Algebra, Multiple Linear Regression"
lr_guren_ch3_p16,"Show that for an n x p matrix X of full column rank the identity
X' X(X' X + kIp)-1 = (k(X'X)-1 + I p) -1
is satisfied for any k 2 O","Linear Algebra, Multiple Linear Regression"
lr_guren_ch3_p17,"The set of all vectors {{3 E JRP : {3' H{3 :s 1'2 } , where H is an p x p
symmetric positive definite matrix and 1'2 > 0, is called a p-dimensional
ellipsoid . The p semi-axes have length
r/ JIii, i=l, ... ,p,
where PI, .. . , lip are the p eigenvalues of the matrix H.
3.10 Problems 207
(a) Under the linear regression model with assumptions (i) to (iv), let
(
2 )-1 H = kIp + (X 'X)-1 and r 2 = a2 . Show that the semi-axes of
the corres ponding ellipsoid are given as
i = 1, . . . ,p,
where )\1 , . . . , Ap are the eigenvalues of X'X. [Hint : Use the spectral
decomp osition X'X = U AU' with UU' = U'U = I p and A =
diag(A1, . . . , Ap ) .]
(b) Compute the length of t he semi-axes of th e ellipsoid from part (a) for
k = 1/5, a2 = 1, and X as in Example 2.6. Compare th e results with
Fig. 3.14.","Linear Algebra, Multiple Linear Regression"
lr_guren_ch3_p18,"The set of all vector 13 for which th e inequality p(j3, 13 k) p(j3,(3) hold
true can be written as {j3 E IRP : 13' Hj3 a 2 } .
(a) Determine the matrix H .
(b) Compute the length of the semi-axes of the corresponding ellipsoid for
k = 1/5 and a2 = 1, and X as in Example 2.6. Compare the results with
Fig. 3.15.","Linear Algebra, Multiple Linear Regression"
lr_guren_ch3_p19,"Let X = (In , X . ) be an n x p matrix of full column rank. Let Z =
(I n, Z . ) where Z . = CX.D- 1 with C = I n - ~l n1 and D - 1 being a
(p - 1) x (p - 1) diagonal matrix with positive main diagonal elements .
(a) Show that C(X) = C(Z). [Hint : The condition a E C(X) is satisfied
if and only if there exists some number 0: and some vector u such that
a = 1no:+X.u. Find a number 'Y and a vector v such that a = 1n'Y+Z.v.
Thi s shows that C(X) C(Z). It then remains to demonstrate the reverse
inclusion.]
(b) Show that the least squares variance estimator for a2 is the same in the
two models y = Xj3 + e and y = Z6 + e , where , e ,..., (0, a2 I n)' [Hint :
The equality of two column spaces is equivalent to the equality of the
ort hogonal projectors onto these column spaces.]","Linear Algebra, Multiple Linear Regression"
lr_guren_ch3_p20,"Consider the situation from Problem 3.19.
(a) Show that
rk(Z) = rk (In - C ,CX. ) = rk(In - C ) + rk(CX. )
holds true . [Hint: For suitable matri ces A and B the formula rk(A , B ) =
rk(A) + rk(B) - dim[C(A) n C(B)] is valid.]
(b) Show that
rk(C X.) =rk(X.) .
[Hint: For suitable matrices A and B the formula rk(AB) = rk (B) -
dim [N(A ) n C(B)] holds true. If A is an orthogonal projector, then
N(A) = C(I - A ).]
208 3 Alt ernative Estimators
(c) Show that Z is of full column rank.","Linear Algebra, Multiple Linear Regression"
lr_guren_ch3_p21,"Consider the linear regression model with n =9 observations given as
1 23 4 5 6 7 89
Yi 41.9 45 49.250.652.655.1 56.2 57.3 57.8
Xi ,I 12.4 16.9 18.4 19.4 20.1 19.619.821.121.7
Xi ,2 28.2 32.2 37 37 38.6 40.7 41.5 42.9 45.3
(Here Y is th e total of consumption in the USA, X l is the total income from
profits and X2 is th e total income from wages. The variables have been observed in the years 1921 to 1929.)
Assume a linear regression mod el with int ercept, st andardize the nonconstant independ ent variables, and compute th e ordinary least squares and
ridge estimates.","Multiple Linear Regression, Estimation"
lr_guren_ch3_p22,"Consider th e estimator
see [64].
(a) Show that 11,8(d)1I 2 11.811 2 is sat isfied.
(b) Show that ,8(0) =.8k=I and ,8 (1) =.8.
(As follows from (b), th e est imator ,8(d) can be seen as a weighte d average between the ordinary least squares and the ridge est imator with ridge
parameter k =1.)","Multiple Linear Regression, Estimation"
lr_guren_ch3_p23,"Consider th e linear regression model described by y = InJ.l+ e, e '""
(0 , 0'2 In ), where In st ands for the n x 1 vector whose every element is 1.
Show th e following statements :
n
(a) The ridge estimator for J.l is given as /ik = --kYo n +
(b) Th . kknif d rid . J:'"" xJ (n - l)k e jac rnre n ge estimator lor J.lIS given as J.lk =J.lk+ n(n _ 1 + k) J.lk ·
k
(c) The almost unbiased ridge estimator is given as /i~ = /ik + --kl1k.","Ridge Regression, Estimation"
lr_guren_ch3_p25,"Consider th e situation from Example 3.20, where the true parameters
are ({3, 0'2) = (G) , 1).
(a) Compute p({3 ,.8(7/3)) and p({3 , fJ ).
(X'X + K )-lX'y is
3.10 Problems 209
(b) Compute A = MSE(,8, ,8) - MSE(,8, ,8(7/ 3)) . Is the matrix A nonn egative definite?",Estimation
lr_guren_ch3_p26,"Show that ,8U1M) can be written as
~, , 2
;q(~ ) _ ,8 X Y ;q _ (n - p)R ;q
!J OM - , !J - ) 2 !J","Linear Algebra, Estimation"
lr_guren_ch3_p30,"Consider the linear regression model with assumptions (i) to (iv) and
0 < R2 < 1. Show that ,8'X'X ,8 - c~€ is positi ve if and only if c < R2/(1-
R2 ) .","Linear Algebra, Estimation"
lr_guren_ch3_p31,"Find a matrix X . and a vector Y . such that the general ridge estimator
,8K = (X 'X + K )- l X'y can be written in the form
,8K = (X: X.)-l X :y • .","Ridge Regression, Estimation"
lr_guren_ch3_p32,"Show that the general rid ge estimator ,8K
unbiased for ,8 if and only if K = O.","Ridge Regression, Estimation"
lr_guren_ch3_p33,"Show that the Marquardt estimator ,8 , c > 0, IS of the form of a
general ridge estimator with K = ((1 - c)/c) 2A2U~,","Ridge Regression, Estimation"
lr_guren_ch3_p34,"Show that the almost unbiased ridge est imator ,8k is of the form of a
general ridge est imator with K =k2(X' X + 2kIp )- 1.","Ridge Regression, Estimation"
lr_guren_ch3_p35,"Show that the iteration estimator ,80 m is of the form of a general
ridge estimator with K = X'X(I p - (I p --.: ilX 'x)m+l)-l - X'X. [Hint:
K = U (Ar- 1 - A)U', where X'X = U AU' an d r is matrix from Theorem
3.15.]","Ridge Regression, Estimation"
lr_guren_ch3_p36,"Sommers [106] considers the est imator
~m
Show th at (3k is of the form of a general ridge estimator with K
k[(X' x )m-l]-l.","Ridge Regression, Estimation"
lr_guren_ch3_p37,"Show that the ordinary least squares estimator /3 is the (uniquely determined) linear minimax estimato r within the set U((3 ) = {Ay : AX = I p }
when (3 E e~ = {(3 E IRP : (3'T(3 ::; e}, i.e. the identity
sup p((3,/3) = _ inf sup p((3 , ~) e~ EU(~ ~E e~
holds true.",Estimation
lr_guren_ch3_p38,"Consider the linear regression model with p > 1 and assumpt ions (i)
to (iv) and (vii), so that X'X = I p • Show that for p((3 ,~ = tr[MSE((3 ,~ )]
the condition
p((3,/3p I) < p((3,/3I p )
is satisfied for any vector (3 E IRP such that (3'(3 ::; a 2 .",Estimation
lr_guren_ch3_p39,"Consider the linear regression model with assumptions (i) to (iv) , where
(3 E IRP is the realization of a p x 1 random vecto r b with E(bb') = a2M.
The p x p matrix M is symmetric nonnegative definite.
(a) Show that the Bayes risk of a homogeneous linear estimator Ay for (3
can be written as
pB((3 ,Ay) = a2 tr[A(X M X ' + I n)A' - AXM - MX'A' + M] .
(b) Let A . = MX'(XMX' + I n)-l . Show th at the Bayes risk of A .y is
given by
(c) Show that the Bayes risk of a homogeneous linear estimator Ay for (3
can be written as
(d) Explain why we can call A .y a homogeneous linear Bayes estimator for
(3.","Linear Algebra, Estimation"
lr_guren_ch3_p40,"Consider the linear regression model with assumptions (i) to (iv), where
/3 E IRP is the realization of a p x 1 random vecto r b with E(b) = I-L and
Cov( b) = a 2T . The p x p matrix T is symmetric nonnegative definite.
3.10 Problems 211
(a) Show that the Bayes risk of a linear estimator Ay+a for 13 can be written
in the form pB(13 ,Ay + a) =f + g, where
f = (j2tr(AA') + (j2tr[(AX - Ip)T(AX - I p)']
and
9 = tr[((AX - Ip)JL + a)((AX - Ip)JL + an'] .
(b) Show that A . y +a . is the linear Bayes estimator for 13, if A. minimizes
th e function f with respect to all p x n matrices A , and moreover a. =
JL - A.X JL holds true.
(c) Show th at the function f is minimal with respect to all p x n matrices A
for A = A. with A. = TX'(XTX' + I n )- 1.","Linear Algebra, Estimation"
lr_guren_ch3_p41,"Let 8 be a p x p symmetric positive definite matrix and let T be a
p x p symmetric nonnegative definite matrix . Show that the matrix T(T +
8-1 )-1 8- 1 is symmetric, so th at
holds true.","Linear Algebra, Estimation"
lr_guren_ch3_p42,"Show that und er the linear regression model with assumptions (i) to
(iv), th e direction modified shrinkage estimator
13{3o([1) = (1 - a)13o+ a13, 1
a=--, 1+0
0 > 0,
is a linear Bayes estimator for 13 , when 13 E lR,v is supposed to be the realization of a p x 1 random vector b with E(b) =130 and Cov(b) =0(j2(X'X)-l .",Estimation
lr_guren_ch3_p43,"Show that under the linear regression model with assumptions (i) to
(iv), the estimator
is a linear Bayes estimator for 13, when 13 E lRP is supposed to be the realization of a p x 1 random vector b with E(b) = 130 and Cov(b) = k(j2Ip",Estimation
lr_guren_ch4_p1,"Show that the inequality Ina'aI~ :SL Ina' holds true if and only if the
n x 1 vector a can be written as a = al n for some number 0 :S a :S lin.
[Hint: Note that Ina'aI~:SL Ina' implies the symmetry of th e n x n matrix
Ina' . Wh at can th en be said about a?]",Linear Algebra
lr_guren_ch4_p2,"Consider th e linear regression model with
(a) Check whether the two estimators
/3 - ( 2Yl ) 1 - 2 -Yl + Y2 - Y3+ 3Y4 ' /3 = (2Y1 + Y2 + Y3 + Y4)
2 Yl + Y2 + Y3 + 2Y4
are linearl y admissible for {3 .
(b) Consider the estimator Ay for {3 , where
A = (a a 00) OOaa '
and a is an arbitrary number with 0 :S a :S 1/2. Check whether Ay is
linearly admissible for {3.","Multiple Linear Regression, Estimation"
lr_guren_ch4_p3,"Show that th e est imator j3(g) is uniformly better than the est imator
13 -(g) from Example 4.2 with respect to p({3 ,/3) = E[(/3 - {3)'(/3 - {3)].","Multiple Linear Regression, Estimation"
lr_guren_ch4_p4,"Consider the est imator Lj3 for some p x p matrix L .
(a) Show that Lj3 is linearly admissible for {3 if and only if X' X L = L'X' X
and in addition all eigenvalues of L lie in [0,1].
4.5 Problems 255
(b) Show that if L13 is linearly admissible for {3 and in addition L = L' holds
true, th en it follows IIL1311 2 :S 111311 2 •","Multiple Linear Regression, Estimation, Linear Algebra"
lr_guren_ch4_p5,"Under the linear regression model with assumptions (i) to (iv) consider
the estimator
13 k,1= (X'X + k Ip + [(X'X + k I p)-1 )X ' y ,
where k,[ 2 0 are non-stochastic, see [14]. Check whether 13k,! is linearl y
admissible for {3.","Multiple Linear Regression, Estimation"
lr_guren_ch4_p6,"Check whether the estimator 0 is linearl y admissible for {3 und er th e
linear regression mod el with assumptions (i) to (iv).","Multiple Linear Regression, Estimation"
lr_guren_ch4_p7,Show th at the Obenchain class £Ob({3) is a convex set.,Linear Algebra
lr_guren_ch4_p8,"Consider th e linear regression model with assumptions (i) to (iv) and
(v). Use Remark 4.1 to demonstrate th at the general ridge estimator 13K =
(X' X + K)-1X'y is admissible for {3 within the set of all estimators (and
with respect to th e unweighted squared error loss) if the condition
p - 2 :S rk(K)
holds true . What can then be said about 13 , 13k and 13( {})?","Ridge Regression, Estimation"
lr_guren_ch4_p9,"Let T be a p x p symmetric nonnegative definite matrix and let S be a
p x p symmet ric positive definite matrix. Moreover, let G =I p - S-1 /2(T +
S-1)-1S-1/ 2.
(a) Show th at the matrix G is symmetric nonn egative definite .
(b) Show th at all eigenvalues of G are strictly smaller than 1.",Linear Algebra
lr_guren_ch4_p10,"Show th at {3o = R'(RR')- 1r satisfies the inequality
lI{30112 :S 1I{311 2 V{3!R,8=r .
[Hint: Consider the set of all possible solutions of R{3 = r with regard to the
specific solut ion {3o .]","Linear Algebra, Estimation"
lr_guren_ch4_p11,"Consider two p x p symmetric nonnegative definite mat rices A and B
sati sfying C(A) C(B) and AB = BA. Let A = U AlU ' and B = U A2U'
spectral decompositions of A and B via the same orthogonal matrix U. (Such
a simultaneous spectral decomp osition does exist in view of th e assumptions.)
Show that C(AI ) C(A2 ) is satisfied.",Linear Algebra
lr_guren_ch4_p12,"Consider t he linear regression model with assumptions (i) to (iv) and
(viii), R{3 = r . Show that a linear estimator 7!J is linearly admissible for {3 if
and only if where S = X'X, and G and g satisfy the conditions G = G' , lJ(G) c [0,1]","Multiple Linear Regression, Estimation, Linear Algebra"
lr_guren_ch4_p13,"Under the linear regression model with assumpt ions (i) to (iv), a linear
estimat or Ay + a is called weakly restricted if it satisfies
E[R(Ay + a )J = r Vf3 E IRP
for some m x p matrix R of full row rank and some m x 1 vector r. Show
that Ay + a is linearl y admissible for f3 under the linear regression model
with assumptions (i) to (iv) and (viii), Rf3 = r , if and only if under th e
unr estricted linear regression model with assumptions (i) to (iv) it is (a)
linearly admissible for f3 and (b) weakly restricted with respect to Rf3 =r .","Multiple Linear Regression, Estimation, Linear Algebra"
lr_guren_ch4_p14,"Check whether f30 = R ' (RR') - Ir is linearly admissible for f3 und er
the linear regression model with assumptions (i) to (iv) and (viii), Rf3 =r .","Multiple Linear Regression, Estimation, Linear Algebra"
lr_guren_ch4_p15,"Under the linear regression model with assumptions (i) to (iv), let M
a p x p symmetric nonn egative definite matrix. Show that for the estimator
f3 = M(M + (X'X)-I)-If3 the difference
a = MSE(f3 ,,8) - MSE(f3,.B)
is nonzero whenever p > 1.","Multiple Linear Regression, Estimation"
lr_guren_ch4_p16,"Under the linear regression model with assumpt ions (i) to (iv), let
f3'f3 :s a2 . Show that the ridge estimator ,8k = (X 'X + kIp)-I X'y is uniformly not worse than ,8 with respect to MSE, and, in addition, linearly
admissible for f3 if and only if the inequalities
2 p<k<--- - - 1 - >'max
are satisfied, where >'max denotes the largest eigenvalue of X'X .","Multiple Linear Regression, Estimation, Linear Algebra"
lr_guren_ch4_p17,"Let T be a p x p symmetric positive definite matrix. Use Theorem 4.23
to show that und er the linear regression model with assumpt ions (i) to (iv),
the inequality
holds t rue if and only if
2-p o:SL --T-1 + (X'X )- I
P
is satisfied. [Hint: ,8PT can be written as ,8PT = M (M + (X 'X )-I)- 1,8 for
M = (1/ p)T- 1 .J","Multiple Linear Regression, Estimation"
lr_guren_ch5_p1,"Under the linear regression model with assumptions (i) to (iv) and (v),
consider the maximum likelihood est imator
&ltL = .!. y'My , M = I n - X (X'X )-lX' , n
for a2 .
(a) Determine th e unweighted squared erro r risk p(a2 , &ltd = E[(&ltL_a2 ) 2 ]
of the maximum likelihood estimator for a2 .
(b) Show that for p =J 2 th e maximum likelihood esti mator &ltL is inadmissible by verifying that
( 2 ( 2 ) P a , a T < p a , aML
for every ({3 , a2 ) E lR.P x (0, 00).","Linear Algebra, Estimation"
lr_guren_ch5_p2,"Show that und er the linear regression model with assumptions (i) to (iv)
and (v) the unweighted squa red erro r risk of th e sample variance &~ is given
by
2a2
p(a 2
, &~) = n _ 1(a2 + 2b) + b
2 ,
where b = (&~ _a2 = n ~ {3 X'CX{3 is th e bias &~ and C = I n- ~I nI~
is the centering matrix.",Estimation
lr_guren_ch5_p3,"Let V be an n x n symmetric positive definite matrix, and let F be a
nonsingular n x n matrix with F'F = V-I . Show that FVF' =In .",Linear Algebra
lr_guren_ch5_p4,"Consider the linear regression model with assumptions (i) to (iii) and
(iv«).
(a) Show that the inequality
X(X'V- 1X)-1 X' :::;L V
holds true. [Hint: Apply Theorem A.65.]
(b) Use the inequality from (a) to demonstrate that
Cov(,8.) :::;L Cov(,8)
holds true, where ,8. and ,8 are generalized and ordinary least squares
estimator for {3, respectively.","Multiple Linear Regression, Estimation, Linear Algebra"
lr_guren_ch5_p5,"Consider the linear regression model with assumptions (i) to (iii) and
(iv»).
(a) Show that a; can be written as
(b) Show that E(O';) = 0'2 for every ({3,O' 2 ) E IRP x (0,00).","Multiple Linear Regression, Estimation, Linear Algebra"
lr_guren_ch5_p6,"For the seemingly unrelated regression model consider the estimators
O'st = (l/n)z~zt with Zs = Ys -Xs(X:Xs)-IXsYs for s,t E {1,2}. Show
that 10'121 :::; VO'llO'22 .","Multiple Linear Regression, Estimation"
lr_guren_ch5_p7,"(a) Consider the ratio
x'.!.1 I' x r(x):= n n n
x'x
and use Theorem A.48 to show that r(x) takes its values in the closed
interval [0,1] as x varies over IRn \ {O}.
(b) Consider the symmetric matrix V = (1- {!)I n + {!InI~ (occurring in the
case of equicorrelation) and show that x'V x > 0 for every x E IRn \ {O}
if and only if -l/(n - 1) < {! < 1.",Linear Algebra
lr_guren_ch5_p8,"Let X be an n x p matrix of full column rank.
(a) Use Theorem A.23 to show that
(In+XX')-1 =In-X(Ip+X'X)-IX' .
5.6 Problems 291
(b) Show that
(c) Show that
X '(In +XX')-1 = (I p + X 'X)-1 X ' .
(d) Show that the ordinary least squares estimat or 73 = (X lX)- 1X'y can
be written as","Linear Algebra, Estimation"
lr_guren_ch5_p9,"Let V be an n x n symmet ric nonnegative definite ma trix, and let X
be an n x p matrix with n > p. Show th at the condition C(VX) C(X) is
equivalent to PV = V P , where P = X X+ is the orthogonal pro jector onto
C(X).",Linear Algebra
lr_guren_ch5_p10,"Consider the linear regression model with assump tions (i) to (iii) and
(iv*) with
V =k2
I n + V 1 + V 2 ,
where k E JR, V 1 is symmetric nonnegative definite with C(V1) C(X),
and V 2 is symmet ric nonnegative definite with C(V2) C(X)l-.
(a) Expl ain why V = (1 - g)I n + ln 1~ is a special case of the above matrix
V .
(b) Show that ordinary and generalized least squares estimator coincide.","Linear Algebra, Estimation"
lr_guren_ch6_p1,"Consider the compa rison of two linear regression models with different
independent variables bu t the same observat ion y. Show that each of inequalities Cp(l) :S Cp (2) (for a fixed 0'), AICp (l ) :S AICp (2), 0'2(1) :S 0'2(2),
- 2 - 2 R (2) :S R (1), and :S ~ (l is equivalent to RSSp (l ) :S RSSp (2).","Multiple Linear Regression, Diagnostic, Remedy"
lr_guren_ch6_p2,"Consider the linear regression model y = X {3+ €: with assumpt ions (i)
to (iv) , and let the n x 1 vector of ones I n belong to the column space of X .
Let
R = 2: (Yi - y)(Yi - y)
( n _2 n A 2) 1/2 2:i=1(Yi - y) 2:i=1 (Yi - y)
be the samp le multiple correlation coefficie nt between Y and the prediction
y.
(a) Show that ~1I = ~y for 11 = Py, P = X (X' X )-1 X' .
(b) Show that R can be written as
R= y'(P - P dy
1/2 ' [y'(In - P 1 )yy'(P - P 1)y]
where P I = /n) ln l~.
(c) Conclude that the centered coefficient of determination is t he square
of R.","Multiple Linear Regression, Diagnostic, Remedy"
lr_guren_ch6_p3,"Consider an n x p matrix X of full column rank such C(l n) C(X).
Show that (l /n) :S Pii for i = 1, . .. , n , where Pii is the i-th main diagonal
element of P = X (X'X )- l X'.",Linear Algebra
lr_guren_ch6_p4,"Consider the linear regression model
(~~) = (~ nI d) (~) + (:~)
with assumptions (i) to (iv), where X l in an n l x P matrix of full column
rank (nl > p).
(a) Show that
328 6 Regression Diagnostics
holds true.
(b) Show that the ordinary least squares estimator for th e vector (f3' ,,')' is
given as
= (X~XI)-I~~YI) . , Y 2 - X 2f3
(c) Show that the residual sum of squares obtained in th e above model is
identical to
(YI -X1!3)'(YI-X1!3) ,
being the residual sum of squares obtained under the model YI = X 1f3 +
el ·","Multiple Linear Regression, Diagnostic, Estimation"
lr_guren_ch6_p5,"Consider the mean-shift outlier model
Y = Xf3 +en+ e
with assumptions (i) to (iv), where X in an n x p matrix of full column rank.
(a) Show that the residual sum of squares obtained under th e mean shift
outlier model is identical to
Y(i)(In - 1 - X (i)(X(i)X(i»)-1 ~)Y ( i ) ,
being the residual sum of squares obtained under th e model Y(i) = X (i)f3 + e(il> where (Y(i),X(i») is obtained from (y ,X) by deleting th e
i-th row.
(b) The orthogonal projector onto the column space of (X ,e.) can be written
as
P + _1_ (In - P)e ie~(In - P) , 1- pu
where P = X(X'X)-IX', see e.g. Property 2.4 in [20]. Show that th e
residual sum of squares obtained under the mean shift outlier model can
be written as
Y I '( P) ei n - y---.
1- Pii
(c) Conclude from (a) and (b) that
2 - P - Ti
a(i ) =a , n-p-1
where Ti denotes the i-th standardized residual.
(d) Show that th e i-th studentized residual can be written as","Multiple Linear Regression, Diagnostic, Remedy"
lr_guren_ch6_p6,"Consider the data given in Table 6.8, providing measurements of the
girth, height and volume of timber in 31 felled black cherry trees, see [93).
(Th is data set is available in R via data(trees) .)
(a) Consider a linear regression model with 'Volume' as dependent variable
and 'Girth' and 'Height' as independent variables. A pairwise plot of the
three variables may indicate that the postulated relationship is not linear.
Compute the p-values for RESET (with squares and cubes of fitted values
as z variables) and the rainbow test and conclude that the hypothesis of
linearity is only weakly supported.
(b) Consider a regression model
In(Volume)i = /31 In(Girth)i + /32In(Height)i + Ci
and compute again p-values for RESET and the rainbow test. Conclude
that the p-values do not act against the hypothesis of linearity.
(c) Compute the adjusted coefficient of determination for the model considered in (b) and conclude that the linear fit is quite high.
(d) Check whether stepwise regression will suggest an alternative model to
(b) with less independent variables, and conclude that t his is not the case.
(e) Check whether there are important outliers or high-leverage points in the
model (b), and conclude that this is not t he case.
(f) Compute t he matrix II of variance-decomposition proportions in the
model (b) and conclude that there is one harmful near linear dependency
in the model, involving the intercept variable and In(Height ).","Multiple Linear Regression, Diagnostic, Hypothesis Testing"
lr_final_2016_ p1,"Consider a joint distribution in which E[log2 Y |X = x] = β0 +β1 log2 x. Under this model, doubling the value of x produces a 4β1 multiplicative effect on the expected value of Y given X = x. True False",probability distribution
lr_final_2016_ p2,"If we take the residuals from the regression of y on x1, and plot them versus the residuals from regressing x2 on x1, the slope for the least squares regression line on this plot will be exactly equal to b2, the estimated coefficient of x2 in the multiple regression of y on x1 and x2. True False","Multiple Linear Regression, residual plots"
lr_final_2016_ p3,"If we take the residuals from the regression of y on x1, and plot them versus the residuals from regressing x2 on x1, the intercept for the least squares regression line on this plot will be exactly equal to zero. True False","Multiple Linear Regression, residual plots"
lr_final_2016_ p4,"The coefficient of multiple determination, denoted R2, gives the proportionate reduction to the total variation in the response variable Y that is achieved by accounting for the linear association between Y and the predictor variables in a regression. True False
","Multiple Linear Regression, r square"
lr_final_2016_ p5,"Let maxj r2j and minj r2j denote the maximum and minimum values of the squared correlation coefficients between the response variable Y and the predictor variables X1, . . . , Xp−1; then the coefficient of multiple determination R2 satisfies minj r2 j ≤ R2 ≤ maxj r2. True False","Multiple Linear Regression, r square"
lr_final_2016_ p6,"In a multiple linear regression with p − 1 predictor variables and an intercept term, and assuming a constant variance, an unbiased estimator of that variance is the mean squared error, MSE = 1 n−pPni=1(yi − yˆi)2. True False","Multiple Linear Regression, estimator"
lr_final_2016_ p7,"Changing the order in which terms are entered into a multiple linear regression model will change the decomposition of the regression sum of squares, but not their sum. With two predictors, for example, although in general SSR(X1) 6= SSR(X1|X2), and SSR(X2|X1) 6= SSR(X2), it will still be the case that SSR(X1) + SSR(X2|X1) = SSR(X2) + SSR(X1|X2) = SSR(X1, X2) . True False","Multiple Linear Regression, residual"
lr_final_2016_ p8,"Suppose we regress Y on X1, save the residuals as e(Y |X1); and likewise regress X2 on X1, save those residuals as e(X2|X1). Then the squared correlation between these two sets of residuals will equal R2Y 2|1 =SSR(X2|X1)SSE(X1), the coefficient of partial determination for X2, accounting for X1. True False","Multiple Linear Regression, residual"
lr_final_2016_ p9,"The coefficient of partial determination R2Y 2|1 will not in general be equal to the corresponding simple coefficient of determination R2Y 2, but the decomposition of the coefficient of multiple determination holds that R2 Y 1 + R2Y 2|1 = R2Y 2 + R2Y 1|2. True False","Multiple Linear Regression, r square"
lr_final_2016_ p10,"Adding a quadratic term to a simple linear regression model may be appropriate when the optimal response value is attained at an intermediate value of the predictor, that is, when the dose-response relationship is not monotone. The additional term in the mean function may have the effect of reducing R2 , but that sacrifice is justified by the improved realism of the model. True False","Simple Linear Regression, r square"
lr_final_2016_ p11,"Consider a regression problem with two quantitative predictors. The mean function for the general second-order model (quadratic in both terms, plus interaction modeled as the product x1x2) will have five parameters. True False
","higher order regression models, parameter"
lr_final_2016_ p12,"Consider a regression problem with two predictor variables, both categorical: one has four levels, and the other has five levels. The mean function for the main effects model (no interaction) will have eight parameters. True False","higher order regression models, parameter"
lr_final_2016_ p13,"Consider a regression problem with two predictor variables, both categorical: one has four levels, and the other has five levels. The mean function for the second order model (main effects and interaction) will have 21 parameters. True False","higher order regression models, parameter"
lr_final_2016_ p14,"Consider a regression problem with two predictor variables, one is categorical with four levels, and the other is continuous. The mean function for the parallel linear regressions (first order) model will have five parameters.True  False","higher order regression models, parameter"
lr_final_2016_ p15,"Consider a regression problem with two predictor variables, one is categorical with four levels, and the other is continuous. The mean function for the separate linear regressions (first order plus interaction) model will have 10 parameters. True False","higher order regression models, parameter"
lr_final_2016_ p16,"Consider a linear regression on two predictor variables x1 and x2. Other things being equal, the greater the difference in their average values, i.e., the greater is |x? - x?|, the greater is the variance of the least squares estimates of both their coefficients. True False","Multiple Linear Regression, Estimation"
lr_final_2016_ p17,"Consider a linear regression on two predictor variables. Other things being equal, the more highly correlated the predictors are to each other, the greater is the variance of the least squares estimates of both of their coefficients. True False","Multiple Linear Regression, Estimation"
lr_final_2016_ p18,"Consider a linear regression on two predictor variables. If the predictor variables are perfectly correlated (collinear), then the Estimation of their coefficients won't even have a unique solution. True False","Multiple Linear Regression, Estimation, collinearity"
lr_final_2016_ p19,"Consider a linear regression on p - 1 predictors, plus intercept. The variance inflation factor for the jth predictor xj is a function of the coefficient of multiple determination from regressing xj on x1, . . . , xj-1; thus the variance inflation factors for a set of predictors will depends on what order the terms are listed in the model specification. True False","Multiple Linear Regression, Prediction"
lr_final_2016_ p20,"If the Schwartz Bayesian Criterion, or BIC, selects a different set of predictors than does AIC, it will generally be the case that the model favored by BIC is simpler (contains fewer terms); this follows since AIC contains a more severe penalty for model complexity. True False",variable selection
lr_final_2016_ p21,"If data are available for P - 1 predictor variables, there are 2P -1 potential first-order models containing an intercept term. For five predictors (25 = 32) or even ten (210 = 1024) it may not be unreasonable for a computer routine to search them all. Much more than that, however, and some sort of search strategy (e.g., stepwise regression) will be necessary (or at least desirable). True False","higher order regression models, parameter"
lr_final_2016_ p22,"Depending on the number of terms in the optimal subset of predictor variables, one of backward selection and forward elimination will be more efficient than the other. But either algorithm will identify and select this ""best model"" eventually. True False","Multiple Linear Regression, variable selection"
lr_final_2016_ p23,"A simplistic but useful approach to testing for lack of fit in multiple linear regression is to consider the model E[Y |X = x] = 0 + 1x1 + ???+ p-1xp-1 + jjx2j and test H0 : jj = 0 versus HA : jj = 0. Do this for each j = 1, . . . , p - 1. True False","Multiple Linear Regression, Hypothesis Testing"
lr_final_2016_ p24,"If we conduct the tests for curvature described above, as well as Tukey's test for nonadditivity, and fail to reject H0 in every single instance, then we can safely conclude that the first order (linear) mean function is correct. True False","Multiple Linear Regression, Hypothesis Testing"
lr_final_2016_ p25,"In the Breusch-Pagan test for nonconstant variance, the alternative hypothesis holds that the variance function depends on a linear combination of the predictor variables, specifically that Var[Y |X = x] = exp {0 + 1x1 + ???+ p-1xp-1} . The null hypothesis holds that 1 = ???= p-1 = 0.True False","Multiple Linear Regression, Hypothesis Testing"
lr_final_2016_ p26,"Suppose we reject H0 in a Breusch-Pagan test, and conclude that the assumption of constant variance does not hold for our data. Some possible courses of action (not an exhaustive list) would be (1) investigate the need to transform the response variable; (2) consider weighted least squares; and (3) do nothing: ordinary least squares estimation of the mean function parameters will still be unbiased (if no longer optimal), and inferences will still be approximately valid. True False","Multiple Linear Regression, Hypothesis Testing"
lr_final_2016_ p27,"In a regression problem with response variable y, and x a vector of predictors, an outlier is characterized by an unusual y-value, given its x-value; it is quite possible that yj sits at or near the center of the marginal distribution of the yi's, but case j could still be an outlier (depending on the value of xj). True False","Multiple Linear Regression, outlier"
lr_final_2016_ p28,"In a regression problem with response variable y, and x a vector of predictors, a high-leverage case is characterized by an unusual x-value. In fact, the formula for computing the ""hat value"" hjj (the usual measure of case j's leverage) does not depend on yj at all. True False","Multiple Linear Regression, high-leverage"
lr_final_2016_ p29,"One approach to assessing the ""outlierness"" of case j in a linear regression problem involves fitting the model Yi = xi + j I{i=j} + i for i = 1, . . . , n. Since the estimation of  is unchanged by the additional term in the above model, the estimate of j is just the case j residual, ej = yj - xjb. True False","Multiple Linear Regression, outlier"
lr_final_2016_ p30,"When we conduct an outlier t-test for only the most ""outlier-ish"" case in a data set, we have implicitly conducted n such tests, and selectively reported only the most striking result. A valid (in fact conservative) adjustment for this multiple testing issue is the so-called Bonferroni correction, in which we simply multiply the p-value by n (capping at 1 of course). True False","Hypothesis Testing, outlier"
lr_final_2016_ p31,"It is mathematically possible (though rarely encountered in practice) for a case whose residual and hat-values are both close to zero to still be an influential case, in the sense of having the highest (or near highest) Cook's distance in a data set. True False",residual
lr_final_2016_ p32,"Under the statistical model Yi  indep Normal xi, 2 for i = 1, . . . , n, the maximum likelihood estimator of  is the value which minimizesnQ () =yi - xi 2 ;i=1 that is, under normality, maximum likelihood and least squares are equivalent criteria. True False","Multiple Linear Regression, Estimation, Estimation"
lr_final_2016_ p33,"Under the statistical model Yi  indep Normal (xi, k/wi) for i = 1, . . . , n, where the wi are known and not all equal, the maximum likelihood estimator of  is the value which minimizesnQ () =yi - xi 2 ;i=1 that is, under normality, even with unequal variance, maximum likelihood and ordinary least squares are equivalent criteria. True False","Multiple Linear Regression, Estimation, Estimation"
lr_final_2016_ p34,One of the strengths of the weighted least squares approach to regression estimation is that it requires no assumption about the relative variance for different observations of the response variable. True False,weighted least squares
lr_final_2016_ p35,The basic idea of the weighted least squares approach can be summarized as: Those observations which are subject to greater variability should be given greater weight in the model estimation. (And those observations subject to less variability should be given lower weight.) True False,weighted least squares
lr_final_2016_ p36,"A national insurance organization wants to study the consumption pattern of cigarettes in all 50 states and the District of Columbia. They fit a linear regression of Sales (packs of cigarettes sold per capita in each state) on average Price (in cents) of a pack of cigarettes, per capita Income, percentage of adults to complete HS, and median Age in each state. Estimated regression coefficients and a sequential ANOVA table are given below.

(a) Identify and interpret the estimated regression coefficient for the Price term: What does this number mean? Can we conclude that a sufficiently large cigarette tax (increasing the price of a pack of cigarettes), will reduce a state's cigarette sales? Briefly explain.

(b) Assess whether the term HS can reasonably be dropped from the model by reporting the P -value for a test of the null hypothesis that its coefficient is zero. Your conclusion?
(c) Assess whether HS and Age can both be dropped from the model, by testing the null hypothesis that their coefficients are simultaneously zero. You should: (i) compute a test statistic, and (ii) specify its null distribution, i.e., the distribution to which this value is compared in order to obtain a P -value. (You do not have to actually obtain that P -value.)
(d) Of the variation in Sales that remains after accounting for its linear association to Price and Income, what proportion remains unexplained after accounting for HS and Age as well?","Multiple Linear Regression, Parameter, Hypothesis Testing, r square"
lr_final_2016_ p37,"Researchers wishing to study teenage gambling in Great Britain took a survey of n = 47 British teenagers, then fit the regression of
y = ln (expenditure on gambling in pounds per year)
on x1 = socioeconomic status score (based on parents' occupation), x2 = ln(income) (in pounds per week), x3 = verbal score in words out of 12 correctly defined, and x4 = sex (0=male, 1=female). The estimated regression coefficients were:

(a) Consider two British teenagers: a girl with status = 30 and verbal = 4, and a boy with status = 60 and verbal = 6 (assume their respective incomes are equal). Use the model fit above to make a prediction of which child gambles more and by how much. Provide as precise an answer as you can, for a client who does not understand logarithms.

(b) Can you compute a standard error for your estimate in part (a)? A portion of the estimated variance-covariance matrix C^ov(b), from the R function vcov(), is given below.

","Multiple Linear Regression, Parameter, prediction"
lr_final_2016_ p38,"A national insurance organization wanted to study the consumption pattern of cigarettes in all 50 states and the District of Columbia. They fit a linear regression of y = packs of cigarettes sold per capita in each state on x1 = median age, x2 = per capita income, and x3 = average price of a pack of cigarettes. Index plots for Cook's distance, outlier t-statistics, and hat values are given below.
Diagnostic Plots


(a) With a single sentence, explain to someone who has zero knowledge of or interest in statistical modeling: What is it about Nevada and New Hampshire (cases 29 and 30, respectively) that makes them stick out in these plots?

(b) Carefully explain how you could generalize the idea of Cook's distance to compute a measure of the combined influence of cases 29 and 30. Can you foresee a practical difficulty in the assessment of this measure?"," Multiple Linear Regression, Diagnostic"
lr_midterm_2016_p1,"1. Given data {(xi
, yi) : i = 1, . . . , n} and assuming the simple linear regression model Yi = β0 +β1xi + εi, it will generally be the case that Xni=1(yi − β0 − β1xi)2 <Xni=1(yi − b0 − b1xi)2 where b1 and b0 denote the least squares estimates of β1 and β0, respectively. True False
","Simple Linear Regression, Estimation"
lr_midterm_2016_p2,"The least squares estimates of the slope and intercept in simple linear regression are the values of b1 and b0 that minimize the sum of the squared vertical distances between the points (xi, yi) and the line y = b0 + b1x in a scatterplot of y versus x. True False","Simple Linear Regression, Estimation"
lr_midterm_2016_p3,"The least squares estimates of the slope and intercept in simple linear regression are found by solving the so-called normal equations, a nonlinear system with no closed-form solution, but readily solvable with modern computers, via an iterative algorithm. True False","Simple Linear Regression, Estimation"
lr_midterm_2016_p4,"In simple linear regression, the estimation error of the least squares line is guaranteed to be zero at at least one point, the sample mean of the x-values in the data set. This is because, although b1 and b0 will not equal β1 and β0, they will satisfy E(Y |X = ¯x) = β0 + β1x¯ = b0 + b1x¯ True False","Simple Linear Regression, Estimation"
lr_midterm_2016_p5,"In simple linear regression, the ANOVA F-test of no linear association, and the t-test of H0 : β1 = 0 versus Ha : β1 6= 0, will give the exact same result; in fact, the test statistics are related by F ∗ = (t ∗)2.
True False","Simple Linear Regression, Hypothesis Testing"
lr_midterm_2016_p6,"One reason the assumption of constant variance is so crucial in simple linear regression is that the least squares estimators b1 and b0 are no longer unbiased for β1 and β0, respectively, if the assumption of constant variance is violated. True False","Simple Linear Regression, constant variance"
lr_midterm_2016_p7,"If we think of SSTO = Pn i=1(yi − y¯) 2 as a measure of the total variation observed in our data set, and define the residual sum of squares for our model fit by SSE = Pni=1(yi − yˆi)2 then we can reasonably interpret SSE/SSTO as the proportion of that variation which is unexplained by the model which yielded the fitted values ˆyi. True False",residual
lr_midterm_2016_p8,"The residuals versus fitted values plot is a useful graphical tool for assessing the assumption that the error term and response variable are uncorrelated, that is, Cov(εi, Yi) = 0 for i = 1, . . . , n. True False",residual plots
lr_midterm_2016_p9,"In simple linear regression, the residuals versus fitted values plot and the residuals versus predictor variable plot are equivalent, as the ˆyi and the xi are simple linear transformations of each other (of course the plot gets “flipped” if the least squares line has negative slope, b1 < 0). True False","Simple Linear Regression, residual plots"
lr_midterm_2016_p10,"A residuals versus fitted values plot like the following would suggest a variance function that is not constant, but rather, increasing with the mean response. True False",residual plots
lr_midterm_2016_p11,"The usual ANOVA F-test of H0 : β1 = 0 versus Ha : β1 6= 0 in simple linear regression is actually a special case of a more general framework for testing H0 : Reduced model vs. Ha : Full  model based on the test statistic F =SSE(Reduced) − SSE(Full)dfReduced − dfFull ,SSE(Full)dfFull True False","Simple Linear Regression, Hypothesis Testing"
lr_midterm_2016_p12,"The F-test for linear association and the F-test for lack of fit in simple linear regression have the same null (reduced) model, but different alternatives. True False","Simple Linear Regression, Hypothesis Testing"
lr_midterm_2016_p13,"The alternative model in the F-test for lack of fit is not fully general, in that it assumes a normally distributed error term; the mean and variance functions, however, are completely arbitrary. True False",Hypothesis Testing
lr_midterm_2016_p14," In the decomposition of SSE that underlies the F-test for lack of fit, the sum of squares for pure error, SSPE =PjPi(yij − y¯j )2, is that which could, in principle, be made zero by a “perfect-fitting” model. True False
",Hypothesis Testing
lr_midterm_2016_p15,"The basic idea underlying data transformations in simple linear regression is that if the model E(Y |X = x) = β0 + β1x and Var(Y |X = x) = σ2 fails to hold, perhaps there exist functions ψ(·) and T(·) such that E[T(Y )|X = x] = β0 + β1ψ(x) and Var[T(Y )|X = x] = σ2 does. True False
","Simple Linear Regression,  power transformation "
lr_midterm_2016_p16,"The great value of optimality criteria for power transformations is that they eliminate the need for subjective judgment on the part of the analyst: simply apply the power transformation that the algorithm suggested, and report that model fit. True False", power transformation 
lr_midterm_2016_p17,"Given a joint 1−α confidence region for the simple linear regression parameters (β0, β1), one can compute a 1 − α confidence band for the entire mean function, that is, E(Y |X = x) = β0 +β1x for all values of x. The Working-Hotelling method is an example. True False","Simple Linear Regression, confidence interval"
lr_midterm_2016_p18,"A necessary and sufficient condition for the simple linear regression model to hold for a bivariate data set {(xi, yi) : i = 1, . . . , n} is that the (xi, yi) be a random sample from a bivariate normal distribution. True False","Simple Linear Regression, probability distribution"
lr_midterm_2016_p19,"This problem concerns a data set on 31 felled black cherry trees. Let xi denote
the diameter of the ith tree (in inches), and let yi denote the volume of timber in cubic feet.
Following are (i) a scatterplot of y versus x, overlaid with lowess and least squares lines for
the regression of y on x; and (ii) a plot of residuals versus fitted values for the least squares
regression of y on x (with lowess curve overlaid).
(a) Without doing any calculation, predict the volume of a tree known to have a trunk diameter
of 14 inches. Indicate whether you’re using lowess or least squares, and defend your choice.
(b) Do the assumptions of the normal simple linear regression model seem to hold for these
data? Support your answer with specific reference to the information provided by the
above plots.
(c) If you had the raw data and a computer, what would be your next step to try to improve
your answer to part (a)? Recall that the volume of a cylinder with height h and radius r
is V = πr2h. (Data on the heights of the 31 trees are not available.)","Simple Linear Regression, Estimation, residual plots, Prediction"
lr_midterm_2016_p20,"Consider an agricultural experiment with n = 24 replications (on 24 different plots
of land). Let xi denote the quantity of phosphorous fertilizer, and yi the total yield, for the ith
plot, both measured in kilograms per hectare.
There were three replications at each of x = 5, 10, 15, 20, 30, and 50 kg/ha, and six replications
at x = 40 kg/ha.
Consider the simple linear regression model
Yi = β0 + β1xi + εi for i = 1, . . . , n
where
ε1, . . . , εn ∼ iid Normal(0, σ2
)
and suppose we wish to conduct a test of lack of fit.
(a) Clearly state the null and alternative hypotheses for this test. You should be explicit
about the mean function, error variance, and error distribution under both your reduced
and full models.
(b) For these data we obtain F
∗ =
MSLF
MSPE = 0.42. Express the P-value for the lack-of-fit test as
a probability P(X > c), specifying the numeric value of c and the probability distribution
of the random variable X.","Simple Linear Regression, Hypothesis Testing"
lr_midterm_2016_p21,"Continue with the agricultural data from the previous problem. Again, we have
x = amount of phosphorous fertilizer, and y = legume yield, both measured in kg/ha. Fitting
the model
Yi ∼ indep Normal(β0 + β1xi
, σ2
) for i = 1, . . . , n
we obtain the following results:
Coefficients:
Estimate Std. Error t value Pr(>|t|)
(Intercept) 2794.571 174.262 16.037 1.27e-13
x 44.066 5.749 7.665 1.19e-07
Residual standard error: 426.9 on 22 degrees of freedom
Multiple R-squared: 0.7275, Adjusted R-squared: 0.7152
F-statistic: 58.75 on 1 and 22 DF, p-value: 1.193e-07
(a) Identify and carefully interpret the estimate of the slope parameter β1 in your model.
(b) Given that t(.975, 22) = 2.074, calculate (but do not interpret) a 95% confidence interval
for the expected legume yield when zero phosphorous fertilizer is used. Do you trust this
interval? Explain.","Simple Linear Regression, Parameter, confidence interval"
lr_midterm_2016_p22,"Continue with the regression of legume yield on amount of phosphorous fertilizer,
both measured in kg/ha, from the previous problem.
Analysis of Variance Table
Response: y
Df Sum Sq Mean Sq F value Pr(>F)
x 1 10704014 10704014 58.748 1.193e-07
Residuals 22 4008485 182204
> predict(m1, data.frame(x=25), interval=""confidence"")
fit lwr upr
1 3896.21 3714.897 4077.522
> qt(.975, df=22)
[1] 2.073873
A 95% confidence interval for the mean yield with phosphorous level x = 25 is [3714, 4078].
(a) Compute an interval which you are 95% confident will contain the value of the yield in
the next plot using fertilizer with phosphorous level x = 25.
(b) Compute an interval which you are 95% confident will contain the average yield for three
plots using fertilizer with phosphorous level x = 25.","Simple Linear Regression, confidence interval"
lr_final_1_p1,"True/False. Please read each statement and put T(True)/F(False) in the beginning (2 pts
for each statement). Suppose we have n observations (x1, y1), . . . ,(xn, yn) from the following model
yi = β0 + β1xi + β2x2
i + i, (1)
where 1, . . . , n are i.i.d. standard normal random variables. Bill has fit the model (1) and obtained
the least square estimates βˆ0, βˆ1, βˆ2. Jim has the same data, but he fits the following model without
the quadratic term
yi = β0 + β1xi + i (2)
and obtains the least squares estimates β˜0, β˜1. In statements (a) – (e) you will compare Bill’s and
Jim’s estimates.
(a) βˆ0 is unbiased.
(b) β˜1 must be biased.
(c) Var(βˆ1) ≥ Var(β˜1).
(d) Since Bill uses the true model (1) and Jim uses the wrong model (2), the mean square
error of β˜0 is at least as big as the mean square error of βˆ0.
(e) If model (1) and (2) have the same R2, we prefer model (2).
(f) Least absolute deviation regression is preferred over least square estimator when there
are possible outliers.
(g) A large VIF (Variance Inflation Factor) indicates that there exists multicollinearity
problem.
(h) If by diagnostic we find the error variances are not equal across observations, we may
use Ridge Regression to solve the problem.
(i) We need to fit the model n times when calculating the PRESS statistic for model (1).
(j) Even if we have multicollinearity problem, we still have good fit of the data and good
prediction."," Multiple Linear Regression, Diagnostic, Parameter"
lr_final_1_p2,"Suppose we have the following two multiple linear regression models. Here i ∼i.i.d N(0, σ2).
Yi = β0 + β1Xi1 + · · · + βp−1Xi,p−1 + i. (3)
Yi = β0 + β1Xi1 + · · · + βp−1Xi,p−1 + βpXi,p + i. (4)
(a) (10 pts) Denote the R2 (the coefficient of multiple determination) for the two models as R2(1)
and R2(2). Is it true that R2(1) ≤ R2(2) always holds? If yes, prove it. If not, give a counter
example. (If you are providing a counter example, please write down the design matrix and the
response explicitly. The reasoning of R2(1) > R2(2) is required.)
2
(b) (10 pts) Denote the R2
a (the adjusted coefficient of multiple determination) for the two models
as R2
a(1) and R2
a(2). Is it true that R2
a(1) ≤ R2
a(2) always holds? If yes, prove it. If not, give a
counter example. (If you are providing a counter example, please write down the design matrix
and the response explicitly. The reasoning of R2
a(1) > R2
a(2) is required.)","Multiple Linear Regression, r square"
lr_final_1_p3,"Suppose we have the following multiple linear regression model. Here �i ∼i.i.d N(0, σ2).
Yi = β0 + β1X1i + β2X2i + β3X3i + �i. (5)
We perform the analysis in R:
> n=100
> X1 = rnorm(n)
> X2 = rnorm(n)
> X3 = X1*0.2 + X2*0.3 + 0.5* rnorm(n)
> Y = 2*X1 + 3*X2+ rnorm(n)
> fit = lm(Y~X1+X2+X3)
> anova(fit)
Analysis of Variance Table
Response: Y
Df Sum Sq Mean Sq F value Pr(>F)
X1 1 460.61 460.61 ? <2e-16 ***
X2 1 873.37 873.37 1036.4910 <2e-16 ***
X3 1 0.63 0.63 0.7529 0.3877
Residuals 96 80.89 0.84
---
Signif. codes: 0 *** 0.001 ** 0.01 * 0.05 . 0.1 1
(a) (5 pts) Calculate the missing F value for X1.
(b) (5 pts) Calculate the SSR (Regression Sum of Squares) from the output.
(c) (5 pts) Calculate the adjusted R-square value from the output.
(d) (5 pts) Perform the following hypothesis test
H0 : β2 = β3 = 0 v.s. H1 : not both β2 and β3 equal zero.
Write down the test method with the rejection rule and calculate the test statistic.
(e) (10 pts) Calculate the Extra Sum of Squares SSR(X3|X1, X2) and the coefficient of partial
determination R2
Y 3|12.","Multiple Linear Regression, r square, Hypothesis Testing"
lr_final_1_p4,"Consider multiple linear regression model as follows:
Yi = β0 + β1Xi,1 + · · · + βp−1Xi,p−1 + i, (6)
where i ∼i.i.d N(0, σ2),i = 1, · · · , n. Assume that X1, X2, ..., Xp−1 are all centered at 0 (i.e., X¯k =
n−1 n
i=1 Xi,k = 0, k = 1, ..., p − 1), and are uncorrelated with each other (i.e.,
n
i=1 Xi,jXi,k = 0,
∀j ∕= k). Denote the Ordinary Least Squares (OLS) estimate as bk for βk, k = 0, · · · , p − 1.
(a) (10 pts) Now for each k = 1, · · · , p − 1, we perform the following simple linear regression model
with the data points (X1,k, Y1), · · · ,(Xn,k, Yn).
Yi = β0 + βkXi,k + i. (7)
Suppose the new OLS estimate for the simple linear regression model is denoted as βˆ
k. Prove
βˆ
k = bk, for k = 1, · · · , p − 1.
(b) (10 pts) Suppose we are considering dropping the predictor X∗ ∈ {X1, X2, ..., Xp−1} from model
(6). Show that, for this purpose, the F-statistics in general linear test is equal to the square of
the t-statistics in the corresponding t-test.","Multiple Linear Regression, Simple Linear Regression, Hypothesis Testing"
lr_final_1_p5,"Suppose we have data (x1, y1), · · · ,(xn, yn), where xi = (xi1, · · · , xip)′
,i = 1, · · · , n. If we
fit the following regression model.
yi = x′
iβ + i,i = 1, · · · , n, (8)
where β = (β1, β2, · · · , βp)′
. Let βˆ be the least square estimate, then it is found out that the i-th
observation lies exactly on the regression line, i.e., yi = x′
iβˆ.
Now, fit the regression model again using all observations except the i-th observation (xi, yi). Denote
the least square estimate of the regression coefficients by βˆ(i). Show βˆ(i) = βˆ.","Multiple Linear Regression, Estimation"
lr_gy_hw1_p1,"Sixteen batches of the plastic were made, and from each batch one test item was molded.
Each test item was randomly assigned to one of the four predetermined time levels, and
the hardness was measured after the assigned elapsed time. The results are shown below;
X is the elapsed time in hours, and Y is hardness in Brinell units. Assume the rst-order
regression model (1.1) is appropriate (model (2.1) in the notes).
Data not displayed
Perform the following tasks:
i. Use R to obtain the estimated regression function.
ii. Use R to create a scatter plot with the line of best t. Make the line of best t red.
iii. Use R to calculate the best point estimate of 2.
iv. Use R to calculate the sample correlation coecient and coecient of determination.","Simple Linear Regression, Estimation"
lr_gy_hw1_p2,"Recall the sample residual is dened by ei = yi􀀀^yi, where yi is the ith response value and ^yi
is its corresponding tted value computed by least squares estimates ^yi = ^ 0 + ^ 1xi. Prove
the following properties:
i.
Xn
i=1
xiei = 0
ii.
Xn
i=1
^yiei = 0
1","Simple Linear Regression, Estimation, parameter"
lr_gy_hw1_p3,"Recall that the ith tted value ^ Yi can be expressed as a linear combination of the response
values, i.e.,
^ Yi =
X
j=1
hijYj ;
where
hij =
1
n
+
(xi 􀀀 x)(xj 􀀀 x)
Sxx
;
and
Sxx =
X
i=1
(xi 􀀀 x)2:
Prove the following properties of the hat-values hij .
i. X
j=1
h2
ij = hii
ii. X
j=1
hijxj = xi","Simple Linear Regression, Estimation, parameter"
lr_gy_hw1_p4,"Consider the regression through the origin model given by
(1) Yi = xi + i i = 1; 2; : : : ; n i
iid  N(0; 2):
The estimated model at observed point (x; y) is
^y = ^ x;
where
(2) ^  =
Pn
Pi=1 xiyi n
i=1 x2i
:
Complete the following tasks
i. Show that
^  =
Pn
i=1 P xiYi n
i=1 x2i
is an unbiased estimator of .
ii. Compute the standard error of estimator ^ .
iii. Identify the probability distribution of estimator ^ .","Simple Linear Regression, Estimation, parameter"
lr_gy_hw2_p1,"Sixteen batches of plastic were made, and from each batch one test item was molded. Each
test item was randomly assigned to one of the four predetermined time levels, and the
hardness was measured after the assigned elapsed time. The results are shown below; X
is the elapsed time in hours, and Y is hardness in Brinell units. Assume the rst-order
regression model (1.1) is appropriate ((2.1) in the notes).
Data not displayed
Use R to perform the following tasks:
i. Estimate the change in the mean hardness when the elapsed time increases by one hour.
Use a 99 percent condence interval. Interpret your interval estimate.
ii. The plastic manufacturer has stated that the mean hardness should increase by 2 Brinell
units per hour. Conduct a two-sided test to decide whether this standard is being
satised; use  = :01.
iii. Set up the ANOVA table.
iv. Test by means of an F-test whether or not there is a linear association between the
hardness of the plastic and the elapsed time. Use  = :01.
v. Does t2
calc from part [ii] equal fcalc from part [iv]? Explain why this identity holds or
does not hold.
vi. Construct 95% Bonferroni joint condence intervals for estimating both the true inter-
cept 0 and the true slope 1.
vii. Construct 95% Bonferroni joint condence intervals for predicting the true average
hardness corresponding to elapsed times 20, 28 and 36 hours.
1","Simple Linear regression, parameter, Hypothesis Testing, t test, F test, ANOVA, simaltaneous inference"
lr_gy_hw2_p2,"Consider the simple linear regression model
Yi = 0 + 1x + i i = 1; 2; : : : ; n i
iid  N(0; 2):
i. Assuming H0 : 1 = 0 is true, use R to simulate the sampling distribution of the
F-statistic
F =
MSR
MSE
=
SSR=1
SSE=(n 􀀀 2)
:
Assume 0 = 10,  = 3, n = 30 and run the loop 10,000 times to generate the sampling
distribution. Run the following code preceding the loop so that everyone has the same
seed and X data vector. Fill in the missing code to receive full credit.
# Set seed
set.seed(0)
# Assign sample size and create x vector
n <- 30
# Empty list for f-statistics
f.list <- NULL
x <- runif(n,min=0,max=100)
# Run loop
for (i in 1:10000) {
# Fill in the body of the loop here...
}
ii. From the simulated sampling distribution, plot a histogram and overlay the correct F-
density on the histogram. Adjust the bin-size to breaks=50 in the histogram. Overlay
the F-density in red.
iii. Compute the 95th percentile of both the simulated sampling distribution and the correct
F-distribution. Compare these values.","Simple Linear Regression, Hypothesis Testing, F test"
lr_gy_hw2_p3,"Consider splitting the response values y1; : : : ; yn into two groups with respective sample sizes
n1 and n2. Dene the dummy variable
(1) xi =
(
1 if group one
0 if group two
Show that the least squares estimators of 1 and 0 are respectively
^ 1 = y1 􀀀 y2 and ^ 0 = y2;
where y1 and y2 are the respective sample means of each group.","Simple Linear Regression, parameter, Estimation, dummy variable"
lr_gy_hw2_p4,"Fusible interlinings are being used with increasing frequency to support outer fabrics and
improve the shape and drape of various pieces of clothing. The article Compatibility of
Outer and Feasible Interlining Fabrics in Tailored Garments gave the accompanying data
on extensibility (%) at 100 gm/cm for both high-quality (H) fabric and poor-quality (P)
fabric specimens.
H 1.2 .9 .7 1.0 1.7 1.7 1.1 .9 1.7
1.9 1.3 2.1 1.6 1.8 1.4 1.3 1.9 1.6
.8 2.0 1.7 1.6 2.3 2.0
P 1.6 1.5 1.1 2.1 1.5 1.3 1.0 2.6
Use R to perform the following tasks.
i. Create an appropriate graphic to visualize the relationship between extensibility and
quality. Do you think there is a relationship between extensibility and quality? Make
sure to label the plot.
ii. Using the indicator variable
x =
(
1 if high quality
0 if low quality
run a regression analysis to test if the average fabric extensibility diers per group.","Simple Linear regression, parameter, Diagnostic, linearity, dummy variable"
lr_gy_hw2_p5,"i. Consider the regression through the origin model given by
(2) Yi = xi + i i = 1; 2; : : : ; n i
iid  N(0; 2):
Derive the maximum likelihood estimators of  and 2.
ii. Consider the residuals ei related to the regression through the origin model (2). Prove
that
Xn
i=1
eixi = 0:
Also, in the regression through the origin model (2), is the sum of residuals equal to
zero? I.e., is the following relation true?
Xn
i=1
ei = 0:
Explain your answer in a few sentences or less.
iii. Consider testing the null/alternative pair
H0 :  = 0 v.s. HA :  6= 0:
Note that 0 is the hypothesized value. Show that the likelihood-ratio test can be based
on the rejection region jTj > k with test statistic
T =
^  􀀀 0
rPn
i=1(YPi􀀀^xi)2=(n􀀀1) n
i=1 x2i
:
Note that k is some positive real number and ^  is the maximum likelihood estimator
of .
iv. Under H0, what is the probability distribution of the above test statistic T?
Hints: To solve 5 Part iii:
(a) Compute the likelihood-ratio test statistic () from Denition 2.4 on Page 45 of
the class notes.
(b) When simplifying the expression, the following trick might be useful:
Xn
i=1
(Yi 􀀀 0xi)2 =
Xn
i=1
(Yi 􀀀 ^ xi + ^ xi 􀀀 0xi)2:
(c) After simplifying  < c, nd a suitable transformation of  that yields the desired
test statistic and rejection rule.","Simple Linear Regression, Estimation, residual, Hypothesis Testing, t test"
lr_gy_hw3_p1,"Consider the model
Yi =  + i i = 1; 2; : : : ; n i
iid  N(0; 2):
The sample mean and sample variance are dened respectively as
 Y =
1
n
Xn
i=1
Yi
and
S2
Y =
1
n 􀀀 1
Xn
i=1
(Yi 􀀀  Y )2:
Use Theorem (3.6) on Page 91 to prove that the sample mean  Y and sample variance S2
Y
are independent random variables.","Multiple Linear Regression, parameter"
lr_gy_hw3_p2,"Consider the single factor anova model with three groups. The three groups are drug dose
1, drug dose 2 and control. Let n1 and y1 respectively denote the number of respondents
and sample mean response for drug dose 1 group. Let n2 and y2 respectively denote the
number of respondents and sample mean response for drug dose 2 group. Let n3 and y3
respectively denote the number of respondents and sample mean response for the control
group. Note that n = n1 +n2 +n3. The one-way anova can be expressed using the multiple
linear regression model
(1) Yi = 0 + 1xi1 + 2xi2 + i; i = 1; 2; : : : ; n i
iid  N(0; 2);
xi1 =
(
1 if drug dose 1
0 otherwise
xi2 =
(
1 if drug dose 2
0 otherwise
1
i. Write down the design matrix and response vector describing model (1).
ii. Compute (XTX)􀀀1 and simplify the result. Note: This requires inverting a 3  3
matrix. Simply look up the formula online.
iii. Estimate  =
􀀀
0 1 2
T
using the least squares equation.
iv. Write down an expression for the estimated covariance matrix of ^
.","Multiple Linear Regression, parameter, Estimation, dummy variable"
lr_gy_hw3_p3,"A commercial real estate company evaluates vacancy rates, square footage, rental rates,
and operating expenses for commercial properties in a large metropolitan area in order to
provide clients with quantitative information upon which to make rental decisions. The
data below are taken from 81 suburban commercial properties that are the newest, best
located, most attractive, and expensive for ve specic geographic areas. The data consists
of variables age (X1), operating expenses and taxes (X2), vacancy rates (X3), total square
footage (X4), and rental rates (Y ). For this data set, we skip residual diagnostics but in
practice, that should be included in the analysis. The data set HW3Problem3.txt is posted
on Canvas. Use R to perform the following tasks:
i. Regress the rental rates (Y ) against all of the covariates; age (X1), operating expenses
and taxes (X2), vacancy rates (X3), total square footage (X4). Write down the estimated
linear model.
ii. What percentage of variation in rental rates is explained by this model?
iii. Are there any marginal relationships between the response variable and covariates?
(Run t-tests on all slope parameters.)
iv. Run a F-test to see if there is an overall relationship between the rental rates and all
of the covariates.
v. Run a F-test to simultaneously test the slopes for age (X1) and vacancy rates (X3).
vi. Run a F-test to see if vacancy rates (X3) is a signicant predictor after holding all other
variables constant. To perform this test, use the full and reduced models. How does
this test relate to the summary output from part (ii)?
vii. The researcher wishes to obtain 95% interval estimates of the mean rental rates for
four typical properties specied as follows. Find the four condence intervals using the
Bonferroni procedure.
2
1 2 3 4
x1 5.0 6.0 14.0 12.0
x2 8.25 8.50 11.50 10.25
x3 0 0.23 0.11 0
x4 250,000 270,000 300,000 310,000","Multiple Linear Regression, Estimation, F test"
lr_gy_hw3_p4,"i. Recall the multiple linear regression model:
(2) Yi = 0 + 1xi1 + 2xi2 +    + p􀀀1xi;p􀀀1 + i:
For each of the following regression models, indicate whether it can be expressed in the
form of (2) by a suitable transformation. To receive full credit, describe the transfor-
mation if it exists.
a. Yi = 0 + 1xi1 + 2 log(xi2) + 3x2i
1 + i
b. Yi = i expf0 + 1xi1 + 2x2i
1g
c. Yi = log(1xi1) + 2xi2 + i
d. Yi = 0 expf1xi1g + i
e. Yi = [1 + expf0 + 1xi1 + ig]􀀀1
ii. Consider the toy data set:
y 2.44 8.36 98.33 115.06 128.91 123.46 148.30 138.10 153.10 119.08
87.66 134.88 91.71 126.81 40.41 54.94 33.03 35.74 14.99 -1.18
2.44 8.36 28.33 45.06 48.91 43.46 118.30 108.10 233.10 199.08
337.66 384.88
x 0.00 0.00 1.00 1.00 2.00 2.00 3.00 3.00 4.00 4.00
5.00 5.00 6.00 6.00 7.00 7.00 8.00 8.00 9.00 9.00
10.00 10.00 11.00 11.00 12.00 12.00 13.00 13.00 14.00 14.00
15.00 15.00
The data set is provided in the le HW3Problem4.txt on canvas. Use multiple linear
regression techniques to t a polynomial to the above data set. To receive full credit,
write down the estimated model and create a scatter plot with the estimated curve
overlaid on the plot.",Generalized Linear Regression
lr_gy_hw4_p1,"The Tri-City Oce Equipment Corporation sells an imported copier on a franchise basis
and performs preventive maintenance and repair service on this copier. The data have been
collected from 45 recent calls on users to perform routine preventive maintenance service;
for each call, x is the number of copiers serviced and Y is the total number of minutes spent
by the service person. The data set HW4Problem1.txt is posted on canvas.
Use R to perform the following tasks:
i. Obtain the estimated regression function.
ii. Create a scatterplot of the data set with the line of best t overlaid on the graph.
Create a QQ plot of the studentized deleted residuals, histogram of the studentized
deleted residuals, line plot of the studentized deleted residuals, plot the studentized
deleted residuals verses predicted values ^y, and studentized residuals verses predictor
variable x. Based on the plots, discuss whether any of the regression assumptions have
been violated. In your descriptions, relate your explanations to the relevant plots.
iii. Perform a Box-Cox procedure on the data set. What is the estimated value of ? Is it
necessary to perform this transformation on the response variable? Brie
y explain your
reasoning.","Simple Linear Regression, Diagnostic, residual plots, Box-Cox procedure"
lr_gy_hw4_p2,"Sixteen of the plastic were made, and from each batch one test item was molded. Each test
item was randomly assigned to one of the four predetermined time levels, and the hardness
was measured after the assigned elapsed time. For this data set; x is the elapsed time
in hours and Y is hardness in Brinell units. Use R to run a F- lack-of-t test to see if a
linear relationship is appropriate for this data set. The data set 1 22.txt is posted on
Canvas.","Simple Linear Regression, Hypothesis Testing, F lack-of-fit test"
lr_gy_hw4_p3,"Consider the non-constant variance linear model
(1) Yi = 0 + 1xi;1 + 2xi;2 +    + p􀀀1xi;p􀀀1 + i;
with
i
iid  N(0; 2
i ); i = 1; : : : ; n:
Dene the reciprocal of the variance 2
i as the weight wi:
wi =
1
2
i
and let
W =
0
BBBB@
w1 0    0
0 w2   
...
...
...
. . . 0
0    0 wn
1
CCCCA
:
We can estimate the non-constant variance model by minimizing the objective function
(2) Qw() =
Xn
i=1
wi(yi 􀀀 0 􀀀 1xi1 􀀀    p􀀀1xi;p􀀀1)2
Task: Derive the weighted least squares equation
(3) ^
w = (XTWX)􀀀1XTWY","Multiple Linear Regression, Estimation"
lr_gy_hw4_p4,"Observations on Y are to be taken when x = 10; 20; 30; 40; and 50, respectively. The true
regression function is E[Y ] = 20 + 10x. The error terms are independent and normally
distributed with E[i] = 0 and V ar[i] = :8x.
i. Generate a random Y observation for each x level and calculate both the ordinary and
weighted least squares estimates of the regression coecient 1 in the simple linear
regression function.
ii. Repeat part (a) 10,000 times, generating new random numbers each time.
iii. Calculate the mean and variance of the 10,000 ordinary least squares estimates of 1
and do the same for the 10,000 weighted least squares estimates.
iv. Do both the ordinary least squares and weighted least squares estimators appear to be
unbiased? Explain. Which estimator appears to be more precise here? Comment.","Multiple Linear Regression, weighted Estimation, prediction"
lr_gy_hw5_p1,"A commercial real estate company evaluates vacancy rates, square footage, rental rates,
and operating expenses for commercial properties in a large metropolitan area in order to
provide clients with quantitative information upon which to make rental decisions. The
data below are taken from 81 suburban commercial properties that are the newest, best
located, most attractive, and expensive for ve specic geographic areas. The data consists
of variables age (X1), operating expenses and taxes (X2), vacancy rates (X3), total square
footage (X4), and rental rates (Y ). The data set HW3Problem3.txt is posted on canvas.
Use R to perform the following exercises. Using type I sums of squares (F-stat), test if age
(X1) is a signicant predictor, after holding operating expenses & taxes (X2), vacancy rates
(X3) and total square footage (X4) constant.","Simple Linear Regression, F test"
lr_gy_hw5_p2,Show that: SSR(x1; x2; x3; x4) = SSR(x1) + SSR(x2; x3jx1) + SSR(x4jx1; x2; x3).,sum of squares
lr_gy_hw5_p3,"Consider the model
Yi = 0 + 1xi1 + 2xi2 + i:
Assuming that the sample correlation between x1 and x2 is zero, i.e.,
1
(n 􀀀 1)s1s2
Xn
i=1
(xi1 􀀀 x1)(xi2 􀀀 x2) = 0;
show that
SSR(x2jx1) = SSR(x2):
1",sum of squares
lr_gy_hw5_p4,"An assistant in the district sales oce of a national cosmetics rm obtained data on advertis-
ing expenditures and sales last year in the district's 44 territories. X1 denotes expenditures
for point-of-sale displays in beauty salons and department stores (in thousand dollars), and
X2 and X3 represent the corresponding expenditures for local media advertising and pro-
rated share of national media advertising, respectively. Let Y denote sales (in thousand
cases). The assistant was instructed to study the in
uence variables X1 and X2 have on
sales Y . The data set CosmeticsSales.txt is posted on Canvas.
Use R to perform the following tasks:
i. Run the simple linear regression Y  X1. Test if expenditures for point-of-sale displays
in beauty salons and department stores (X1) statistically in
uences sales (Y ).
ii. Run the simple linear regression Y  X2. Test if expenditures for local media advertis-
ing (X2) statistically in
uences sales (Y ).
iii. Now run the the full regression Y  X1 + X2 + X3. Perform marginal t-tests to see
if X1 statistically in
uences sales (Y ) and if X2 statistically in
uences sales (Y ), after
controlling for the variance of X3. Brie
y compare the results to Parts i. and ii. and
comment on any discrepancies. Note: No need for Bonferroni.
iv. You should have noticed some discrepancies in Part iii. Explain why these discrepan-
cies are occurring and provide graphical or exploratory evidence to complement your
argument.
2","Simple Linear Regression, Hypothesis Testing, t test"
lr_gy_hw5_p5,"Consider the standardized regression model
Y 
i = 
1x
i1 + 
2x
i2 + 
i :
The variables Y 
i ; xi
1 and x
21 are standardized versions of Yi; xi1 and xi2, i.e.,
Y 
i =
1
p
n 􀀀 1

Yi 􀀀  Y
sY

; x
i1 =
1
p
n 􀀀 1

xi1 􀀀 x1
s1

; x
i2 =
1
p
n 􀀀 1

xi2 􀀀 x1
s2

Task: Using the least squares equation, derive the estimators
^ 
1 =
rY 1 􀀀 r12rY 2
1 􀀀 r2
12
and ^ 
2 =
rY 2 􀀀 r12rY 1
1 􀀀 r2
12
:
Consider the linear regression model
Yi = 0 + 1xi1 + 2xi2 + i; i = 1; 2; : : : ; n; i
iid  N(0; 2):
It is easy to show that the parameters 
1 ; 
2 in the standardized regression model and the
original parameters 0; 1; 2 are related as follows:
1 =
sY
s1

1 ; 2 =
sY
s2

2 ; and 0 =  Y 􀀀 1x1 􀀀 2x2:
Reference pages 273 to 278 of the textbook for further details.
3","Generalized Linear Regression, standardization"
lr_hw_hw7_2016_p1,"(Problem 9.10 in KNN) A personnel ocer in a governmental agency administered four newly
developed aptitude tests to each of 25 applicants for entry-level clerical positions in the agency.
For purpose of the study, all 25 applicants were accepted for positions irrespective of their
test scores. After a probationary period, each applicant was rated for prociency on the job.
The scores on the four tests (X1;X2;X3;X4) and the job prociency score (Y ) for the 25
applicants were as given in the data le job_proficiency.txt, available in the CourseWorks
Data folder.
(a) Prepare a set of adjacent box plots for the test scores of the four newly developed aptitude
tests (that is, all four box plots on one set of axes). Are there any noteworthy features
in these plots? Comment.
(b) Obtain the scatterplot matrix for response and predictor variables combined. What
do the scatterplots suggest abut the nature of the functional relationship between the
response variable Y and each of the predictor variables? Are any serious multicollinearity
problems evident? Explain.
(c) Fit the multiple regression function containing all four predictor variables as rst-order
terms. Does it appear that all predictor variables should be retained? Explain.","Simple Linear Regression, Diagnostic, residual plots, Multiple Linear Regression"
lr_hw_hw7_2016_p2,"(Problem 9.18 in KNN) Continue with the Job prociency data from the previous exercise.
(a) Run the backward elimination algorithm using AIC as the model selection criterion.
What mean function is selected by the algorithm?
(b) Run a forward selection routine, again choosing a \best"" model based on AIC. What
mean function does this algorithm suggest?
(c) Repeat parts (a) and (b) using the Bayesian criterion SBC instead of AIC. Does your
conclusion about the \best"" mean function change? Explain.","Remedy, variable selection"
lr_hw_hw7_2016_p3,"(Project 9.25 in KNN) Consider again the SENIC Project data, this time regressing Y =
(Length of stay)􀀀1 on the predictor variables
Variable Description
X1 Average age of patients
X2 Infection risk
X3 log(Routine culturing ratio)
X4 Routine chest X-ray ratio
X5 log(Number of beds)
X6 log(Average daily census)
X7 log(Number of nurses)
X8 Available facilities and services
(a) Use the Box-Cox methodology to justify the choice of response and predictor transfor-
mations suggested in the variable denitions above.
(b) Examine (but do not submit) a scatterplot matrix of response and all predictors. Which
batch of predictor variables are most highly correlated?
(c) Run a forward selection algorithm, starting with the intercept-only model, and taking all
eight predictor variables as the scope. Which variables are included in the mean function
suggested by the forward selection algorithm?
(d) Run the backward elimination routine on the mean function that includes all eight predic-
tor variables. Does this algorithm choose the same mean function as forward selection?
(e) Present and interpret a nal tted model of your choosing, that you feel provides the best
available aid to our understanding of how the average length of hospital stay is related
to the hospital characteristics encompassed by the variables in the SENIC dataset.","Diagnostic, Box-Cox procedure, variable selection"
lr_hw_hw8_2016_p1,"(Problem 6.15 in KNN) Continue with the Patient satisfaction data from an earlier homework,
where the response variable Y is patient satisfaction, and the predictors are patient's age in
years (X1) and indexes of illness severity and anxiety level (X2 and X3, respectively).
(a) Consider the regression model where Y jX = x is normally distributed with with mean
E(Y jX = x) = 0 + 1x1 + 2x2 + 3x3
and variance
Var(Y jX = x) = 2 :
Test for lack of t in the linear mean function by testing the need for a quadratic term in
each of the three variables individually, as well as the tted values (the latter is Tukey's
test of additivity). What do you conclude?
(b) Conduct the Breusch-Pagan test for nonconstancy of the variance function. Clearly state
your null and alternative hypotheses, report a p-value, and clearly state your conclusion.","Simple Linear Regression, Hypothesis Testing, F lack-of-fit test"
lr_hw_hw8_2016_p2,"(Problem 10.11 in KNN) Continue with the Patient satisfaction data from the previous exer-
cise.
(a) Obtain the studentized deleted residuals and identify any cases whose outlier t-test gives
a Bonferroni-adjusted p-value less than 1. What do you conclude?
(b) Obtain the diagonal elements of the hat matrix, i.e., the \hat-values"" hii. Do there seem
to be any high-leverage cases in this data set?
1
(c) Hospital management wishes to estimate the mean patient satisfaction for patients who
are X1 = 30 years old, whose index of illness severity is X2 = 58, and whose index of
anxiety level is X3 = 2:0. Might this estimate involve a \hidden extrapolation""? Answer
with reference to the new case leverage value
hnew,new = x0
new(X0X)􀀀1xnew
(d) Calculate Cook's distance Di for each case and prepare an index plot. Which is the most
in
uential case in the dataset according to this measure? Is this case in
uential because
of its \outlierness,"" high leverage, or both?","Hypothesis Testing, simultaneous inference, Diagnostic, high leverage, outlier"
lr_hw_hw8_2016_p3,"(Project 10.27 in KNN) Continue with our analysis of the SENIC Project data. Here we take
average length of stay as the response variable Y , and consider three predictors: X1 = age,
X2 = routine chest X-ray ratio, and X3 = average daily census.
(a) Consider the regression model where Y jX = x is normally distributed with with mean
E(Y jX = x) = 0 + 1x1 + 2x2 + 3x3
and variance
Var(Y jX = x) = 2 :
Test for lack of t in the linear mean function by testing the need for a quadratic term in
each of the three variables individually, as well as the tted values (the latter is Tukey's
test of additivity). What do you conclude?
(b) Conduct the Breusch-Pagan test for nonconstancy of the variance function. Clearly state
your null and alternative hypotheses, report a p-value, and clearly state your conclusion.
(c) Obtain the studentized deleted residuals and identify any cases whose outlier t-test gives
a Bonferroni-adjusted p-value less than 1. What do you conclude?
(d) Obtain the diagonal elements of the hat matrix, i.e., the \hat-values"" hii. Do there seem
to be any high-leverage cases in this data set?
(e) Calculate Cook's distance Di for each case and prepare an index plot. Identify the two
most in
uential cases in the dataset according to this measure. Are these cases in
uential
because of their \outlierness,"" high leverage, or both?","Simple Linear Regression, Hypothesis Test, F lack-of-fit test, simultaneous inference, high-leverage, outlier"
lr_hw_hw8_2016_p4,"Repeat the previous exercise, but this time we take as our response variable
Y = (Length of stay)􀀀1
and consider the predictors X1 = Age, X2 = X-ray ratio, and X3 = log(Census). How do
your conclusions change under these transformations?","Generalized Linear Regression, power transformation"
lr_hw_hw8_p1,"Suppose that Yi = 0 + 1Xi1 + : : : + pXip + i, where i
iid  N(0; 2) and dene
P = X(XTX)􀀀1XT .
(a) Prove that P2 = P.
(b) Prove that Tr(P) = p + 1.
(c) Use part (a) to prove that 0  Pii  1.
(d) Suppose that ^ Yi = ^ 0 + ^ 1Xi1 + : : : + ^ pXip, where ^
denotes the estimate of the
OLS. Prove that
Yi 􀀀 ^ Yi  N(0; 2(1 􀀀 Pii))
Pii is known as the leverage score for observation i.","Multiple Linear Regression, parameter, Estimation"
lr_hw_hw8_p2,"Consider the dataset that was given in the nal exam. Include all the features that you
want in the model and do forward variable selection to order all the variables (including
dummy variables). Then use cross validation (using the test set that is given to you) to
pick the best number of elements you would like to put in. Compare the performance
of your new model with that of Model 5 in the exam(use the zip-code as a categorical
variable).","Remedy, model validation"
lr_hw_hw8_p3,"Consider the prostate cancer data that was introduced in HW5 and answer the following
questions about that dataset. If a datapoint has F in its last column you should put
it in a test set. Otherwise, put it in your training set.
(a) Using visual inspection of the scatter and box-plots only propose a model for your
data. Feel free to introduce new predictors if you think they are useful.
(b) Fit the model you proposed in the last part, and use residual plots to see if there
are still patterns you want to capture.
(c) Try to improve your model.
(d) Use forward stepwise to order the predictors and then nd the optimal number
of predictors you would like to include using the estimate of the prediction error
based on the test set.
(e) Use both ridge and LASSO to predict lpsa. Use cross validation (based on the
estimate of the prediction error on the test set) to optimize the parameter () of
LASSO and Ridge.","Diagnostic, residual plots, Remedy, variable selection, Ridge Regression, LASSO Regression"
lr_ll_hw1_p1,"Sixteen batches of the plastic were made, and from each batch one test item was molded.
Each test item was randomly assigned to one of the four predetermined time levels, and
the hardness was measured after the assigned elapsed time. The results are shown below;
X is the elapsed time in hours, and Y is hardness in Brinell units. Assume the rst-order
regression model (1.1) is appropriate (model (2.1) in the notes).
Data not displayed
Data can be found in HW 1 Problem 1 Data.R. Perform the following tasks:
1. Use R to obtain the estimated regression function.
2. Use R to create a scatter plot with the line of best t. Make the line of best t red.
3. Use R to calculate the best point estimate of 2.
4. Use R to calculate the sample correlation coecient and coecient of determination.","Simple Linear Rregression,  least squares method, data plot, parameter, estimation, correlation coefficient, coefficient of determination"
lr_ll_hw1_p2,"Xi's are one-dimensional random variables. Assume that
Xi
i.i.d.  N(; 2); for i = 1; 2; and 3:
1. Given 3 observations (x1; x2; x3), what is the maximize likelihood estimator of (; 2)
(note that we want to get the MLE of 2 instead of )?
2. Denote the MLE of 2 as b2
MLE. Is the MLE unbiased, i.e., does E(b2
MLE) = 2 hold?
To answer this question, we rst try to conduct a simulation in R. Generate 5000
data sets, with each of them contains three independent observations following a one-
dimensional normal distribution with mean  = 2 and variance 2 = 9. Denote the
data sets as
X(1) = (x(1)
1 ; x(1)
2 ; x(1)
3 );
X(2) = (x(2)
1 ; x(2)
2 ; x(2)
3 );
...
X(5000) = (x(5000)
1 ; x(5000)
2 ; x(5000)
3 ):
1
Based on each data set X(b), calculate the MLE (b2
MLE)(b). The average
1
5000
X5000
b=1
(b2
MLE)(b)
is supposed to be a very close approximation to E(b2
MLE). Is the average close to 2?
3. Calculate E(b2
MLE) explicitly. Is it unbiased?
4. If in each sample there are n observations, then what is E(b2
MLE)? Based on this result,
can you nd a new estimator which is unbiased for 2, i.e., nd a new estimator ^2,
such that E(^2) = 2?
5. If we do not assume that Xi's are normally distributed, but still assume that they are
independent and identically distributed random variables, based on n observations
(x1; : : : ; xn), what is an unbiased estimate of Var(X1)?
2","maximize likelihood estimation, simulation, parameter, estimation"
lr_ll_hw2_p1,"Recall the sample residual is dened by ei = yi􀀀^yi, where yi is the ith response value and ^yi
is its corresponding tted value computed by least squares estimates ^yi = ^ 0 + ^ 1xi. Prove
the following properties:
i.
Xn
i=1
xiei = 0
ii.
Xn
i=1
^yiei = 0","Simple Linear Regression, residual, independence"
lr_ll_hw2_p2,"Recall that the ith tted value ^ Yi can be expressed as a linear combination of the response
values, i.e.,
^ Yi =
X
j=1
hijYj ;
where
hij =
1
n
+
(xi 􀀀 x)(xj 􀀀 x)
Sxx
;
and
Sxx =
X
i=1
(xi 􀀀 x)2:
Prove the following properties of the hat-values hij .
i. X
j=1
h2
ij = hii
ii. X
j=1
hijxj = xi","Simple Linear Regression, hat matrix, prediction"
lr_ll_hw2_p3,"Consider the regression through the origin model given by
(1) Yi = xi + i i = 1; 2; : : : ; n i
iid  N(0; 2):
The estimated model at observed point (x; y) is
^y = ^ x;
where
(2) ^  =
Pn
Pi=1 xiyi n
i=1 x2i
:
Complete the following tasks
i. Show that
^  =
Pn
i=1 P xiYi n
i=1 x2i
is an unbiased estimator of .
ii. Compute the standard error of estimator ^ .
iii. Identify the probability distribution of estimator ^ .
iv. Show that ^  is also the maximum likelihood estimator of . What is the MLE of 2?
v. Consider the residuals ei related to the regression through the origin model (1). Prove
that
Xn
i=1
eixi = 0:
Also, in the regression through the origin model (1), is the sum of residuals equal to
zero? I.e., is the following relation true?
Xn
i=1
ei = 0:
Explain your answer in a few sentences or less.
2","Simple Linear Regression, parameter, estimation, Estimation, residual, standardize,"
lr_ll_hw2_p4,"Sixteen batches of plastic were made, and from each batch one test item was molded. Each
test item was randomly assigned to one of the four predetermined time levels, and the
hardness was measured after the assigned elapsed time. The results are shown below; X
is the elapsed time in hours, and Y is hardness in Brinell units. Assume the rst-order
regression model (1.1) is appropriate ((2.1) in the notes).
Data not displayed
Data can be found in HW 2 Problem 4 Data.R. Use R to perform the following tasks:
i. Estimate the change in the mean hardness when the elapsed time increases by one hour.
Use a 99 percent condence interval. Interpret your interval estimate.
ii. The plastic manufacturer has stated that the mean hardness should increase by 2 Brinell
units per hour. Conduct a two-sided test to decide whether this standard is being
satised; use  = :01.
iii. Set up the ANOVA table.
iv. Test by means of an F-test whether or not there is a linear association between the
hardness of the plastic and the elapsed time. Use  = :01.
v. Does t2
calc from part [ii] equal fcalc from part [iv]? Explain why this identity holds or
does not hold.","Simple Linear Regression, estimation, confidence interval, Hypothesis Testing, t test, ANOVA, F test, "
lr_ll_hw2_p5,"Consider splitting the response values y1; : : : ; yn into two groups with respective sample sizes
n1 and n2. Dene the dummy variable
(3) xi =
(
1 if group one
0 if group two
Show that the least squares estimators of 1 and 0 are respectively
^ 1 = y1 􀀀 y2 and ^ 0 = y2;
where y1 and y2 are the respective sample means of each group.","dummy variable, Simple Linear Regression, least squared estimation, parameter, "
lr_ll_hw2_p6,"Consider the simple linear regression model
Yi = 0 + 1x + i i = 1; 2; : : : ; n i
iid  N(0; 2):
i. Assuming H0 : 1 = 0 is true, use R to simulate the sampling distribution of the
F-statistic
F =
MSR
MSE
=
SSR=1
SSE=(n 􀀀 2)
:
Assume 0 = 10,  = 3, n = 30 and run the loop 10,000 times to generate the sampling
distribution. Run the following code preceding the loop so that everyone has the same
seed and X data vector. Fill in the missing code to receive full credit.
# Set seed
set.seed(0)
# Assign sample size and create x vector
n <- 30
# Empty list for f-statistics
f.list <- NULL
x <- sample(1:100/30,n,replace=T)
# Run loop
for (i in 1:10000) {
# Fill in the body of the loop here...
}
ii. From the simulated sampling distribution, plot a histogram and overlay the correct F-
density on the histogram. Adjust the bin size to breaks=50 in the histogram. Overlay
the F-density in red.
iii. Compute the 95th percentile of both the simulated sampling distribution and the correct
F-distribution. Compare these values.","Simple Linear Regression, Hypothesis Testing, F test"
lr_ll_hw3_p1,"Consider the regression through the origin model given by
(1) Yi = xi + i i = 1; 2; : : : ; n i
iid  N(0; 2):
The estimated model at observed point (x; y) is
^y = ^ x;
where
(2) ^  =
Pn
Pi=1 xiyi n
i=1 x2i
:
Complete the following tasks
i. Consider testing the null/alternative pair
H0 :  = 0 v.s. HA :  6= 0:
Note that 0 is the hypothesized value. Show that the likelihood-ratio test can be based
on the rejection region jTj > k with test statistic
T =
^  􀀀 0
rPn
i=1(YPi􀀀^xi)2=(n􀀀1) n
i=1 x2i
:
Note that k is some positive real number and ^  is the maximum likelihood estimator
of .
ii. Under H0, what is the probability distribution of the above test statistic T?
Hints: To solve Part i:
(a) Compute the likelihood-ratio test statistic () from Denition 2.4 on Page 47 of
the class notes.
(b) When simplifying the expression, the following trick might be useful:
Xn
i=1
(Yi 􀀀 0xi)2 =
Xn
i=1
(Yi 􀀀 ^ xi + ^ xi 􀀀 0xi)2:
(c) After simplifying  < c, nd a suitable transformation of  that yields the desired
test statistic and rejection rule.","Simple Linear Regression, Hypothesis Testing, T test, Estimation, T test"
lr_ll_hw3_p2,"Sixteen batches of plastic were made, and from each batch one test item was molded. Each
test item was randomly assigned to one of the four predetermined time levels, and the
hardness was measured after the assigned elapsed time. The results are shown below; X
is the elapsed time in hours, and Y is hardness in Brinell units. Assume the rst-order
regression model (1.1) is appropriate ((2.1) in the notes).
Data not displayed
Data can be found in HW 2 Problem 4 Data.R. Use R to perform the following tasks:
i. Construct 95% Bonferroni joint condence intervals for estimating both the true inter-
cept 0 and the true slope 1.
ii. Construct 95% Bonferroni joint condence intervals for predicting the true average
hardness corresponding to elapsed times 20, 28 and 36 hours.","Simple Linear Regression, Simultaneous inference, confidence interval, prediction"
lr_ll_hw3_p3,"Consider the single factor anova model with three groups. The three groups are drug dose
1, drug dose 2 and control. Let n1 and y1 respectively denote the number of respondents
and sample mean response for drug dose 1 group. Let n2 and y2 respectively denote the
number of respondents and sample mean response for drug dose 2 group. Let n3 and y3
respectively denote the number of respondents and sample mean response for the control
group. Note that n = n1 +n2 +n3. The one-way anova can be expressed using the multiple
linear regression model
(3) Yi = 0 + 1xi1 + 2xi2 + i; i = 1; 2; : : : ; n i
iid  N(0; 2);
xi1 =
(
1 if drug dose 1
0 otherwise
xi2 =
(
1 if drug dose 2
0 otherwise
i. Write down the design matrix and response vector describing model (3).
ii. Compute (X>X)􀀀1 and simplify the result. This requires inverting a 3  3 matrix.
iii. Estimate  =
􀀀
0 1 2
>
using the least squares equation.
iv. Write down an expression for the estimated covariance matrix of ^
.","Multiple Linear Regression, dummy variable, parameter, Estimation, covariance"
lr_ll_hw3_p4,"In this problem, we will use the dataset state.x77 that comes with standard R installation.
It is a data set about the 50 states of united states.
Type help(state.x77) in your console window to read more about this data set.
To load the data set, use the following lines of code
library(datasets)
statedata=as.data.frame(state.x77)
We study the association between life expectancy (Life Exp) and income (Income).
i. Make a scatter plot of these two variables. Label the points by state abbreviations.
ii. Consider a population of 50 states. Fit a simple linear regression model between Y
(Life Exp) and X (Income) at the population level.
(4) Y = 0 + 1X + ;   N(0; 2):
The least-squares estimate b0 and b1 based on data of all 50 states can be viewed as the
true parameter for this population. Calculate the estimate and add the true regression
line to the scatter plot you obtained in i (you may use the function abline).
iii. Now we consider 4 random samples. Use a for loop to run an identical regression
analysis on 4 randomly selected samples. Within the loop, we will implement the
following steps for each repetition.
Step 1: randomly select 10 states using the sample function of R. Color the selected
states in red in the scatter plot.
Step 2: run least square regression on the selected states only and add the estimated
regression line to the scatter plot.
Step 3: compute and add a Working-Hotelling 95% condence band (Section 2.15 in
lecture notes) for the true regression line in the population using the sample.
In your homework, you should show 4 plots, with each of them for one randomly selected
sample. In each plot, you need to show the scatter plot for 50 states, the true regression,
the selected samples (in red), the estimated regression line (in red), and the condence
band (in red) constructed based on the random sample.","Simple Linear Regression, Estimation, data plot, Multiple Linear Regression, confidence interval"
lr_ll_hw4_p1,"Consider the model
Yi =  + i i = 1; 2; : : : ; n i
iid  N(0; 2):
The sample mean and sample variance are dened respectively as
 Y =
1
n
Xn
i=1
Yi
and
S2
Y =
1
n 􀀀 1
Xn
i=1
(Yi 􀀀  Y )2:
Use Theorem 3.6 on Page 93 to prove that the sample mean  Y and sample variance S2
Y are
independent random variables.","Generalized Linear Regression, sum of squares, parameter"
lr_ll_hw4_p2,"A commercial real estate company evaluates vacancy rates, square footage, rental rates,
and operating expenses for commercial properties in a large metropolitan area in order to
provide clients with quantitative information upon which to make rental decisions. The
data below are taken from 81 suburban commercial properties that are the newest, best
located, most attractive, and expensive for ve specic geographic areas. The data consists
of variables age (X1), operating expenses and taxes (X2), vacancy rates (X3), total square
footage (X4), and rental rates (Y ). For this data set, we skip residual diagnostics but in
practice, that should be included in the analysis. The data set HW4Problem2.txt is posted
on Canvas. Use R to perform the following tasks:
i. Regress the rental rates (Y ) against all of the covariates; age (X1), operating expenses
and taxes (X2), vacancy rates (X3), total square footage (X4). Write down the estimated
linear model.
ii. What percentage of variation in rental rates is explained by this model?
1
iii. Are there any marginal relationships between the response variable and covariates?
(Run t-tests on all slope parameters.)
iv. Run a F-test to see if there is an overall relationship between the rental rates and all
of the covariates.
v. Run a F-test to simultaneously test the slopes for age (X1) and vacancy rates (X3).
vi. Run a F-test to see if vacancy rates (X3) is a signicant predictor after holding all other
variables constant. To perform this test, use the full and reduced models. How does
this test relate to the summary output from part (ii)?
vii. The researcher wishes to obtain 95% interval estimates of the mean rental rates for
four typical properties specied as follows. Find the four condence intervals using the
Bonferroni procedure.
1 2 3 4
x1 5.0 6.0 14.0 12.0
x2 8.25 8.50 11.50 10.25
x3 0 0.23 0.11 0
x4 250,000 270,000 300,000 310,000","Multiple Linear Regression, Hypothesis Testing, coefficient of determination, t test, F test, confidence interval, Simultaneous inference"
lr_ll_hw4_p3,"i. Recall the multiple linear regression model:
(1) Yi = 0 + 1xi1 + 2xi2 +    + p􀀀1xi;p􀀀1 + i:
For each of the following regression models, indicate whether it can be expressed in the
form of (1) by a suitable transformation. To receive full credit, describe the transfor-
mation if it exists.
a. Yi = 0 + 1xi1 + 2 log(xi2) + 3x2i
1 + i
b. Yi = i expf0 + 1xi1 + 2x2i
1g
c. Yi = log(1xi1) + 2xi2 + i
d. Yi = 0 expf1xi1g + i
e. Yi = [1 + expf0 + 1xi1 + ig]􀀀1
ii. Consider the toy data set:
The data set is provided in the le HW4Problem3.txt on canvas. Use multiple linear
regression techniques to t a polynomial to the above data set. To receive full credit,
2
y 2.44 8.36 98.33 115.06 128.91 123.46 148.30 138.10 153.10 119.08
87.66 134.88 91.71 126.81 40.41 54.94 33.03 35.74 14.99 -1.18
2.44 8.36 28.33 45.06 48.91 43.46 118.30 108.10 233.10 199.08
337.66 384.88
x 0.00 0.00 1.00 1.00 2.00 2.00 3.00 3.00 4.00 4.00
5.00 5.00 6.00 6.00 7.00 7.00 8.00 8.00 9.00 9.00
10.00 10.00 11.00 11.00 12.00 12.00 13.00 13.00 14.00 14.00
15.00 15.00
write down the estimated model and create a scatter plot with the estimated curve
overlaid on the plot.","Generalized Linear Regression, Multiple Linear Regression, data plot"
lr_ll_hw4_p4,"The Tri-City Oce Equipment Corporation
sells an imported copier on a franchise basis and performs preventive maintenance and
repair service on this copier. The data have been collected from 45 recent calls on users to
perform routine preventive maintenance service; for each call, x is the number of copiers
serviced and Y is the total number of minutes spent by the service person. The data set
HW4Problem4.txt is posted on canvas.
Use R to perform the following tasks:
i. Obtain the estimated regression function.
ii. Create a scatterplot of the data set with the line of best t overlaid on the graph.
Create a QQ plot of the studentized deleted residuals, histogram of the studentized
deleted residuals, line plot of the studentized deleted residuals, plot the studentized
deleted residuals verses predicted values ^y, and studentized residuals verses predictor
variable x. Based on the plots, discuss whether any of the regression assumptions have
been violated. In your descriptions, relate your explanations to the relevant plots.
iii. Perform a Box-Cox procedure on the data set. What is the estimated value of ? Is it
necessary to perform this transformation on the response variable? Brie
y explain your
reasoning.","Multiple Linear Regression, Diagnostics, data plot, residual plot, Box-Cox procedure,"
lr_ll_hw4_p5,"Observations on Y are to be taken when x = 10; 20; 30; 40; and 50, respectively. The true
regression function is E[Y ] = 20 + 10x. The error terms are independent and normally
distributed with E[i] = 0 and V ar[i] = :8x.
i. Generate a random Y observation for each x level and calculate both the ordinary and
weighted least squares estimates of the regression coecient 1 in the simple linear
3
regression function.
ii. Repeat part (a) 10,000 times, generating new random numbers each time.
iii. Calculate the mean and variance of the 10,000 ordinary least squares estimates of 1
and do the same for the 10,000 weighted least squares estimates.
iv. Do both the ordinary least squares and weighted least squares estimators appear to be
unbiased? Explain. Which estimator appears to be more precise here? Comment.","Simple Linear Regression, parameter, residual, weighted Estimation, Estimation"
lr_ll_hw5_p1,"An assistant in the district sales oce of a national cosmetics rm obtained data on advertis-
ing expenditures and sales last year in the district's 44 territories. X1 denotes expenditures
for point-of-sale displays in beauty salons and department stores (in thousand dollars), and
X2 and X3 represent the corresponding expenditures for local media advertising and pro-
rated share of national media advertising, respectively. Let Y denote sales (in thousand
cases). The assistant was instructed to study the in
uence variables X1 and X2 have on
sales Y . The data set CosmeticsSales.txt is posted on Canvas.
Use R to perform the following tasks:
i. Run the simple linear regression Y  X1. Test if expenditures for point-of-sale displays
in beauty salons and department stores (X1) statistically in
uences sales (Y ).
ii. Run the simple linear regression Y  X2. Test if expenditures for local media advertis-
ing (X2) statistically in
uences sales (Y ).
iii. Now run the the full regression Y  X1 + X2 + X3. Perform marginal t-tests to see
if X1 statistically in
uences sales (Y ) and if X2 statistically in
uences sales (Y ), after
controlling for the variance of X3. Brie
y compare the results to Parts i. and ii. and
comment on any discrepancies. Note: No need for Bonferroni.
iv. You should have noticed some discrepancies in Part iii. Explain why these discrepan-
cies are occurring and provide graphical or exploratory evidence to complement your
argument.","Simple Linear Regression, Multiple Linear Regression, Hypothesis Testing, t test, multicollinearity"
lr_ll_hw5_p2,"In this problem, we compare dierent variable selection methods. We study the Credit
data set, which can be downloaded from CourseWorks. The data set records balance
(average credit card debt) as well as several quantitative predictors: age, cards (number
of credit cards), education (years of education), income (in thousands of dollars), limit
(credit limit), and rating (credit rating). There are also four qualitative variables: gender,
1
student (student status), status (marital status), and ethnicity (Caucasion, African
American or Asian). We want to t a regression model of balance on the rest of the
variables.
 (Best subset selection) The regsubsets() function in R (part of the leaps library)
performs the best subsec selection by indentifying the best model that contains a given
number of predictors, where best is dened to be the one which minimizes the residual
sum-of-squares (RSS).
Here we need to represent the qualitative predictors by dummy variables. gender,
student and ethnicity are all two-level categorical variables, and each of them is
coded by one dummy variable. ethnicity takes on tree values and is coded by two
dummy variables. Therefore, we have 11 predictors in total.
 (Forward stepwise selection) We can also use the regsubsets() function to perform
forward stepwise selection, using the argument method=``forward''.
 (Backward stepwise selection) The regsubsets() function can be used to perform
backward stepwise selection as well ( method=``backward''). Here we start from the
full model and at each step remove a predictor which leaves a model having smallest
RSS.
 (Choosing the optimal model) After obtaining a sequence of models by using the sub-
sect selection approaches, we will choose a single best model which minimizes the
prediction error. For this problem, we use Cp or BIC statistc as estimates of the
prediction error. Cp statistic is dened by
Cp =
1
n
(RSS + 2p^2);
where ^2 is an estimate of the variance of the error term. BIC is dened by
BIC =
1
n
(RSS + log(n)p^2):
The summary() fuction returns RSS, Cp and BIC. You DO NOT need to compute
them by yourselves.
Homework problems.
i. Apply the three subset selection methods mentioned above to Credit data set. Plot the
RSS as a function of the number of variables for these three methods in the same gure.
ii. Each subsect selection method selects a sequence of models. For each approach, choose
a single optimal model by using Cp and BIC statistics respectively. Report the optimal
models for each approach (i.e. specify the predictors in the optimal model).","Remedy, variable selection, sum of squares, "
lr_ll_mid_p1,"Consider the simple linear regression model
(1) Yi = 0 + 1xi + i;  = 1; : : : ; n; i
iid  N(0; 2);
and least squares estimators
^ 1 =
Sxy
Sxx
and ^ 0 =  Y 􀀀 ^ 1x:
For this problem, you can use the following results:
(2) E[ ^ 0] = 0; E[ ^ 1] = 1; V ar[ ^ 0] = 2

1
n
+
x2
Sxx

; V ar[ ^ 1] =
2
Sxx
:
For this exercise, use the scalar form of the simple linear regression model, i.e.,
don't use matrices.","Simple Linear Regression, Estimation"
lr_ll_mid_p1_a,"Under model (1), prove that ^ 0 􀀀 ^ 1 is an unbiased estimator of 0 􀀀1. Note that you can
directly use the relations from (2).","parameter, estimation"
lr_ll_mid_p1_b,"Under model (1), derive an expression for Cov( ^ 0; ^ 1), where ^ 0 and ^ 1 are the least squares
estimators. Simplify the result as much as possible. Note that you can directly use the
relations from (2).","covariance, Estimation"
lr_ll_mid_p1_c,"Under model (1), derive an expression for V ar[ ^ 0 􀀀 ^ 1], where ^ 0 and ^ 1 are the least
squares estimators. Simplify the result as much as possible. Note that you can directly use
the relations from (2).
Note: if you cannot complete Part B, then express the solution to Part C in
terms of Cov( ^ 0; ^ 1).","Estimation, parameter"
lr_ll_mid_p1_d,"Although not a very useful or common approach, we now consider a testing procedure to see
if the intercept statistically diers from the slope, i.e., consider testing the null/alternative
pair
H0 : 0 = 1 versus HA : 0 6= 1:
Write down both the T-statistic and F-statistic for testing the above null/alternative pair.
When constructing the F-statistic, also identify the full and reduced models. Write your
solution on pages 4 and 5.
Note: when specifying the full and reduced models, you do not have to derive
the maximum likelihood estimators but make sure to identify them.","Hypothesis Testing, t test, F test,"
lr_ll_mid_p1_e,"Consider the following toy dataset displayed in the scatter plot below. Let the predictor
variable be assigned as x, the response as Y and assign n as the sample size. Note that there
are n = 100 cases in this dataset.Using the R code and output displayed on pages 6, 7 and 8, test the if the intercept statis-
tically diers from the slope, i.e., test the null/alternative pair:
H0 : 0 = 1 versus HA : 0 6= 1:
To receive full credit, compute both the T-statistic and F-statistic for testing the above
null/alternative pair. Also compute the correct p-value and state the statistical conclusion.
Write the solution on the top of page 7.
Note:
1-pt(t.calc,98)=0.1185218 1-pf(f.calc,1,98)=0.2370437
1-pt(t.calc,99)=0.1185074 1-pf(f.calc,1,99)=0.2370147","data plot, Hypothesis Testing, t test, F test"
lr_ll_mid_p2,"Consider the following study examining the eects of dierent amounts of THC, the major
ingredient in marijuana, injected directly in the brain. The response variable (Y ) is locomo-
tor activity. In this approach, the researchers run an ANCOVA model (or multiple linear
regression model) on the pos-injection scores, partialling out pre-injection dierences. Such
a procedure would adjust for the fact that much of the variability in post-injection activity
could be accounted for by the variability in pre-injection activity. Note that variables D1
through D4 are indicator variables representing the dierent dosage levels and xi is the
continuous variable pre-injection activity.
control .1 micro g (D1) .5 micro g (D2) 1 micro g (D3) 2 micro g (D4)
X Y X Y X Y X Y X Y
Pre Post Pre Post Pre Pos Pre Pos Pre Pos
4.34 1.30 1.55 0.93 7.18 5.10 6.94 2.29 4.00 1.44
3.50 0.94 10.56 4.44 8.33 4.16 6.10 4.75 4.10 1.11
...
...
...
...
...
...
...
...
... ...
...
...
...
...
...
...
7.35 2.35
...
...
...
...
...
...
6.30 4.84
...
...
1.90 0.93 9.58 4.22 5.54 2.93
n1 = 10 n2 = 10 n3 = 9 n4 = 8 n5 = 10
The statistical model used in our setting is:
Yi = 0 + 1Di1 + 2Di2 + 3Di3 + 4Di4 + 5xi + i
i = 1; : : : ; 47; i
iid  N(0; 2);
where
Di1 =
(
1 if .1 micro grams
0 if otherwise
; Di2 =
(
1 if .5 micro grams
0 if otherwise
Di3 =
(
1 if 1 micro grams
0 if otherwise
; Di4 =
(
1 if 2 micro grams
0 if otherwise
and xi is the respondent's pre-injection locomotor activity.","Multiple Linear Regression, dummy variable"
lr_ll_mid_p2_a,"Run a single Hypothesis Testing procedure to see if THC dosage levels statistically in
uence
post-locomotor activity, after controlling for pre-locomotor activity. To receive full credit,
state the correct null/alternative pair, compute the test statistic and identify the correct
P-value. To complete this exercise, use the R code & output displayed on Page 11. Assume
 = 0:05 signicance.","Hypothesis Testing, t test, F test,"
lr_ll_mid_p2_b,"Run a Bonferroni procedure to test which THC dosage levels statistically in
uence post-
locomotor activity, after controlling for pre-locomotor activity, i.e., simultaneously test the
null hypotheses H0 : 1 = 0; H0 : 2 = 0; H0 : 3 = 0; H0 : 4 = 0. To receive full
credit, circle the correct R output and brie
y identify which THC dosage levels statistically
in
uence post-locomotor activity. Assume 95% family-wise error rate.",Hypothesis Testing
lr_ll_final_p1,"The article \Truth and DARE: Tracking Drug Education to Graduation"" (Social Problems
[1994]:448-456) compared the drug use of 288 randomly selected high school seniors exposed
to drug education programs (DARE) and 335 randomly selected high school seniors who
were not exposed to such a program. Data for marijuana use are given in the following
table:
Number who Number who do
use marijuana not use marijuana Sample size
Exposed to DARE 141 147 288
Not exposed to DARE 181 154 335
In this setting, we use simple logistic regression to study the relationship between mar-
ijuana usage and whether or not the respondents were exposed to DARE. Both the inde-
pendent and dependent variables are represented as dichotomous, i.e.,
y =
(
1 use marijuana
0 don't use marijuana
x =
(
1 not exposed to DARE
0 exposed to DARE
The maximum likelihood estimate of the vector  =
􀀀
0 1
T
is:
b
=
􀀀
􀀀0:0417 0:2032
T
The Hessian matrix of the log-likelihood function evaluated at bis:
G = G(b
) =

􀀀155:1747 􀀀83:2060
􀀀83:2060 􀀀83:2060
","Logistic regression, dummy variable, Estimation, parameter, covariance, "
lr_ll_final_p1_a,"Fill out the missing entries of the logistic regression summary table displayed on the
next page:
2
Estimate Std. Error z value Pr(> jzj)
(Intercept) 0.724
x 0.207 ","logistic regression, Hypothesis Testing"
lr_ll_final_p1_b,"Interpret the estimated slope ^ 1 in terms of the application. Also construct a 95%
condence interval for 1. Does the program DARE have a signicant impact on
whether or not a respondent uses marijuana? For full credit, write down the correct
null/alternative pair, condence interval and statistical conclusion.
Note: z:05=2 = 1:96",Confidence interval
lr_ll_final_p1_c,"Construct a 95% condence interval for the true proportion of people who have used
marijuana given they were not exposed to DARE.",confidence interval
lr_ll_final_p2,"It is at least part of the folklore that repeated experience with any standardized test leads
to better scores, even without any intervening study. Suppose that we obtain eight subjects
and give them a standardized admissions exam every Saturday morning for 2 weeks. The
data follow:
Subject 1 2 3 4 5 6 7 8
Exam 1 550 440 610 650 400 700 490 580
Exam 2 580 470 610 670 450 710 510 590
Our research question is:
Does the data suggest that the exam scores dier after the repeated trial?
The motivation behind this problem is the repeated measures design, where respondents
are measured multiple times in a statistical experiment. The above example is a very basic
application of repeated measures, which is also equivalent with the paired (not pooled)
two-sample t-test. In our setting, we will use the linear regression model to study this
data set. To do so, consider stacking exam scores (Y) and using an indicator variable for
exam (x1), i.e.,
Y =
􀀀
550 440    580 580 470    590
T
x1 =
􀀀
0 0    0 1 1    1
T
To analyze this dataset correctly, the experimenter should also incorporate subject variabil-
ity into the model to account for how respondents perform dierently on exams.","Multible Linear Regression, t test, dummy variable, "
lr_ll_final_p2_a,"Suppose that we use the simple linear regression model and do not control for subject
variability. In this case, the model is Yi = 0+1xi1+i. Using the R output displayed
on pages 8-9, test if the exam scores statistically changed after the repeated trial. For
full credit, write down the correct null/alternative pair, test-statistic, p-value and
statistical conclusion. Use  = :05","Simple Linear Regression, Hypothesis Testing"
lr_ll_final_p2_b,"Now suppose that we do account for subject variability. Write down the multiple
linear regression model and the correct design matrix for this scenario.","Multiple Linear Regression, design matrix"
lr_ll_final_p2_c,"Using the R output displayed on pages 8-9, test if students performance statistically
diers, after taking into account exam variability. Note: this is not our research
question. For full credit,
a. Write down the correct null/alternative pair.
b. Compute the f-statistic by hand using the general linear f-stat formula. Show
all steps in this calculation.
c. Identify the p-value and write down the statistical conclusion.
Note: use  = :05. Hint: This is a balanced design, i.e., Type I SS = Type III SS.","Hypothesis Testing, F test"
lr_ll_final_p2_d,"Using the R output displayed on pages 8-9, test if the exam scores statistically dier
after the repeated trial, taking into account subject variability. Note: this is our
research question. For full credit, write down the correct null/alternative pair,
test-statistic, p-value and statistical conclusion. Use  = :05",Hypothesis Testing
lr_ll_final_p2_e,"In a few sentences, describe how the results from Part 2.i to Part 2.iv change,
i.e., describe the dierence in signicance in relation to R2, SSE and overall model
specication.","coefficient of determination, sum of square"
lr_ll_final_p2_f,"In this problem part, we compare Type I versus Type III sums of squares. Suppose
we add some noise into our model. This might represent recording the respondents'
height in the study, which clearly doesn't contribute to their test scores. We will
compare the four ANOVA tables shown below:
ANOVA.1: anova(lm(ScoresExam+Student fac))
ANOVA.2: anova(lm(ScoresStudent fac+Exam))
ANOVA.3: anova(lm(Scoresnoise+Exam+Student fac))
ANOVA.4: anova(lm(Scoresnoise+Student fac+Exam))
Notice from the R output on page 11, the Type I SS are the same for ANOVA.1 and
ANOVA.2, regardless of the order that the variables are entered into the model. How-
ever, this relationship does not hold for ANOVA.3 and ANOVA.4.
In a few sentences, describe why the order of the variables changes the Type I SS in
the presence of the continuous noise variable. Please use complete sentences and any
relevant computations to support your argument.","Hypothesis Testing, ANOVA, "
lr_ll_final_p3,"In a small-scale experimental study of the relation between degree of brand liking (Y ) and
moisture content (X1) and sweetness (X2) of the product, the following results were obtained
from the experiment based on a completely randomized design.
Case Y X1 X2
1 64.00 4.00 2.00
2 73.00 4.00 4.00
3 61.00 4.00 2.00
4 76.00 4.00 4.00
5 72.00 6.00 2.00
6 80.00 6.00 4.00
7 71.00 6.00 2.00
8 83.00 6.00 4.00
9 83.00 8.00 2.00
10 89.00 8.00 4.00
11 86.00 8.00 2.00
12 93.00 8.00 4.00
13 88.00 10.00 2.00
14 95.00 10.00 4.00
15 94.00 10.00 2.00
16 100.00 10.00 4.00
Also consider the following quantities:
(XTX)􀀀1XTY =
0
@
37:650
4:425
4:375
1
A
YT 􀀀
I 􀀀
1
n
J

Y = 1967
YT 􀀀
I 􀀀 H

Y = 94:3
(XTX)􀀀1 =
0
@
1:2375 􀀀0:0875 􀀀0:1875
􀀀0:0875 0:0125 0:0000
􀀀0:1875 0:0000 0:0625
1
A",Multiple Linear Regression
lr_ll_final_p3_a,"Fill out the missing entries of the multiple linear regression summary table and stan-
dard ANOVA table, both displayed below:
Summary Table
Estimate Std. Error t value Pr(> jzj)
(Intercept) 1:20  10􀀀8
X1 1:78  10􀀀9
X2 2:01  10􀀀5
ANOVA Table
Df Sum Sq Mean Sq F value Pr(>F)
Regression 2:658  10􀀀9
Residuals BLANK BLANK BLANK
Total BLANK BLANK BLANK",ANOVA
lr_ll_final_p3_b,"Run the relevant test to see if sweetness (X2) is signicantly related to the degree of
brand liking after moisture is held constant. To receive full credit, show all relevant
steps of the testing procedure.","Hypothesis Testing, t test"
lr_ll_final_p3_c,"Consider estimating parameters 0, 1 and 2 simultaneously using a Bonferroni pro-
cedure with familywise condence level 95%. Construct the Bonferoni interval for 2.
Note: In practice I want to construct three intervals but to save time on the exam,
I am having students construct only one of the three intervals. Also note that one of
the critical values below is correct.
t0:025;13 = 2:1604; t0:0125;13 = 2:5326; t0:0083;13 = 2:7459
t0:025;14 = 2:1447; t0:0125;14 = 2:5096; t0:0083;14 = 2:7178",simultaneous inference
lr_ll_final_p4,"Consider the least squares estimated multiple linear regression model
(1) ^Y
= X^
= HY;
where H is the hat-matrix and X is the design matrix of dimensions (np). Using properties
of the hat-matrix, prove that any vector in the column space of X is orthogonal to the
residual vector e.","Multiple Linear Regression, hat matrix, residual, independence"
lr_ll_final_p5,"Suppose that respondents are randomly allocated into two distinct groups of size n1 and
n2. Also suppose that some continuous measurement (Y ) is recorded from each case. In our
setting, we consider the linear model:
(2) Yi = 1xi1 + 2xi2 + i; i = 1; : : : ; n; i
iid  N(0; 2);
where
xi1 =
(
1 Group 1
0 Group 2
xi2 =
(
1 Group 2
0 Group 1
Note that xT1
x2 =
P
xi1xi2 = 0.","Multiple Linear Regression, dummy variable"
lr_ll_final_p5_a,"Using the least squares equation, compute and simplify an expression for b",Estimation
lr_ll_final_p5_b,"Using the least squares equation, compute and simplify an expression for var[b
].","Estimation, estimation"
lr_ll_final_p5_c,"In this setting, is the following identity true?
Xn
i=1
ei =
Xn
i=1
(yi 􀀀 ^yi) = 0
Justify your answer in a few sentences.",residual
lr_hw_hw3_p1,"*3.4, Refer to Copier maimtenance Problem 1.20.
a. Prepare a dot plot for the number of copiers serviced X;. What information is provided by

this plot? Are there any outlying cases with respect to this variable? 1 ~
b. The cases are given in time order. Prepare a time plot for the number of copiers serviced.
What does your plot show?

c. Prepare a stem-and-leaf plot of the residuals. Are there any noteworthy features in this plot?

d. Prepare residual plots of e; versus f; and e; versus X; on separate graphs. Do these plots
provide the same information? What departures from regression model (2.1) can be studied
from these plots? State your findings.

e. Prepare a normal probability plot of the residuals. Also obtain the coefficient of correlation
between the ordered residuals and their expected values under normality. Does the normality
assumption appear to be tenable here? Use Table B.6 and & = .10.

f. Prepare a time plot of the residuals to ascertain whether the error terms are correlated over
time. What is your conclusion?

g. Assume that (3.10) is applicable and conduct the Breusch-Pagan test to determine whether
or not the ertor variance varies with the level of X. Use a = .05. State the alternatives,
decision rule, and conclusion.

h. Information is given below on two variables not included in the regression model, namely,
mean operational age of copiers serviced on the call (Xz, in months) and years of experience
of the service person making the call (X3). Plot the residuals against Xz and X3 on separate

” graphs to ascertain whether the model can be improved by including either or both of these
variables. What do you conclude?

i 1 2 3 43 44 45
Xo 20 19 27 28 26 33
X3: 4 5 4 3 3 6",data plot
lr_hw_hw3_p2,"*3.17. Sales growth. A marketing researcher studied annual sales of a product that had been introduced.
10 years ago. The data are as follows, where X is the year (coded) and ¥ is sales in thousands

Chapter 3 Diagnostics and Remedial Measures 151

of units:

i 1 2 3 4 5 6 7 8 9 10

Xr 0 1 2 3 4 5 6 7 8 9
Yz 98 135 162 178 221 232 283 300 374 395

a. Prepare a scatter plot of the data. Does a linear relation appear adequate here?

b. Use the Box-Cox procedure and standardization (3.36) to find an appropriate power transfor-
mation of Y. Evaluate SSE for ) = .3, 4, .5, .6, .7. What transformation of Y is suggested?

c. Use the transformation ¥’ = \/Y and obtain the estimated linear regression function for the
transformed data.

d, Plot the estimated regression line and the transformed data. Does the regression line appear
to be a good fit to the transformed data?

e. Obtain the residuals and plotthem against the fitted values. Also prepare a normal probability
plot. What do your plots show?

f. Express the estimated regression function in the original units,",data plot
lr_hw_hw4_p1,"*5.25. Refer to Airfreight breakage Problems 1.21 and 5.6.
al Using matrix methods, obtain the following: (1) (X’""X)~', (2) b, (3) e, (4) B, (5) SSE,
(6) s?{b}, (7) ¥, when X, = 2, (8) s?{¥n} when X), = 2.
b. From part (a6), obtain the following: (1) s*{by}; (2) s{Bp, by}; (3) s{bp}.
c. Find the matrix of the quadratic form for SSR.","parameter, estimation, prediction, sum of squares"
lr_hw_hw4_p2,"*6.15. Patient satisfaction, A hospital administrator wished to study the relation between patient
satisfaction (¥) and patient’s age (X), im years), severity of iltness (X2, aut index), and anxiety

Chapter 6 Multiple Regression 251

level (Xs, an index). The administrator randomly selected 46 patients and collected the data
presented below, where larger values of Y, Xp, and X's are, respectively, associated with more
satisfaction, increased severity of illness, and more anxiety.

 

i: 1 2 3 “4 45 46
50 36 40 : 45 37 28
51 46 48 51 53 46
23 23 2.2 22 24 18
48 57 66 68 59 92

   

a. Prepare ster-and-leaf plot for each of the predictor variables. Are any noteworthy features
revealed by these plots?

b. Obtain the scatter plot matrix and the correlation matrix. Interpret these andtstate your

rincipal findings.

¢. Fit regression model (6.5) for three predictor variables to the data and state the estimated
regression function. How is b interpreted here?

d. Obtain the residuals and prepare a box plot of the residuals. Do there appear to be any
outliers?

. Plot the residuals against f, each of the predictor variables, and each two-factor interaction
term on separate graphs. Also prepare a normal probability plot. Interpret your plots and
summarize your findings.

£ Can you conduct a formal test for lack of fit here?

g Conduct the Breusch-Pegan test for constancy of the error variance, assuming log 0? =
WtnXnt+MXatwXis; use @=.01. State the alternatives, decision rule, and
conclusion.","Multiple Linear Regression, data plot, Hypothesis Testing, "
lr_hw_hw5_p1,"*7,5. Refer to Patient satisfaction Problem 6.15.
a. Obtain the analysis of variance table that decomposes the regression sum of squares into
extra sums of squares associated with X2; with X), given X5; and with X3, given X2 and X1.
b. Test whether X3 can be dropped from the regression model given that X, and Xz areretained.
Use the F* test statistic and level of significance .025. State the alternatives, decision rule,
and conclusion. What is the P-yalue of the test?","sum of squares, Hypothesis Testing, F test"
lr_hw_hw5_p2,"*7.6. Refer to Patient satisfaction Problem 6.15. Test whether both X2 and X3 can be dropped from
the regression model given that X, is retained. Use a = .025. State the alternatives, decision
rule, and conclusion. What is the P-value of the test?

2","Hypothesis Testing, F test"
lr_hw_hw6_p1,"8.34. In a regression study, three types of banks were involved, namely, commercial, mutual savings,
and savings and Joan. Consider the following system of indicator variables for type of bank:

Type of Bank Xz X3
Cormmercial 1 0
Mutual savings 0 1
Savings and loan =i -1

. Develop a fitst-order linear regression model for relating last year’s profit or loss (Y) to size

of bank (X1) and type of bank (Xa, X3).

. State the response functions for the three types of banks.
. Interpret each of the following quantities: (1) B2, (2) 63. (3) — B2 — Bs.","dummy variable, Simple Linear Regression, estimation"
lr_hw_hw7_p1,"*.9. Refer to Patient satisfaction Problem 6.15. The hospital administrator wishes to determine the
best subset of predictor variables for predicting patient satislaction.

 

Chapter 9 Building the Regression Model I: Model Selection and Validation 377

a. Indicate which subset of predictor variables you would recommend as best for predicting
patient satisfaction according to each of the following criteria: (1) R? p> (2) AIC,, (3) Cp,
(4) PRESS, . Support your recommendations with appropriate graphs.

b. Do the four criteria in part (a) identify the same best subset? Does this always happen?

c. Would forward stepwise regression have any advantages here as a screening procedure Over
the all-possible-regressions procedure?","Remedy, variable selection"
lr_hw_hw7_p2,"*9.17. Refer to Patient satisfaction Problems 6.15 and 9.9. The Hospital administrator was interested
to learn how the forward stepwise selection procedure and some of its variations would perform
here.

a. Determine the subset of variables that is selected as best by the forward stepwise regression
procedure, using F limits of 3.0 and 2.9 to add or delete a variable, respectively. Show your
steps.

b. To what level of significance in any individual test is the F limit of 3.0 for adding a variable
approximately equivalent here?

c, Determine the subset of variables that is selected as best by the forward selection procedure,
using an F limit of 3.0 to add a variable. Show your steps.

d. Determine the subset of variables that is selected as best by the backward elimination
procedure, using an F limit of 2.9 to delete a variable. Show your steps.

e, Compare the results of the three selection procedures, How consistent are these results?
How do the results compare with those for all possible regressions in Problem 9.9?","Remedy, variable selection"
lr_hw_hw8_p1,"*10.11. Refer to Patient satisfaction Problem 6.15.

a. Obtain the studentized deleted residuals and identify any outlying Y observations. Use the
Bonferroni outlier test procedure with a = .10, State the decision rule and conclusion.

b. Obtain the diagonal elements of the hat matrix. Identify any outlying X observations.

 

wo Multiple Linear Regression

d.

Hospital management wishes to estimate mean patient satisfaction for patients who ate
X, = 30 years old, whose index of illness severity is Xz = 58, and whose index of anxiety
level is X; = 2.0. Use (10.29) to determine whether this estimate wil! involve a hide,
extrapolation,

The three largest absolute studentized deleted residuals ure for cases 14.17, and 27. Obtain
the DFFITS, DFBETAS, and Cook’s distance values for this case to assess its influence
What do you conclude?

Calculate the average absolute percent difference in the fitted values with and without each
of these cases. What does this measure indicate about the influence of eaclt of these cases
Calculate Cook’s distance Dj for each case and prepare an index plot. Are any cases
influential according to this measure?","Diagostic, outlier, prediction, variable selection, "
lr_hw_hw8_p2,"IL.1.

11.2.

11.3.

114.

ILS.

One student remarked to another: “Your residuals show that nonconstancy of error variange
is clearly present. Therefore, your regression results are completely invalid.” Comment,

An analyst suggested: “One nice thing about robust regression is that you need not Wony
about outliers and influential observations.” Comment.

Lowess smoothing becomes difficult when there are many predictors and the sample size is
small. This is sometimes referred to as the “curse of dimensionality.” Discuss the nature of
this problem.

Regression trees become difficult to utilize when there are many predictors and the sample
size is small. Discuss the nature of this problem.

Describe how bootstrapping might be used to obtain confidence intervals for regression coef.
ficients when ridge regression is employed.","Diagostic, bootstrap, robust refression, regression trees"
lr_hw_mid2_p1_a,"(a) Letting Ri denote the coefficient of determination for the regression y ~ x1, and R3
denote that for y ~ x2, and R?, that for y ~ x1 + x2, we must have R}, > max{R7, R3}.",coefficient of determination
lr_hw_mid2_p1_b,"(b) Letting a denote the estimated residual variance for the regression y ~ x1, and aS
that for y ~ x2, and G25 that for y ~ x1 + x2, it is necessarily the case that G25 >
max{6?, a3}.",estimation
lr_hw_mid2_p1_c,"(c) Researchers wishing to study the relationship between cholesterol and patient height
and weight consider a regression model with mean function E(Y|X = x) = $9 + Bix,
where Y = LDL cholesterol in mg/dL, and X = BMI = weight in kg /(height in m)?; in
the terminology of this course, BMI is the predictor, and height and weight are the two
regressors.",Simple Linear Regression
lr_hw_mid2_p1_d,"(d) Given n observations from a model with the mean function
E(Y|X1 = %,...,Xp = ap) = Bot Siti +... + bpp,

the vector of responses Y = (¥j,...,Y;)/ can be written Y = X +e, where X is nx p,
Bispx1, and Ee) =0.",Multiple Linear Regression
lr_hw_mid2_p1_e,"(e) Consider the multiple linear regression model Y = X G+e, where E(e) = 0 and Var(e) =
o7I. As long as X has full column rank, the least squares estimates B are unique
and satisfy E(@) = 9 and Var(8) = o2(X'X)~!; if in addition that the error term is
multivariate normal, then the sampling distribution of B is multivariate normal as well.","Multiple Linear Regression, estimation, standardization"
lr_hw_mid2_p1_f,"(f) If the effect of the predictor variable X2 differ depending on the value of the predictor
variable X, (and the effect of X1 varies depending on the value of X2), then the two pre-
dictor variables are said to have an interaction effect; one common approach to modeling
interaction is to include a product term (X,X2) as a regressor.","Diagnostic, Generalized Linear Regression"
lr_hw_mid2_p2,"(10 points) Measurements were made on n = 251 men in order to relate the percentage of
body fat determined by underwater weighing (bodyfat), which is inconvenient and costly to
‘obtain, to abdomen circumference in em (abdomen) and hip circumference in em (Bip), both
recorded using only measuring tape.

 

 

8
®

bodyfat

 

 

 

abdomen,

  

 

 

hip

 

 

 

 

 

 

 

 

 

Partial summary.2a() R output for bodyfat ~ abdozen + hip:

> summary (23)

 

 

call:
In(fermula = bodyfat ~ abdomen + hip)
Coefficients:

Estimate Std. Error t value Pr(>Itl)
(Intercept) -26.92937 5.02802 ~5.187 .146-07
abdomen 0.86159 0.08549 15.628 < 20-16
hip -0.34637 0.08709 -3.977 9.162-05

Residual standard error: 4,583 on 248 degrees of freedon
Multiple R-squared: 0.6993, Adjusted R-squared: 0.6968
Festatistic: 288.3 on 2 and 248 DF, p-value: < 2.2e-16","Multiple Linear Regression, correlation"
lr_hw_mid2_p2_a,"(a) (5 points) Briefly (30 to 40 words max) summarize the information in the scatterplot
matrix.",data plot
lr_hw_mid2_p2_b,"(b) (5 points) Do you think adjusted R-squared is a better metric for evaluating the fitness of
multiple regression models than multiple R-Squared? Briefly explain why? In addition,
why is the adjusted R-squared is smaller than multiple R-Squared?",coefficient of determination
lr_hw_mid2_p3,"(15 points) A study was conducted to develop predictive equations for lean body weight, a
measure of men’s health. Specifically, measurements were made on n = 251 men in order to
relate the percentage of bodyfat determiend by underwater weighing (bodyfat) to abdomen
circumference in cm (abdomen), and hip circumference in cm (hip). The fitted mean function
is

E(bodyfat|abdomen, hip) = —25.93 + 0.86 abdomen — 0.35 hip.",Multiple Linear Regression
lr_hw_mid2_p3_a,"(a) (5 points) Give the coefficients in the estimated mean function if abdomen and hip cir-
cumference had been measured in inches. You are reminded that one inch equals 2.54
cm.",estimation
lr_hw_mid2_p3_b,"(b) (5 points) Carefully interpret the estimated coefficient of hip in the multiple regression
model (in cm).",estimation
lr_hw_mid2_p3_c,"(c) (5 points; students in 5205 only) Define the variables
mean = (abdomen + hip)/2 and diff = abdomen — hip

where abdomen and hip are both measured in cm. Give the estimated coefficients in the
mean function

E(bodyfat|abdomen, hip) = 49+; mean+ fp diff.",estimation
lr_hw_mid2_p4,"(30 points) Consider the multiple linear regression with response body fat percentage,
predictor variables triceps skin fold thickness (X1), thigh circumference (X2) and
midarm circumference (X3). The following ANOVA table is obtained:

 

Source SS df MS F

 

Regression (SSR(X1,Xo,X3)) 396.98
SSR(X1) 352.27
SSR(Xo|X1) 33.17

SSR(X3|X1, X2)
Error 98.41 16

Total 495.39 19","Multiple Linear Regression, sum of squares,"
lr_hw_mid2_p4_a,"(a) (5 points) Fill in the above ANOVA table. In the above table,for the F column, only fill
in the F statistic needed in part (d).","ANOVA, F test"
lr_hw_mid2_p4_b,"(b) (5 points) What is the extra sum of squares by adding X3 to the model given X; and X2
are already in the model?",sum of squares
lr_hw_mid2_p4_c,"(c) (5 points) Briefly explain how extra sum of squares can be understood from error sum of
squares and regression sum of squares perspectives?",sum fo squares
lr_hw_mid2_p4_d,"(a) (10 points) Test if X3 (midarm circumference) can be dropped from the model at sig-
nificance level 0.01? Given F(0.99, 1,16) = 8.543. Please state the null and alternative
hypothesis.","F test, Hypothesis Testing"
lr_hw_mid2_p4_e,"(ce) (6 points) What would be the proper alternative hypothesis for the test of the null hy-
pothesis Ho : 3) = 83 = 0? Please also find the value of the test statistic.",Hypothesis Testing
lr_hw_mid_sample_p1,"Suppose that y; = 80 + 61a; + ¢;, where x;’s are constant and ¢;’s are independent normal random variables
with mean 0 and variance o?. Compute the likelihood at (89, 31) = (0.1, 1) when we have the following data.

observation 1 2 3
x 1 2 3
y 15 2 2","Simple Linear Regression, estimation"
lr_hw_mid_sample_p2,"Suppose that the regression model is yj = 39 + 614%; + €;. We have n = 6 observations. The summary
statistics are as follows: 7 yi = 8.5, 0 vi = 6, x? = 16, So aiys = 15.5, Dy? = 17.25.

(a) Compute the least square point estimates of Jo and (4.

(b) Calculate SSE and MSE.

(c) Use a 5% level of significance to conduct the test of Ho : 3; = 0 vs Hy : 3; #0. Remark: t(0.95, 4) =
2.132, t(0.975, 4) = 2.776.","Simple Linear Regression, Estimation, sum of square, Hypothesis Testing, confidence interval"
lr_hw_mid_sample_p3,"(a) (T/F) Suppose that the 95% confidence interval of 3; is found to be [1,2]. Then, the probability of 31
lies in this interval is 0.95.

(b) (T/F) A useful tool for assessing the appropriateness of model assumptions is a residuals versus fitted
values plot; if the model assumptions hold, this should resemble a null plot.","confidence interval, residual plot"
lr_ALRM_p1,"For the matrix:

a= [53]

find the quadratic form of the observations Y; and Y>.",Linear Algebra
lr_ALRM_p2,"For conducting statistical tests concerning the parameter 84, why is the f test more versatile than
the F test?","t test, F test"
lr_ALRM_p3,"Home computers. A computer manufacturer hired a market research firm to investigate the
relationship between the likelihood a family will purchase a home computer and the price of the
home computer. The data that follow are based on replicate surveys done in two similar cities.
(One thousand heads of households in each city were randomly selected and asked if they would
be likely to purchase a home computer at a given price. Eight prices (x, in dollars) were studied,
and 100 heads of households in each city were randomly assigned to a given price. The
proportion likely to purchase at a given price is denoted by ¥.

ity A
ao203 4 5 6 7 8

200 400 800 1200 1600 2000 3000 4000
65 A 34 260 TS GH

 

City B
i: 9 0 MU 2 Bu 6

Xi: 200 400 800 1200 1600 2000 3000 4000

Yr 63 SO 302419120
No location effect is expected and the data are to be treated as independent replicates at each of
the 8 prices. The following exponential model with independent normal error terms is deemed to
be appropriate

Y= y+ nexp(—nXi) +6)
a. To obtain initial estimates of yo, v4. and yo, note that f(X, y) approaches a lower asymptote Yo
as X increases without bound. Hence, let _¢{® = 0 and observe that when we ignore the error
term, a logarithmic transformation then yields Y/ = By + B; X;, where Y/ = log, ¥i,

Bo = log, y2, and ; = — yy. Therefore, fit a linear regression function based on the

transformed data and use as initial estimates g — 0, g = —b,, and gS” = exp(bo)

 

 

. Using the starting values obtained in part (a), find the least squares estimates of the
parameters Yo, v3, and yz","Generalized Linear Regression, estimation, least squares estimaion"
lr_ALRM_p4,"Observations on Y are to be taken when X= 10, 20, 30, 40, and 50, respectively. The true
regression function is E{Y} = 20 + 10X. The error terms are independent and normally
distributed, with E{e} = 0 and o{e} = .8X;.

a. Generate a random Y observation for each X level and calculate both the ordinary and
weighted least squares estimates of the regression coefficient 8; in the simple linear regression
function.

b. Repeat part (a) 200 times, generating new random numbers each time.

c. Calculate the mean and variance of the 200 ordinary least squares estimates of 8; and do the
same for the 200 weighted least squares estimates.

d. Do both the ordinary least squares and weighted least squares estimators appear to be
unbiased? Explain. Which estimator appears to be more precise here? Comment.","lSimple Linear Regression, estimation, weighted Estimation, Estimation"
lr_ALRM_p5,"For the matrices below, obtain (1) A +B, (2) AB, (3) AC, (4) AB’ (5) BA

“EY +f] ote

State the dimension of each resulting matrix.",Linear Algebra
lr_ALRM_p6,"A student in accounting enthusiastically declared: “Regression is a very powerful tool. We can
isolate fixed and variable costs by fitting a linear regression model, even when we have no data
for small lots.” Discuss.",regression
lr_ALRM_p7,"Toxicity experiment. In an experiment testing the effect of a toxic substance, 1,500
experimental insects were divided at random into six groups of 250 each. The insects in each
group were exposed to a fixed dose of the toxic substance. A day later, each insect was
observed. Death from exposure was scored 1, and survival was scored 0. The results are shown
below; Xj denotes the dose level (on a logarithmic scale) administered to the insects in group j

and Y,, denotes the number of insects that died out of the 250 (n) in the group.

pd 2 3 4 3 6

x 1 2 3 4 5 6
ni 250-280 250250 250250
Y 2% «539318197

Logistic regression model (14.20) is assumed to be appropriate.

a. Plot the estimated proportions p) = ¥/n) against Xj. Does the plot support the analyst's belief
that the logistic response function is appropriate?

. Find the maximum likelihood estimates of By and 8}. State the fitted response function.

c. Obtain a scatter plot of the data with the estimated proportions from part (a), and superimpose
the fitted logistic response function from part (b). Does the fitted logistic response function
appear to fit well?

d. Obtain exp(b4) and interpret this number.

©. What is the estimated probability that an insect dies when the dose level is X = 3.5?

1. What is the estimated median lethal dose—that is, the dose for which 50 percent of the
experimental insects are expected to die?","Generalized Linear Regression, estimation, least squares estimaion, Estimation, data plot, prediction, "
lr_ALRM_p8,"Lung pressure. Increased arterial blood pressure in the lungs frequently leads to the,
development of heart failure in patients with chronic obstructive pulmonary disease (COPD). The
standard method for determining arterial lung pressure is invasive, technically difficult, and
involves some risk to the patient. Radionuclide imaging is a noninvasive, less risky method for
estimating arterial pressure in the lungs. To investigate the predictive ability of this method, a
cardiologist collected data on 19 mild-to-moderate COPD patients. The data that follow on the
next page include the invasive measure of systolic pulmonary arterial pressure (Y) and three
potential noninvasive predictor variables. Two were obtained by using radionuclide imaging—
emptying rate of blood into the pumping chamber of the heart (X;) and ejection rate of blood
pumped out of the heart into the lungs (X2)—and the third predictor variable measures a blood
gas (%3).

a. Prepare separate dot plots for each of the three predictor variables. Are there any noteworthy
features in these plots? Comment.

. Obtain the scatter plot matrix. Also obtain the correlation matrix of the X variables. What do the
scatter plots suggest about the nature of the functional relationship between Y and each of the.
predictor variables? Are any serious multicollinearity problems evident? Explain

c. Fit the multiple regression function containing the three predictor variables as first-order terms.
Does it appear that all predictor variables should be retained?

 

Subject
i x Xa Xa %

1 45 3600 4549

2 30 28 40 55

3 un 16 420 BS

7 27 st 49
18 37 32 54 40
19 34 40 3600

Adapted from A. T. Marmor et al., “Improved Radionuclide Method for Assessment of Pulmonary
Artery Pressure in COPD.” Chest 89 (1986), pp. 64-69.","Multiple Linear Regression, data plot, Hypothesis Testing"
lr_ALRM_p9,"The true regression model is ¥;= 10 + 24X; + &, where &= 8; + U; and u,are independent
NO. 25)

a. Generate 11 independent random numbers from N(0, 25). Use the first random number as £9,
obtain the 10 error terms ¢4,... £49, and then calculate the 10 observations Yj,.... Yo
corresponding to X4 = 1, Xp = 2..... Xjq = 10. Fit linear regression function by ordinary least
squares and calculate MSE.

b. Repeat part (a) 100 times, using new random numbers each time.

c. Calculate the mean of the 100 estimates of b. Does it appear that b, is an unbiased estimator
of By despite the presence of positive autocorrelation?

d. Calculate the mean of the 100 estimates of MSE. Does it appear that MSE is a biased
estimator of 0? If so, does the magnitude of the bias appear to be small or large?","Diagnostic, constant variance, estimation, sum fo square, Estimation"
lr_ALRM_p10,"Find the inverse of each of the following matrices:

432
a=} i] B=|6 5 10
: 1 6

Check in each case that the resulting matrix is indeed the inverse.",Linear Algebra
lr_ALRM_p11,"In forward stepwise regression, why should the a-to-enter value for adding variables never
exceed the a-to-remove value for deleting variables?","diagnostic, variable selection"
lr_ALRM_p12,"Assessed valuations. A tax consultant studied the current relation between selling price and
assessed valuation of one-family residential dwellings in a large tax district by obtaining data for
a random sample of 16 recent “arm’s-length’ sales transactions of one-family dwelling located on
corner lots and for a random sample of 48 recent sales of one-family dwellings not located on
comer lots. In the data that follow, both selling price (Y) and assessed valuation (X;) are
expressed in thousand dollars, whereas lot location (Xp) is coded 1 for comer lots and 0 for non-
corner lots.

1 2 3 626364
Fa yea 3S ATO

° 0 0 ° 0 1
788 738 646 ... 976 844 705

     

Assume that the error variances in the two populations are equal and that regression model
(8.49) is appropriate.

a. Plot the sample data for the two populations as a symbolic scatter plot. Does the regression
relation appear to be the same for the two populations?

. Test for identity of the regression functions for dwellings on comer lots and dwellings in other
locations; control the risk of Type | error at .05. State the alternatives, decision rule, and
conclusion.

c. Plot the estimated regression functions for the two populations and describe the nature of the
differences between them.","Multiple Linear Regression, data plot"
lr_ALRM_p13,"A student asked: “Why is it necessary to perform diagnostic checks of the fit when R” is large?”
Comment,","Diagnostic, coefficient of determination"
lr_ALRM_p14,"Production time. In a manufacturing study, the production times for 111 recent production runs
were obtained. The table below lists for each run the production time in hours (Y) and the
production lot size (X).

 

i: 1 23 wo
x 1 8 7 2 9 (45
Yr 1428 880 1249 1637 1145.15.78

a. Prepare a scatter plot of the data. Does a linear relation appear adequate here? Would a
transformation on X or Y be more appropriate here? Why?

b. Use the transformation X’= ./X and obtain the estimated linear regression function for the
transformed data.

c. Plot the estimated regression line and the transformed data. Does the regression line appear
to be a good fit to the transformed data?

d. Obtain the residuals and plot them against the fitted values. Also prepare a normal probability
plot. What do your plots show?

e. Express the estimated regression function in the original units.","Simple Linear Regression, data plot, Generalized Linear Regression, residual plot"
lr_ALRM_p15,"Derive the relations between the 8, and ; in (7.46a) for p

Step-by-step solution

Step 1016 ~

Consider the general linear regression model, with normal error terms, simply in terms of X
variables given below:

Y= Bot BX y+ BX +
Here, fy. B,. Br»

¢, are independent with (0,07), and i=1,2,

 

Bp Xipr *&

 

»B,..are parameters, X,,X,,--+++ ,B,.,ar known constants",Multiple Linear Regression
lr_ALRM_p16,"Consider the simultaneous equations:

 

a. Write these equations in matrix notation.

b. Using matrix methods, find the solutions for y; and ya",Linear Algebra
lr_ALRM_p17,"Sales growth. A marketing researcher studied annual sales of a product that had been
introduced 10 years ago. The data are as follows, where X is the year (coded) and Yis sales in
thousands of units:

1 2 3 4 5 6 7 8 9 Ww

ee
Ys 98 135 162 178 221-232 -283«300«374 395

a. Prepare a scatter plot of the data. Does a linear relation appear adequate here?

b. Use the Box-Cox procedure and standardization (3.36) to find an appropriate power
transformation of ¥. Evaluate SSE for A= 3, .4, .5, 6, .7. What transformation of Y is suggested?

c. Use the transformation y’ = /F and obtain the estimated linear regression function for the
transformed data

d. Plot the estimated regression line and the transformed data. Does the regression line appear
to be a good fit to the transformed data?

€. Obtain the residuals and plot them against the fitted values. Also prepare a normal probability
plot. What do your plots show?

1. Express the estimated regression function in the original units.","Simple Linear Regression, data plot, Box-Cox procedure, standardization, Generalized Linear Regression, "
lr_ALRM_p18,"Ina test of the altematives Ho: 8 <0 versus H: 8; > 0, an analyst concluded Ho. Does this
conclusion imply that there is no linear association between X and ¥ ? Explain",Hypothesis Testing
lr_ALRM_p19,"The effects of five dose levels are to be studied in a completely randomized design, and 20,
experimental units are available. Each dose level is to be assigned to four experimental units
selected at random. Use a table of random digits or a random number generator to make an
appropriate randomization of assignments.","Multiple Linear Regression, experimental design"
lr_ALRM_p20,"A person asks if there is a difference between the “mean response at X = X;,” and the “mean of m
new observations at X = Xp.” Reply.","prediction, estimation"
lr_ALRM_p21,"‘A member of a student team playing an interactive marketing game received the following
computer output when studying the relation between advertising expenditures (X) and sales (Y)
for one of the team’s products:

Estimated regression equation: Y= 350.7 - 18x
Two-sided P-value for estimated slope: 91

The student stated: “The message | get here is that the more we spend on advertising this
product, the fewer units we sell!” Comment.",Hypothesis Testing
lr_ALRM_p22,"Weight and height. The weights and heights of twenty male students in a freshman class are
recorded in order to see how well weight (Y, in pounds) can be predicted from height (x, in
inches). The data are given below. Assume that first-order regression (1.1) is appropriate

 

1 2 3 E 18 1 20
x 74 65, 72 69 68 67
Yi 185195216 7 145137

a. Fita simple linear regression model using ordinary least squares, and plot the data together
with the fitted regression function. Also, obtain an index plot of Cook's distance (10.33). What do
these plots suggest?

. Obtain the scaled residuals in (11.47) and use the Huber weight function (11.44) to obtain case
Weights for a frst iteration of IRLS robust regression. Which cases receive the smallest Huber
weights? Why?

c. Using the weights calculated in part (b), obtain the weighted least squares estimates of the
regression coefficients. How do these estimates compare to those found in part (a) using
ordinary least squares?

d. Continue the IRLS procedure for two more iterations. Which cases receive the smallest
weights in the final iteration? How do the final IRLS robust regression estimates compare to the
ordinary least squares estimates obtained in part (a)?","Simple Linear Regression, data plot, robust regression, Estimation"
lr_ALRM_p23,"Ifthe true response function is J-shaped when the response variable is binary, would the use of
the logistic response function be appropriate? Explain.",dummy variable
lr_ALRM_p24,Explain in what sense the regression sum of squares SSR(X;) is an extra sum of squares.,sum of squares
lr_ALRM_p25,"Distinguish between (1) residual and semistudentized residual, (2) E{e} = 0 and é = 0, (3) error
term and residual","diagnostic, residual"
lr_ALRM_p26,"Consider the following functions of the random variables Y;, Y>, and Y3

Wanthths
W=""-%r
Wy=""-h-%

a. State the above in matrix notation.

. Find the expectation of the random vector W.

c. Find the variance-covariance matrix of W.","Linear Algebra, covariance "
lr_ALRM_p27,"Commercial properties. A commercial real estate company evaluates vacancy rates, square
footage, rental rates, and operating expenses for commercial properties in a large metropolitan
area in order to provide clients with quantitative information upon which to make rental decisions.
The data below are taken from 81 suburban commercial properties that are the newest, best
located, most attractive, and expensive for five specific geographic areas. Shown here are the
age (X;), operating expenses and taxes (Xp), vacancy rates (X3), total square footage (X4), and

rental rates (Y).

   

   
  

 

 

  

a. Prepare a stem-and-leaf plot for each predictor variable. What information do these plots
provide?

. Obtain the scatter plot matrix and the correlation matrix. Interpret these and state your
principal findings.

c. Fit regression model (6.5) for four predictor variables to the data. State the estimated
regression function

d. Obtain the residuals and prepare a box plot of the residuals. Does the distribution appear to be
fairly symmetrical?

e. Plot the residuals against Y, each predictor variable, and each two-factor interaction term on
separate graphs. Also prepare a normal probability plot. Analyze your plots and summarize your
findings.

1. Can you conduct a formal test for lack of fit here?

Q. Divide the 81 cases into two groups, placing the 40 cases with the smallest fitted values ¥; into

group 1 and the remaining cases into group 2. Conduct the Brown-Forsythe test for constancy of
the error variance, using a = .05. State the decision rule and conclusion","Multiple Linear Regression, data plot, Hypothesis Testing, "
lr_ALRM_p28,"Find the inverse of the following matrix:

13
A=|4 05
9 6

Check that the resulting matrix is indeed the inverse",Linear Algebra
lr_ALRM_p29,"A person states that bp and b, in the fitted regression function (1.13) can be estimated by the
method of least squares. Comment.",Estimation
lr_ALRM_p30,"Drug concentration. A pharmacologist employed linear regression model (2.1) to study the
relation between the concentration of a drug in plasma (Y) and the log-dose of the drug (X). The
residuals and log-dose levels follow.

 

1 23 Ss 6 7 8 8
Xo 0 o 1 4 0 4
e oS 2-34 7 420-6 26 -40

 

a. Plot the residuals e, against X;. What conclusions do you draw from the plot?

b. Assume that (3.10) is applicable and conduct the Breusch-Pagan test to determine whether or
not the error variance varies with log-dose of the drug (X). Use a= .05. State the alternatives,
decision rule, and conclusion. Does your conclusion support your preliminary findings in part (a)?","Simple Linear Regression, residual plot, Hypothesis Testing"
lr_ALRM_p31,"For each of the following response functions, indicate whether itis a linear response function, an
intrinsically linear response function, or a nonlinear response function. In the case of an
intrinsically linear response function, state how it can be linearized by a suitable transformation:

a. 7(K, ¥) = explo + V4)

Db. FRKy) = wt nO)"" — yaX2

ce SKY) nthe",Generalized Linear Regression
lr_ALRM_p32,"For each of the following regression models, indicate whether it is a general linear regression
mode.. If itis not, state whether it can be expressed in the form of (6.7) by a suitable

transformation:
a. Bo + BiXes + Br logo Xiz + AsXh + 61
b. 5) €XP(Fo + BiXii + B2X})

 

¥) = logyg(BiXi) + P2Xia +6:
Yi = Bo exp(BiXin) +6
Yj = [1 +exp(fo+ AiXa +e)

eee

Reference Form (6.7)

XK

 

Bot PiXin + BoXi2 +--+ + By-X,p1 +8 7",Generalized Linear Regression
lr_ALRM_p33,"‘Show that the ratio SSR/SSTO is the same whether Y; is regressed on 9 or 9 is regressed on
Y4. (Hint. Use (1.10a) and (2.51)]",sum of squares
lr_ALRM_p34,"Airfreight breakage. A substance used in biological and medical research is shipped by
airfreight to users in cartons of 1,000 ampules. The data below, involving 10 shipments, were
collected on the number of times the carton was transferred from one aircraft to another over the
shipment route (x) and the number of ampules found to be broken upon arrival (Y). Assume that
first-order regression model (1.1) is appropriate.

1203 4 5 6 7

1 0 2 0 3 1 0 4 2
6 9 7 2 2 3B 8 1 9

 

 

. Obtain the estimated regression function. Plot the estimated regression function and the data.
Does a linear regression function appear to give a good fit here?

. Obtain a point estimate of the expected number of broken ampules when X= 1 transfer is,
made.

c. Estimate the increase in the expected number of ampules broken when there are 2 transfers
as compared to 1 transfer.

d. Verify that your fitted regression line goes through the point (, P)","Simple Linear Regression, estimation, parameter"
lr_ALRM_p35,"In a regression study, three types of banks were involved, namely, commercial, mutual savings,
and savings and loan. Consider the following system of indicator variables for type of bank:

TypeofBank ke

Commercial 10
Mutual savings = 1
Savingsand loan 1-1

a. Develop a first-order linear regression model for relating last year’s profit or loss (Y) to size of
bank (X;) and type of bank (Xp, X3)

. State the response functions for the three types of banks.

c. Interpret each of the following quantities; (1) Bo, (2) 83, (3) — 82 - 83.","Multiple Linear Regression, estimation"
lr_ALRM_p36,"A student who used a regression model that included indicator variables was upset when
receiving only the following output on the multiple regression printout: XTRANSPOSE X
SINGULAR. What is a likely source of the difficulty?",regression
lr_ALRM_p37,"Property assessments. The data that follow show assessed value for property tax purposes
(%. in thousand dollars) and sales price (Y9, in thousand dollars) for a sample of 15 parcels of
land for industrial development sold recently in ‘arm’s length’ transactions in a tax district.
Assume that bivariate normal model (2.74) is appropriate here.

 

 

Po Bn 8 Bus
Yr 139 160 103 9 129°~«158
Ye: 286 347 21.0 35.1 300 36.2

a. Plot the data in a scatter diagram. Does the bivariate normal model appear to be appropriate
here? Discuss.

b. Calculate r;2. What parameter is estimated by r;2? What is the interpretation of this,
parameter?

c. Test whether or not Y; and Y> are statistically independent in the population, using test
statistic (2.87) and level of significance .01. State the alternatives, decision rule, and conclusion

d. To test 01 = 6 versus 042 # .6, would it be appropriate to use test statistic (2.87)?","Simple Linear Regression, data plot, estiamtion, Hypothesis Testing"
lr_ALRM_p38,"Refer to regression model (1.1). Assume that X= 0 is within the scope of the model. What is the
implication for the regression function if Bg = 0 so that the model is ¥; = 6X) + ¢? How would the

regression function plot on a graph?","parameter, standardization"
lr_ALRM_p39,"Flavor deterioration. The results shown below were obtained in a small-scale experiment to
study the relation between °F of storage temperature (X) and number of weeks before flavor
deterioration of a food product begins to occur (Y).

 

i 1 2 3 4 5
Xr 8 4 ° -4 -8
Y: 78 9.0 10.2 11.0 Wz

Assume that first-order regression model (2.1) is applicable. Using matrix methods, find (1) Y'Y,
(2) XX, (3) XY.","parameter, Linear Algebra"
lr_ALRM_p40,"Two authors wrote as follows: “Our research utilized a multiple regression model. Two of the
predictor variables important in our theory tured out to be highly correlated in our data set. This
made it difficult to assess the individual effects of each of these variables separately. We retained
both variables in our model, however, because the high coefficient of multiple determination
makes this difficulty unimportant.” Comment.",coefficient of determination
lr_ALRM_p41,"For the matrices below, obtain (1) A + C, (2) A- C, (3) BA, (4) AC! (5) C'A

B= c=

6
9
3
1

hooe

1
5
a
8.

State the dimension of each resulting matrix.",Linear Algebra
lr_ALRM_p42,"A speaker stated: “In well-designed experiments involving quantitative explanatory variables, a
procedure for reducing the number of explanatory variables after the data are obtained is not
necessary.” Discuss.","variable selection, diagnostic"
lr_ALRM_p43,"A social scientist stated: “The conditions for the bivariate normal distribution are so rarely met in
my experience that | feel much safer using a regression model.” Comment.",regression
lr_ALRM_p44,"Consider the multiple logistic regression model with XB = y+ #.Xi + faXo + #sXi%s. Derive an
expression for the odds ratio for X;. Does exp(84) have the same meaning here as for a

regression model containing no interaction term?","logistic regression, parameter"
lr_ALRM_p45,"Lowess smoothing becomes difficult when there are many predictors and the sample size is
‘small. This is sometimes referred to as the “curse of dimensionality.” Discuss the nature of this
problem",regression
lr_ALRM_p46,"Microcomputer components. A staff analyst for a manufacturer of microcomputer components
has compiled monthly data for the past 16 months on the value of industry production of
processing units that use these components (X, in million dollars) and the value of the firm's
components used (Y, in thousand dollars). The analyst believes that a simple linear regression
relation is appropriate but anticipates positive autocorrelation. The data follow:

 

 

th 1 2 3 “a 14 15 16
2.052 2.026 2.002... 2.080 2.102 2.150
Ye: 102.9 101.5 100.8 104.8 105.0 107.2

 

a. Fita simple linear regression model by ordinary least squares and obtain the residuals. Also
obtain s{D9} and s{b;}

. Plot the residuals against time and explain whether you find any evidence of positive
autocorrelation

c. Conduct a formal test for positive autocorrelation using a = .05. State the alternatives, decision
rule, and conclusion. Is the residual analysis in part (b) in accord with the test result?","Simple Linear Regression, least squares estiamtion, residaul, residual plot, Hypothesis Testing"
lr_ALRM_p47,"The random variables Y; and Y2 follow the bivariate normal distribution in (2.74). Show that if
012 = 0, Y; and Y> are independent random variables.","independence, correlation"
lr_ALRM_p48,"A speaker stated in a workshop on applied regression analysis: “In business and the social
sciences, some degree of multicollinearity in survey data is practically inevitable.” Does this
statement apply equally to experimental data?",multicollinearity
lr_ALRM_p49,"Can o?{pred} in (2.37) he brought increasingly close to 0 as n becomes large? Is this also the
case for 07{¥} in (2.29b)? What is the implication of this difference?",prediction
lr_ALRM_p50,"Consumer finance. The data below show, for a consumer finance company operating in six
cities, the number of competing loan companies operating in the city (x) and the number per
thousand of the company’s loans made in that city that are currently delinquent (Y)

i: 12 3 4 5 6
Xi: 44 2 3 3 4
i ww  § 0 18 13 22

Assume that first-order regression model (2.1) is applicable. Using matrix methods, find (1) Y'Y,
(2) XX, (3) XY.","Simple Linear Regression, Linear Algebra"
lr_ALRM_p51,"For each of the following questions, explain whether a confidence interval for a mean response
or a prediction interval for a new observation is appropriate.

a. What will be the humidity level in this greenhouse tomorrow when we set the temperature level
at 31°C?

b. How much do families whose disposable income is $23,500 spend, on the average, for meals
away from home?

c. How many kilowatt-hours of electricity will be consumed next month by commercial and
industrial users in the Twin Cities service area, given that the index of business activity for the
area remains at its present level?",confidence interval
lr_ALRM_p52,"Per capita earnings. A sociologist employed linear regression model (2.1) to relate per capita
earnings (Y) to average number of years of schooling (X) for 12 cities. The fitted values, and
the semistudentized residuals ¢? follow.

ho 2 3. 10 u 2
LN ee
f 99 93 102 1360 2B

gq: 12 Bl 76. 378A 32

a. Plot the semistudentized residuals against the fitted values. What does the plot suggest?

b. How many semistudentized residuals are outside #1 standard deviation? Approximately how
many would you expect to see if the normal error model is appropriate?","residaul, semistudentized residual, residual plot, "
lr_ALRM_p53,"Advertising ageney. The managing partner of an advertising agency is interested in the
possibility of making accurate predictions of monthly billings. Monthly data on amount of billings
(¥. in thousands of constant dollars) and on number of hours of staff time (X, in thousand hours)
for the 20 most recent months follow. A simple linear regression model is believed to be
appropriate, but positively autocorrelated error terms may be present.

 

t: 1 2 3 ons 18 19 20
Xe 2.521 2171 2.234 . 3.117 3.623 3.618
Ye: 220.4 203.9 207.2 se 252.4 278.6 278.5

 

a. Fita simple linear regression model by ordinary least squares and obtain the residuals. Also
obtain s{D9} and s{b;}

. Plot the residuals against time and explain whether you find any evidence of positive
autocorrelation

c. Conduct a formal test for positive autocorrelation using a = .01. State the alternatives, decision
rule, and conclusion. Is the residual analysis in part (b) in accord with the test result?","Simple Linear Regression, residual , Estimation, Hypothesis Testing, "
lr_ALRM_p54,"For regression model (6.1), show that the coefficient of simple determination between Y, and Yj

equals the coefficient of multiple determination R2.",coefficient of determination
lr_ALRM_p55,"‘Show how the following expressions are written in terms of matrices: (1) ¥;— ¥j= e;, (2) = Xej=

0. Assume /= 1...., 4.","parameter, residual"
lr_ALRM_p56,"In time series analysis, the X variable representing time usually is defined to take on values 1, 2
etc., for the successive time periods. Does this represent an allocated code when the time
periods are actually 1989, 1990, etc.?",prediction
lr_ALRM_p57,"When the predictor variable is so coded that ¢ = 0 and the normal error regression model (2.1)
applies, are bp and 6, independent? Are the joint confidence intervals for By and ; then

independent?",independence
lr_ALRM_p58,"Computer programmers employed by a sofware developer were asked to participate in a month-
ong training seminar. During the seminar, each employee was asked to record the number of
hours spent in class preparation each week. After completing the seminar, the productivity level
of each participant was measured. A positive linear statistical relationship between participants’
productivity levels and time spent in class preparation was found. The seminar leader concluded
that increases in employee productivity are caused by increased class preparation time.

@. Were the data used by the seminar leader observational or experimental data?

b. Comment on the validity of the conclusion reached by the seminar leader.

c. Identify two or three alternative variables that might cause both the employee productivity
scores and the employee class participation times to increase (decrease) simultaneously.

d. How might the study be changed so that a valid conclusion about causal relationship between
class preparation time and employee productivity can be reached?",variable selection
lr_ALRM_p59,"For the bivariate normal distribution with parameters 3,02 = 4, and p42

80.

 

a. State the characteristics of the marginal distribution of ¥}

. State the characteristics of the conditional distribution of Y> when Y; = 55.

c. State the characteristics of the conditional distribution of Y; when Y> = 95.",probability distribution
lr_ALRM_p60,"(Obtain an expression for the variance-covariance matrix of the fitted values Yj, 7= 1....,n, in
terms of the hat matrix.",covariance
lr_ALRM_p61,"Bottle return. A carefully controlled experiment was conducted to study the effect of the size of
the deposit level on the likelihood that a returnable one-lter soft-drink bottle will be returned. A

bottle return was scored 1, and no return was scored 0. The data to follow show the number of
bottles that were retuned (Y) out of 500 sold (n)) at each of six deposit levels (1), in cents)

kod 2 3 4 5 6
Deposit level X/ 2 5 10 2 2 30
Numbersold'nj: $00. $00,500 S00. 500500

Numberretumed Yj: 72,—«=«103-—='=—«*170.—='s—«296 G68

An analyst believes that logistic regression model (14.20) is appropriate for studying the relation
between size of deposit and the probability a bottle will be returned

a. Plot the estimated proportions p) = ¥/n) against Xj. Does the plot support the analyst's belief
that the logistic response function is appropriate?

. Find the maximum likelihood estimates of By and 8}. State the fitted response function.

c. Obtain a scatter plot of the data with the estimated proportions from part (a), and superimpose
the fitted logistic response function from part (b). Does the fitted logistic response function
appear to fit well?

d. Obtain exp(b4) and interpret this number.

©. What is the estimated probability that a bottle will be returned when the deposit is 15 cents?

1. Estimate the amount of deposit for which 75 percent of the bottles are expected to be returned","logistic regression, data plot, Estimation"
lr_ALRM_p62,"Ina small-scale regression study, five observations on Y were obtained corresponding to X= 1
4,10, 11, and 14. Assume that 0 = 6, y= 5, and By = 3.

a. What are the expected values of MSR and MSE here?

. For determining whether or not a regression relation exists, would it have been better or worse
to have made the five observations at X = 6, 7, 8, 9, and 10? Why? Would the same answer
apply if the principal purpose were to estimate the mean response for X = 8? Discuss.","sum of squares, prediction"
lr_ALRM_p63,"The true quadratic regression function is E{Y} = 15 + 20X + 3x”. The fitted linear regression
function is Y= 13 + 40X, for which E{bo} = 10 and E{b;} = 45. What are the bias and sampling
error components of the mean squared error for X) = 10 and for x; = 20?","Generalized Linear Regression, estimation"
lr_ALRM_p64,"Describe several informal methods that can be helpful in identifying multicollinearity among the X
variables in a multiple regression model","Multicollinearity, Diagostic, multiple regression model"
lr_ALRM_p65,"A management trainee in a production department wished to study the relation between weight of
rough casting and machining time to produce the finished block. The trainee selected castings so
that the weights would be spaced equally apart in the sample and then observed the
corresponding machining times. Would you recommend that a regression or a correlation model
be used? Explain.",Remedy
lr_ALRM_p66,"Find the matrix A of the quadratic form:

BY? +10¥,%2 + 1793",Linear Algebra
lr_ALRM_p67,"A student working on a summer internship in the economic research department of a large
corporation studied the relation between sales of a product (¥. in million dollars) and population
(. in million persons) in the firm's 50 marketing districts. The normal error regression model (2.1)
was employed. The student first wished to test whether or not a linear association between Y and
X existed. The student accessed a simple linear regression program and obtained the following
information on the regression coefficients:

95 Percent
Parameter Estimated Value Confidence Limits
intercept 743119) =178518 ——*16.0476
Slope (755048 452886 1.08721

a. The student concluded from these results that there is a linear association between Y and X.
Is the conclusion warranted? What is the implied level of significance?

b. Someone questioned the negative lower confidence limit for the intercept, pointing out that
dollar sales cannot be negative even if the population in a district is zero. Discuss.","Simple Linear Regression, Hypothesis Testing"
lr_ALRM_p68,"A junior investment analyst used a polynomial regression model of relatively high order in a
research seminar on municipal bonds and obtained an R* of 991 in the regression of net interest
yield of bond (Y) on industrial diversity index of municipality (X) for seven bond issues. A

classmate, unimpressed, said: “You overfitted. Your curve follows the random effects in the data.”

a. Comment on the criticism.

b. Might R? defined in (6.42) be more appropriate than R* as a descriptive measure here?",Remedy
lr_ALRM_p69,"Using the normal error regression model (2.1) in an engineering safety experiment, a researcher
found for the first 10 cases that A? was zero. Is it possible that for the complete set of 30 cases
FR? will not be zero? Could R? not be zero for the first 10 cases, yet equal zero for all 30 cases?
Explain",Remedy
lr_ALRM_p70,"Chemical shipment. The data to follow, taken on 20 incoming shipments of chemicals in drums
arriving at a warehouse, show number of drums in shipment (Xj), total weight of shipment (Xp, in

hundred pounds), and number of minutes required to handle shipment (Y)

 

is 1 2 3 a 18 19 20
Xn 7 18 Ss a a 6 ""
Xai 5.1 16.72 32000... 15.21 3.64 9.57
Yi 58 182 a 155 39 90

Given below are the estimated ridge standardized regression coefficients, the variance inflation
factors, and R? for selected biasing constants c.

c_.000 005 ot?
th: 451453455460 460459 ASB AA
bf: 561 55655252617 «508504473

(VIP) = (VIF): 7.03 6.20 5.51265 2.0361 1.467
R?: 9869 9869 9869 9862 9856 9852 9844 9780

 

a. Fit regression model (6.1) to the data and find the fitted values.

b. Make a ridge trace plot for the given c values. Do the ridge regression coefficients exhibit
substantial changes near ¢ = 0?

c. Why are the (VIF); values the same as the (VIF) values here?

d. Suggest a reasonable value for the biasing constant c based on the ridge trace, the VIF
values, and R2

e. Transform the estimated standardized regression coefficients selected in part (c) back to the
original variables and obtain the fitted values for the 20 cases. How similar are these fitted values
to those obtained with the ordinary least squares fit in part (a)?","Multiple Linear Regression, Estimation, Remedy"
lr_ALRM_p71,"Set up the X matrix and B vector for each of the following regression models (assume i= 1..... 5)

a ¥ = BiXn + Xin + BX +6

D. JV = Bo + BiXis + Br logig Xi2 +6",Generalized Linear Regression
lr_ALRM_p72,"McGill Company sales. The data below show seasonally adjusted quarterly sales for the McGill
Company (, in million dollars) and for the entire industry (X, in million dollars) for the most
recent 20 quarters.

 

1 2 3 ome 18 19 20

Xe 127.3 130.0 132.7
. 165.6 168.7 17;
Ye 20.96 21.40 21.96 27.78 28.24 a ms

a. Would you expect the autocorrelation parameter p to be positive, negative, or zero here?

. Fita simple linear regression model by ordinary least squares and obtain the residuals. Also
obtain s{D9} and s{b;}

. Plot the residuals against time and explain whether you find any evidence of positive
autocorrelation

d. Conduct a formal lest for positive autocorrelation using a = .01. Slate the alternatives, decision
rule, and conclusion. Is the residual analysis in part (c) in accord with the test result?","Simple Linear Regression, Estimation, Hypothesis Testing"
lr_ALRM_p73,"Performance ability. A psychologist conducted a study to examine the nature of the relation, if
any, between an employee's emotional stability (X) and the employee's ability to perform ina
task group (¥). Emotional stability was measured by a written test for which the higher the score,
the greater is the emotional stability. Ability to perform in a task group (Y= 1 if able, Y= Oif
unable) was evaluated by the supervisor. The results for 27 employees were:

 

 

i 1 2 3 2 eo
x: 478432 AS 362 506-600
¥: ° 0 ° 1 ° 1

Logistic regression model (14.20) is assumed to be appropriate.

a. Find the maximum likelihood estimates of By and 8}. State the fitted response function.

b. Obtain a scatter plot of the data with both the fitted logistic response function from part (a) and
a lowess smooth superimposed. Does the fitted logistic response function appear to fit well?

. Obtain exp(b;) and interpret this number.

d. What is the estimated probability that employees with an emotional stability test score of 550
will be able to perform in a task group?

e. Estimate the emotional stability test score for which 70 percent of the employees with this test
score are expected to be able to perform in a task group.","Logistic Regression, Estimation"
lr_ALRM_p74,"Refer to first-order autoregressive error model (12.1). Suppose Y; is company’s percent share of
the market, X; is company's selling price as a percent of average competitive selling price, By =

     

 

 

100, By =-35, 9 = 6, 0? = 1, and ep = 2.403. Let X; and Uy be as follows for t= 1,..., 10:
th 1 2 3 4 5 6 7 8 9 wo
Xe 100 ws 120 90 85 7s 70 9S 10s 10

uy: 764 $09 — 242-1808 -48S 501-539 434 -299 030

 

a. Plot the true regression line. Generate the observations Y; (

  

10), and plot these on the
same graph. Fit a least squares regression line to the generated observations Y; and plot it also
on the same graph. How does your fitted regression line relate to the true line?

 

b. Repeat the steps in part (a) but this time let p = 0. In which of the two cases does the fitted
regression line come closer to the true line? Is this the expected outcome?

 

c. Generate the observations Y; for o = -7. For each of the cases p = .6, 9 = 0, ando=

 

obtain the successive error term differences ¢— 4 (t= 1...., 10).

d. For which of the three cases in part (c) is Sie,
generalization does this suggest?

? Smallest? For which is it largest? What","Simple Linear Regression, Estimation"
lr_ALRM_p75,"Plastic hardness. Refer to Problems 1.3 and 1.14. Sixteen batches of the plastic were made,
and from each batch one test item was molded. Each test item was randomly assigned to one of
the four predetermined time levels, and the hardness was measured after the assigned elapsed
time. The results are shown below: X is the elapsed time in hours, and Yis hardness in Brinell
units. Assume that first-order regression model (1.1) is appropriate.

   

1 2 5 “ws 6
6 16 16 40 4040
199 205196 28 253246,

. Obtain the estimated regression function. Plot the estimated regression function and the data.
Does a linear regression function appear to give a good fit here?

. Obtain a point estimate of the mean hardness when X = 40 hours.

c. Obtain a point estimate of the change in mean hardness when X increases by 1 hour.","Simple Linear Regression, Estimation, Prediction"
lr_ALRM_p76,"(Calculus needed.) Consider the multiple regression mode
%=BiXn+PXat si =1 a
where the ¢ are uncorrelated, with Efe} = 0 and 0fe} = 02.

a. State the least squares criterion and derive the least squares estimators of 8; and Bp

b. Assuming that the ¢; are independent normal random variables, state the likelihood function
and obtain the maximum likelihood estimators of 8; and Bp. Are these the same as the least
squares estimators?","Multiple Linear Regression, Estimation"
lr_ALRM_p77,"Consider the normal error regression model (1.24). Suppose that the parameter values are By =
200, 8; = 5.0, and o= 4.

a. Plot this normal error regression model in the fashion of Figure 1.6. Show the distributions of ¥
for X= 10, 20, and 40.

. Explain the meaning of the parameters By and 8. Assume that the scope of the model
includes X= 0.","Probability Distribution, Prediction, Estimation"
lr_ALRM_p78,"Car purchase. A marketing research firm was engaged by an automobile manufacturer to
conduct a pilot study to examine the feasibility of using logistic regression for ascertaining the
likelinood that a family will purchase a new car during the next year. A random sample of 33
suburban families was selected. Data on annual family income (X;, in thousand dollars) and the
current age of the oldest family automobile (Xp, in years) were obtained. A follow-up interview
conducted 12 months later was used to determine whether the family actually purchased a new
car (Y= 1) or did not purchase a new car (Y= 0) during the year.

 

‘Multiple logistic regression model (14.41) with two predictor variables in first-order terms is
assumed to be appropriate

a. Find the maximum likelihood estimates of Bp, 83, and Bo. State the fitted response function.

b. Obtain exp(b4) and exp(b2) and interpret these numbers.

. What is the estimated probability that a family with annual income of $50 thousand and an
oldest car of 3 years will purchase a new car next year?","Multiple Linear Regression, Estimation, Prediction"
lr_ALRM_p79,"Enzyme kinetics. In an enzyme kinetics study the velocity of a reaction (Y) is expected to be
related to the concentration (X) as follows:

WX
n+X

 

 

+e

Eighteen concentrations have been studied and the results follow:

 

 

i 1 2 3 16 7 18
Xi 1 52 30 35 40
Nis 2100 25049 - 197 BG

a. To obtain starting values for yo, and 3, observe that when the error term is ignored we have
¥/ = Bo + BiX), where 1%, Bo = 1/7 Bi = ¥a/ yo» and X} = 1/X). Therefore fit a linear
regression function to the transformed data to obtain initial estimates {® = 1/h, and

40? = bi/bo

      

 

. Using the starting values obtained in part (a), find the least squares estimates of the
parameters yo, and v3.","Generalized Linear Regression, Estimation"
lr_ALRM_p80,"Prepare a prototype residual plot for each of the following cases: (1) error variance decreases
with x: (2) true regression function is Uy shaped, but a linear regression function is fitted",Diagnostics
lr_ALRM_p81,"Derive an extension of the Bonferroni inequality (4.2a) for the case of three statements, each
with statement confidence coefficient 1 — a.",Inference
lr_ALRM_p82,"Let A be defined as follows

018
A=|031
055

a. Are the column vectors of A linearly dependent?
b. Restate definition (5.20) in terms of row vectors. Are the row vectors of A linearly dependent?
c. What is the rank of A?

d. Calculate the determinant of A.",Linear Algebra
lr_ALRM_p83,"Show that:

a. SSR, Xp, Xz, Xq) = SSRI) + SSRN, XGIXq) + SSRKAIXy, Xp, X3).

b. SSR (X4, Xp, X, Xq) = SSR(X, X3) + SSRUG Xp, Xg) + SSRK|Ny, Xp, X3).",Estimation
lr_ALRM_p84,"Computer-assisted learning. Data from a study of computer-assisted learning by 12 students,
showing the total number of responses in completing a lesson (X) and the cost of computer time
(% in cents), follow.

hoy 203064 US U6 Ul le

X16 14 220 10 14 «17° «1018928
Y: 77 70 85 SO 62 70 SS 63 88 S7 81 SI

a. Fita linear regression function by ordinary least squares, obtain the residuals, and plot the
residuals against X. What does the residual plot suggest?

. Divide the cases into two groups, placing the six cases with the smallest fitted values ¥; into

group 1 and the other six cases into group 2. Conduct the Brown-Forsythe test for constancy of
the error variance, using a = .05. State the decision rule and conclusion

c. Plot the absolute values of the residuals against X. What does this plot suggest about the
relation between the standard deviation of the error term and X?

d. Estimate the standard deviation function by regressing the absolute values of the residuals
against X, and then calculate the estimated weight for each case using (11.16a). Which case
receives the largest weight? Which case receives the smallest weight?

e. Using the estimated weights, obtain the weighted least squares estimates of 8p and 8. Are
these estimates similar to the ones obtained with ordinary least squares in part (a)?

1. Compare the estimated standard deviations of the weighted least squares estimates byyo and
yy in part (@) with those for the ordinary least squares estimates in part (a). What do you find?

g. Iterate the steps in parts (d) and (e) one more time. Is there a substantial change in the
estimated regression coefficients? If so, what should you do?","Simple Linear Regression, Estimation"
lr_ALRM_p85,"Employee salaries. A group of high-technology companies agreed to share employee salary
information in an effort to establish salary ranges for technical positions in research and
development. Data obtained for each employee included current salary (Y), a coded variable
indicating highest academic degree obtained (1 = bachelor’s degree, 2 = master's degree, 3 =
doctoral degree), years of experience since last degree (X3), and the number of persons

currently supervised (X,). The data follow.

Employee
i Ya Degree Xn Xu

1 58.8 3 4.49 0

2 34.8 1 292 0

3 163.7 3 29.54 42

63 40.0 2 a4 °
64 60.5 3 2.10 0

65 104.8 3 19.81 24

a. Create two indicator variables for highest degree attained

Degree Xa
Bachelor's ° 0
Master's 1 0
Doctoral ° 1

b. Regress Yon X4, Xp, X3, and Xz, using a first-order model and ordinary least squares, obtain
the residuals, and plot them against Y. What does the residual plot suggest?

. Divide the cases into two groups, placing the 33 cases with the smallest fitted values Yj into

group 1 and the other 32 cases into group 2. Conduct the Brown-Forsythe test for constancy of
the error variance, using a = .01 State the decision rule and conclusion

 

4. Plot the absolute residuals against X3 and against X4. What do these plots suggest about the
relation between the standard deviation of the error term and X3 and X4?

€. Estimate the standard deviation function by regressing the absolute residuals against X3 and
Xz in first-order form, and then calculate the estimated weight for each case using (11.16a)

1 Using the estimated weights, obtain the weighted least squares fit of the regression model. Are
the weighted least squares estimates of the regression coefficients similar to the ones obtained
with ordinary least squares in part (b)?

g. Compare the estimated standard deviations of the weighted least squares coefficient
estimates in part (f) with those for the ordinary least squares estimates in part (b). What do you
find?

h. Iterate the steps in parts (e) and (f) one more time. Is there a substantial change in the
estimated regression coefficients? If so, what should you do?","Multiple Linear Regression, Generalized Linear Regression, Estimation"
lr_ALRM_p86,"A behavioral scientist said, “I am never sure whether the regression line goes through the origin
Hence, | will not use such a model.” Comment.",Diagnostics
lr_ALRM_p87,"When testing whether or not 8; = 0, why is the F test a one-sided test even though H, includes
both By <0 and By > 0? [Hint: Refer to (2.57)]",Hypothesis Testing
lr_ALRM_p88,"In forward stepwise regression, what advantage is there in using a relatively small a-to-enter
value for adding variables? What advantage is there in using a larger a-to-enter value?",Remedy
lr_ALRM_p89,"Typographical errors. Shown below are the number of galleys for a manuscript (X) and the
dollar cost of correcting typographical errors (Y) in a random sample of recent orders handled by
a firm specializing in technical manuscripts. Assume that the regression model Y; = 84X) + €iS

appropriate, with normally distributed independent error terms whose variance is 0? = 16.

i 1 2 3 4 s 6
x 7 2 4 4 2 30
y -12B 2137S 280445540

a. State the likelihood function for the six Y observations, for 0? = 16.

. Evaluate the likelihood function for 8 = 17, 18, and 19. For which of these 8; values is the
likelihood function largest?

c. The maximum likelihood estimator is b; = > X:¥i/ > X?. Find the maximum likelihood
estimate. Are your results in part (b) consistent with this estimate?

d. Using a computer graphics or statistics package, evaluate the likelinood function for values of
{8 between 6, = 17 and 8, = 19 and plot the function. Does the point at which the likelihood

function is maximized correspond to the maximum likelihood estimate found in part (c)?","Simple Linear Regression, Estimation"
lr_ALRM_p90,"a. Define each of the following extra sums of squares: (1) SSR(%5|X;): (2) SSRUG, X4lX}); (8)

SSRKAING, XQ, X3).

. For a multiple regression model with five X variables, what is the relevant extra sum of
squares for testing whether or not 85 = 0? whether or not 62 = 84 = 0?","Estimation, Hypothesis Testing"
lr_ALRM_p91,"a. Plot the exponential response function:

LK, y) =49— 3Oexp(-1.1X) X20

. What is the asymptote of this response function? For what value of X does the response
function reach 95 percent of its asymptote?","Generalized Linear Regression, Inference"
lr_ALRM_p92,"Regression trees become difficult to utilize when there are many predictors and the sample size
is small. Discuss the nature of this problem",Remedy
lr_ALRM_p93,"‘The members of a health spa pay annual membership dues of $300 plus a charge of $2 for each
visit to the spa. Let Y denote the dollar cost for the year for a member and X the number of visits,
by the member during the year. Express the relation between X and Y mathematically. Is it a
functional relation or a statistical relation?",Diagnostics
lr_ALRM_p94,"Consider the simultaneous equations:

4y, +79:
2y1+3y2 = 12

 

a. Write these equations in matrix notation.

b. Using matrix methods, find the solutions for y; and ya",Linear Algebra
lr_ALRM_p95,"Drug responsiveness. A pharmacologist modeled the responsiveness to a drug using the
following nonlinear regression modet

»
Fy arias

S

X denotes the dose level, in coded form, and Y the responsiveness expressed as a percent of
the maximum possible responsiveness. In the model, yo is the expected response at saturation,

 

v2 is the concentration that produces a half-maximal response, and y; is related to the slope. The
data for 19 cases at 13 dose levels follow:

W 8 9

Xp 1 2 3 7 8 9
Yi: S23 34 948 962 96.4",Inference
lr_ALRM_p96,"Water flow. An engineer, desiring to estimate the coefficient of correlation 0; between rate of
water flow at point A in a stream (Y;) and concurrent rate of flow at point B (¥2), obtained ry =
83 in a sample of 147 cases. Assume that bivariate normal model (2.74) is appropriate.

a. Obtain a 99 percent confidence interval for 042.

b. Convert the confidence interval in part (a) to a 99 percent confidence interval for pi,",Logistic Regression
lr_ALRM_p97,"a. Plot the logistic response function:

300

'(X, y) = ——_—_____
LOY = TT Giyexp(= 13)

x20

. What is the asymptote of this response function? For what value of X does the response
function reach 90 percent of its asymptote?",Remedy
lr_ALRM_p98,"An engineer has stated: “Reduction of the number of explanatory variables should always be
done using the objective forward stepwise regression procedure.” Discuss.",Linear Algebra
lr_ALRM_p99,"Let B be defined as follows

15 0
B=|1 05
105

a. Are the column vectors of B linearly dependent?
b. What is the rank of B?

c. What must be the determinant of B?",Remedy
lr_ALRM_p100,"An attendee at a regression modeling short course stated: “I rarely see validation of regression
models mentioned in published papers, so it must really not be an important component of mode!
building.” Comment.","Simple Linear Regression, Estimation, Inference"
lr_ALRM_p101,"Annual dues. The board of directors of a professional association conducted a random sample
survey of 30 members to assess the effects of several possible amounts of dues increase. The
sample results follow. X denotes the dollar increase in annual dues posited in the survey
interview, and Y= 1 if the interviewee indicated that the membership will not be renewed at that
amount of dues increase and 0 if the membership will be renewed

I 1 3 2 9 0

x00 0 5050
i o 4 ° °o 4 1

 

Logistic regression model (14.20) is assumed to be appropriate.

a. Find the maximum likelihood estimates of By and 8}. State the fitted response function.

b. Obtain a scatter plot of the data with both the fitted logistic response function from part (a) and
a lowess smooth superimposed. Does the fitted logistic response function appear to fit well?

. Obtain exp(b;) and interpret this number.

d. What is the estimated probability that association members will not renew their membership if
the dues are increased by $40?

e. Estimate the amount of dues increase for which 75 percent of the members are expected not
to renew their association membership.",Linear Algebra
lr_ALRM_p102,"Prepare a contour plot for the quadratic response surface iy) = 140 + 4x? — 2x3 + Sex.
Describe the shape of the response surface.",Simple Linear Regression
lr_ALRM_p103,"‘Show that for the fitted least squares regression line through the origin (4.15), > Xie = 0.",Generalized Linear Regression
LR_DO_ch5_P13,"This problem uses data from Kuehl (1994, p. 128).
a) Get lregdata and lregpack into R. Type the following commands. Then simultaneously press the Ctrl and c keys. In Word use the menu command 鈥淧aste.鈥?Print out the figure.
  y <- ycrab+1/6
  aovtplt(crabhab,y)
b) From the figure, what response transformation should be used: Y = 鈭氣垰
1/Z,Y =1/ Z,Y =log(Z),Y = Z,orY =Z?

","Computational Statistics, Generalized Linear Regression"
LR_DO_ch5_P14,"The following data set considers the number of warp breaks per loom, where the factor is tension (low, medium, or high).
a) Copy and paste the commands for this problem into R.
Highlight the ANOVA table by pressing the left mouse key and dragging the cursor over the ANOVA table. Then use the menu commands 鈥淓dit> Copy.鈥?Enter Word and use the menu command 鈥淧aste.鈥?b) To place the residual plot in Word, get into R and click on the plot, hit the Ctrl and c keys at the same time. Enter Word and use the menu command 鈥淧aste鈥?or hit the Ctrl and v keys at the same time.
c) Copy and paste the commands for this part into R.
Click on the response plot, hit the Ctrl and c keys at the same time. Enter Word and use the menu command 鈥淧aste.鈥?","Computational Statistics, Hypothesis Testing"
LR_DO_ch5_P15,"Obtain the Box et al. (2005, p. 134) blood coagulation data from lregdata and the R program ganova from lregpack. The program does graph- ical Anova for the one way Anova model.
a) Enter the following command and include the plot in Word by simulta- neously pressing the Ctrl and c keys, then using the menu command 鈥淧aste鈥?in Word, or hit the Ctrl and v keys at the same time.","Computational Statistics, Hypothesis Testing"
LR_DO_ch5_P16,"5.17. To get in ARC, you need to find the ARC icon. Suppose the ARC icon is in a math progs folder. Move the cursor to the math progs folder, click the right mouse button twice, move the cursor to ARC, double click, move the cursor to ARC, double click. These menu commands will be written 鈥渕ath progs > ARC > ARC.鈥?To quit ARC, move cursor to the x in the northeast corner and click.
This Cook and Weisberg (1999a, p. 289) data set contains IQ scores on 27 pairs of identical twins, one raised by foster parents IQf and the other by biological parents IQb. C gives the social class of the biological parents: C = 1 for upper class, 2 for middle class and 3 for lower class. Hence the Anova test is for whether mean IQ depends on class.
a) Activate twins.lsp dataset with the menu commands 鈥淔ile > Load > Data > twins.lsp.鈥?b) Use the menu commands 鈥淭wins>Make factors,鈥?select C and click on OK. The line 鈥渰F}C Factor 27 Factor鈥揻irst level dropped鈥?should appear on the screen.
c) Use the menu commands 鈥淭wins>Description鈥?to see a description of the data.
d) Enter the menu commands 鈥淕raph&Fit>Fit linear LS鈥?and select {F}C as the term and IQb as the response. Highlight the output by pressing the left mouse key and dragging the cursor over the output. Then use the menu commands 鈥淓dit> Copy.鈥?Enter Word and use the menu command 鈥淧aste.鈥?5.7 Problems 211 e) Enter the menu commands 鈥淕raph&Fit>Boxplot of鈥?and enter IQb in
the selection box and C in the Condition on box. Click on OK. When the boxplots appear, click on the Show Anova box. Click on the plot, hit the Ctrl and c keys at the same time. Enter Word and use the menu command 鈥淧aste.鈥?Include the output in Word. Notice that the regression and Anova F statistic and p-value are the same.
f) Residual plot: Enter the menu commands 鈥淕raph&Fit>Plot of,鈥?select 鈥淟1:Fit-Values鈥?for the 鈥淗鈥?box and 鈥淟1:Residuals鈥?for the 鈥淰鈥?box, and click on 鈥淥K.鈥?Click on the plot, hit the Ctrl and c keys at the same time. Enter Word and use the menu command 鈥淧aste.鈥?g) Response plot: Enter the menu commands 鈥淕raph&Fit>Plot of,鈥?select 鈥淟1:Fit-Values鈥?for the 鈥淗鈥?box and 鈥淚Qb鈥?for the 鈥淰鈥?box, and click on 鈥淥K.鈥?When the plot appears, move the OLS slider bar to 1 to add the identity line. Click on the plot, hit the Ctrl and c keys at the same time. Enter Word and use the menu command 鈥淧aste.鈥?h) Perform the 4 step test for Ho: 渭1 = 渭2 = 渭3.","Computational Statistics, Hypothesis Testing"
LR_DO_ch5_P17,"This McKenzie and Goldman (1999, p. T-234) data set has 30 three-month- old infants randomized into five groups of 6 each. Each infant is shown a mobile of one of five multicolored designs, and the goal of the study is to see if the infant attention span varies with type of design of mobile. The times that each infant spent watching the mobile are recorded.
b) Choose 鈥淪tat>Basic Statistics>Display Descriptive Statistics,鈥?select 鈥淐1 Time鈥?as the 鈥淰ariable,鈥?click the 鈥淏y variable鈥?option and press Tab. Select 鈥淐2 Design鈥?as the 鈥淏y variable.鈥?c) From the window in b), click on 鈥淕raphs鈥?the 鈥淏oxplots of data鈥?option, and 鈥淥K鈥?twice. Click on the plot and then click on the printer icon to get a plot of the boxplots.
d) Select 鈥淪tat>ANOVA>One-way,鈥?select 鈥淐1-time鈥?as the response and 鈥淐2-Design鈥?as the factor. Click on 鈥淪tore residuals鈥?and click on 鈥淪tore fits.鈥?Then click on 鈥淥K.鈥?Click on the output and then click on the printer icon.
e) To make a residual plot, select 鈥淕raph>Plot.鈥?Select 鈥淩esi1鈥?for 鈥淵鈥?and 鈥淔its1鈥?for 鈥淴鈥?and click on 鈥淥K.鈥?Click on the plot and then click on the printer icon to get the residual plot.
f) To make a response plot, select 鈥淕raph>Plot.鈥?Select 鈥淐1 Time鈥?for 鈥淵鈥?and 鈥淔its1鈥?for 鈥淴鈥?and click on 鈥淥K.鈥?Click on the plot and then click on the printer icon to get the response plot.
g)Dothe4steptestforHo:渭1 =渭2 =路路路=渭5.
To get out of Minitab, move your cursor to the 鈥渪鈥?in the NE corner of the screen. When asked whether to save changes, click on 鈥渘o.鈥?,Hypothesis Testing
LR_DO_ch6_P1,. The above output uses data from Kutner et al. (2005", prob- lems 19.16鈥?7). A study measured the number of minutes to complete a repair job at a large dealership. The two explanatory variables were 鈥淎 = technician鈥?and 鈥淏 = make of drive.鈥?The output is given above.
a) Give a four step test for no interaction.,,
"b) Give a four step test for the B main effects.""",Hypothesis Testing,
LR_DO_ch6_P2,"Suppose A has 5 levels and B has 4 levels. Sketch an interaction plot if there is no interaction.
Two Way Anova in SAS
InSAS,Y =A|BisequivalenttoY =A B A鈭桞.ThustheSASmodel statement could be written in either of the following two forms.

","Hypothesis Testing, Computational Statistics, Diagnostic"
LR_DO_ch6_P3,"Cut and paste the SAS program from (http://lagrange.math.siu.edu/Olive/lreghw.txt) for 6.3 into the SAS Editor.
To execute the program, use the top menu commands 鈥淩un>Submit.鈥?An output window will appear if successful. The data is from Montgomery (1984, p. 198) and gives the maximum output voltage for a typical type of storage battery. The two factors are material (1,2,3) and temperature (50, 65, 80鈼).a) Copy and paste the SAS program into SAS, use the file command 鈥淩un>Submit.鈥?b) Click on the 鈥淕raph1鈥?window and scroll down to the second interaction plot of 鈥渢mp鈥?vs 鈥測mn.鈥?Press the printer icon to get the plot.
c) Is interaction present?
d) Click on the output window then click on the printer icon. This will produce 5 pages of output, but only hand in the ANOVA table, response plot, and residual plots.
(Cutting and pasting the output into Word resulted in bad plots. Using Notepad gave better plots, but the printer would not easily put the ANOVA table and two plots on one page each.)
e) Do the residual and response plots look ok?

","Hypothesis Testing, Computational Statistics, Diagnostic"
LR_DO_ch6_P4,"a) Copy the SAS data for problem 6.3 into Notepad. Then hit 鈥淓nter鈥?every three numbers so that the data is in 3 columns.b) Copy and paste the data into Minitab using the menu commands Edit>Paste Cells and click on 鈥淥K.鈥?Right below C1 type 鈥渕aterial,鈥?below C2 type 鈥渢emp鈥?and below C3 type 鈥渕voltage.鈥?c) Select Stat>ANOVA>Two-way, select 鈥淐3 mvoltage鈥?as the response and 鈥淐1 material鈥?as the row factor and 鈥淐2 temp鈥?as the column factor. Click on 鈥淪tore residuals鈥?and click on 鈥淪tore fits.鈥?Then click on 鈥淥K.鈥?Click on the output and then click on the printer icon.
d) To make a residual plot, select Graph>Plot. Select 鈥淩esi1鈥?for 鈥淵鈥?and 鈥淔its1鈥?for 鈥淴鈥?and click on 鈥淥K.鈥?Click on the printer icon to get a plot of the graph.
e) To make a response plot, select Graph>Plot. Select 鈥淐3 mvoltage鈥?for 鈥淵鈥?and 鈥淔its1鈥?for 鈥淴鈥?and click on 鈥淥K.鈥?Click on the printer icon to get a plot of the graph.
f) Use the menu commands 鈥淪tat>ANOVA>Interaction Plots.鈥?Enter mvoltage in the 鈥淩esponses鈥?box and material and temp in the 鈥淔actors鈥?box. Click on 鈥淥K鈥?and print the plot.","Hypothesis Testing, Computational Statistics, Diagnostic"
LR_DO_ch6_P5,"The Box et al. (2005, p. 318) poison data has 4 types of treatments (1,2,3,4) and 3 types of poisons (1,2,3). Each animal is given a poison and a treatment, and the response is survival in hours. Get the poison data from lregdata.
a) Type the following commands to see that the output for the three models is the same. Print the output.
  out1<-aov(stime~ptype*treat,poison)
  summary(out1)
  out2<-aov(stime~ptype + treat + ptype*treat,poison)
  summary(out2)
  out3<-aov(stime~.^2,poison)
  summary(out3)
  #The three models are the same.
b) Type the following commands to see the residual plot. Include the plot in Word.
  plot(fitted(out1),resid(out1))
  title(""Residual Plot"")
c) Type the following commands to see the response plot. Include the plot in Word.
6.5 Problems 225
  FIT <- poison$stime - out1$resid
  plot(FIT,poison$stime)
  abline(0,1)
  title(""Response Plot"")
d) Why is the two way Anova model inappropriate?
e) Now the response Y = 1/stime will be used. Type the following com- mands to get the output. Copy the output into Word.
  attach(poison)
  out4 <- aov((1/stime)~ptype*treat,poison)
  summary(out4)
f) Type the following commands to get the residual plot. Copy the plot into Word.
  plot(fitted(out4),resid(out4))
  title(""Residual Plot"")
g) Type the following commands to get the response plot. Copy the plot into Word.
  FIT <- 1/poison$stime - out4$resid
  plot(FIT,(1/poison$stime))
  abline(0,1)
  title(""Response Plot"")
h) Type the following commands to get the interaction plot. Copy the plot into Word.
  interaction.plot(treat,ptype,(1/stime))
  detach(poison)
i) Test whether there is an interaction using the output from e).","Hypothesis Testing, Computational Statistics, Diagnostic"
LR_DO_ch7_P1,"7.1. Snedecor and Cochran (1967, p. 300) give a data set with 5 types of soybean seed. The response frate = number of seeds out of 100 that failed to germinate. Five blocks were used. Assume the appropriate model can be used (although this assumption may not be valid due to a possible interaction between the block and the treatment).
a) Did blocking help? Explain briefly.
b) Perform the appropriate 4 step test using the output above.","Hypothesis Testing, Diagnostic"
LR_DO_ch7_P2,"Current nitrogen fertilization recommendations for wheat include applications of specified amounts at specified stages of plant growth. The treatment consisted of six different nitrogen application and rate schedules. The wheat was planted in an irrigated field that had a water gradient in one direction as a result of the irrigation. The field plots were grouped into four blocks, each consisting of six plots, such that each block occurred in the same part of the water gradient. The response was the observed nitrogen content from a sample of wheat stems from each plot. The experimental units were the 24 plots. Data is from Kuehl (1994, p. 263).
a) Did blocking help? Explain briefly.
b) Perform the appropriate 4 step test using the output above.","Hypothesis Testing, Diagnostic"
LR_DO_ch7_P3,"An experimenter wanted to test 4 types of an altimeter. There were eight helicopter pilots available for hire with from 500 to 3000 flight hours of experience. The response variable was the altimeter reading error. Perform the appropriate 4 step test using the output below. Data is from Kirk (1982, p. 244).","Hypothesis Testing, Diagnostic"
LR_DO_ch7_P4,"This problem is for a one way block design and uses data from Box et al. (2005, p. 146).
a) Copy and paste the SAS program for this problem from (http://lagrange.math.siu.edu/Olive/lreghw.txt). Print out the out- put but only turn in the ANOVA table, residual plot, and response plot.
b) Do the plots look ok?
c) Copy the SAS data into Minitab much as done for Problem 6.4. Right below C1 type 鈥渂lock,鈥?below C2 type 鈥渢reat,鈥?and below C3 type 鈥測ield.鈥?d) Select Stat>ANOVA>Two-way, select 鈥淐3 yield鈥?as the response and 鈥淐1 block鈥?as the row factor and 鈥淐2 treat鈥?as the column factor. Click on 鈥淔it additive model,鈥?click on 鈥淪tore residuals,鈥?and click on 鈥淪tore fits.鈥?Then click on 鈥淥K.鈥?e) block response scatterplot: Use file commands 鈥淓dit>Command Line Editor鈥?and write the following lines in the window.
GSTD
LPLOT 鈥榶ield鈥?vs 鈥榖lock鈥?codes for 鈥榯reat鈥?
","Hypothesis Testing, Diagnostic,Computational Statistic"
LR_DO_ch7_P5,"This problem is for a Latin square design and uses data from Box et al. (2005, pp. 157鈥?60).
244 7 Block Designs Copy and paste the SAS program for this problem from
(http://lagrange.math.siu.edu/Olive/lreghw.txt).
a) Click on the output and use the menu commands 鈥淓dit>Select All鈥?and 鈥淓dit>Copy. In Word use the menu command 鈥淧aste鈥?then use the left mouse button to highlight the first page of output. Then use the menu command 鈥淐ut.鈥?Then there should be one page of output including the ANOVA table. Print out this page.
b) Copy the data for this problem from (http://lagrange.math.siu.edu/Olive/lregdata.txt)
into R. Use the following commands to create a residual plot. Copy and paste the plot into Word. (Click on the plot and simultaneously hit the Ctrl and c buttons. Then go to Word and use the menu command 鈥淧aste.鈥?
  z<-aov(emissions~rblocks+cblocks+additives,auto)
  summary(z)
  plot(fitted(z),resid(z))
  title(""Residual Plot"")
abline(0,0)
c) Use the following commands to create a response plot. Copy and paste the plot into Word. (Click on the plot and simultaneously hit the Ctrl and c buttons. Then go to Word and use the menu command 鈥淧aste.鈥?
  attach(auto)
  FIT <- auto$emissions - z$resid
  plot(FIT,auto$emissions)
  title(""Response Plot"")
  abline(0,1)
  detach(auto)
d) Do the plots look ok?
e) Were the column blocks useful? Explain briefly. f) Were the row blocks useful? Explain briefly.
g) Do an appropriate 4 step test.

","Hypothesis Testing, Diagnostic,Computational Statistic"
LR_DO_ch7_P6,"Obtain the Box et al. (2005, p. 146) penicillin data from (http://lagrange.math.siu.edu/Olive/lregdata.txt) and the R pro- gram ganova2 from (http://lagrange.math.siu.edu/Olive/lregpack. txt). The program does graphical Anova for completely randomized block designs.
a) Copy and paste the R commands for this problem into R. Include the plot in Word by simultaneously pressing the Ctrl and c keys, then using the menu command 鈥淧aste鈥?in Word.
b) Blocking seems useful because some of the scaled block deviations are outside of the spread of the residuals. The scaled treatment deviations are in the middle of the plot. Do the treatments appear to be significantly different?","Hypothesis Testing, Diagnostic,Computational Statistic"
LR_DO_ch8_P1,"From the above least squares output, what is the AB effect?",Diagnostic
LR_DO_ch8_P2,"Ledolter and Swersey (2007, pp. 108鈥?09) describes a 23 experiment designed to increase subscriptions of the magazine Ladies鈥?Home Journal. The 2005 campaign made 8 brochures containing an order card. Each brochure was mailed to 15042 households, and the response Y was the percentage of orders. Factor A was front side of order card with (鈭?) highlighting 鈥淒ouble our Best Offer鈥?and (+1) highlighting 鈥淲e never had a bigger sale.鈥?Factor B was back side of order card with (鈭?) emphasizing 鈥淭wo extra years free,鈥?while (+1) featured magazine covers of a previous issue. Factor C was brochure cover with (鈭?) featuring Kelly Ripa and (+1) Dr. Phil. Assume m = 1.
a) Find the A effect.
b) Find the C effect.
c) Find SSC = MSC.
d) If two of the three factors A, B and C are active, which is inactive?",Diagnostic
LR_DO_ch8_P3,"The above table of 23 contrasts is for 25鈭? data. III
a) Estimate the B effect. b) Estimate the D effect.

",Diagnostic
LR_DO_ch8_P4,"Suppose that for 23 data with m = 2, the MSE = 407.5625. Find SE(effect).

",Diagnostic
LR_DO_ch8_P5,"Ledolter and Swersey (2007, p. 131) describe a 27鈭? data set shown
III
with the table of 23 contrasts above. Estimate the D effect.
",Diagnostic
LR_DO_ch8_P6,"Kuehl (1994, pp. 361鈥?66) describes a 23 experiment designed to investigate the effects of furnace temperature (1840 or 1880oF), heating time (23 or 25 sec) and transfer time (10 or 12 sec) on the quality of a leaf spring used for trucks. (The response Y was a measure of the quality.) The table of contrasts is shown above.
a) Find the A effect.
b) Find the B effect.
c) Find the AB effect.
d) If m = 1, find SSA.
e) If m = 1, find SSB.
f) If m = 1, find SSAB.
g) If m = 2 and MSE = 9, find SE(effect).
(The SE is the same regardless of the effect.)
h) Suppose high Y = y is desirable. If two of the factors A, B, and C are
inert and one is active, then which is active and which are inert. (Hint: look at the 4 highest values of y. Is there a pattern?)
i) If one of the factors has an interaction with the active factor, what is the interaction (e.g. AB, AC, or BC)?

",Diagnostic
LR_DO_ch8_P7,"Suppose the B effect = 鈭?, SE(effect) = 2, and dfe = 8.
i) Find a 95% confidence interval for the B effect. ii) Is the B effect significant? Explain briefly.","Diagnostic, Inference"
LR_DO_ch8_P8,"Copy the Box et al. (2005, p. 199) product development data from (http://lagrange.math.siu.edu/Olive/lregdata.txt) into R.
Then type the following commands.
  out <- aov(conversion~K*Te*P*C,devel)
  summary(out)
a) Include the output in Word.
b) What are the five effects with the biggest mean squares? Note: an AB interaction is denoted by A:B in R.","Residual, Diagnostic"
LR_DO_ch8_P9,"Get the SAS program for this problem from (http://lagrange.math.siu. edu/Olive/lreghw.txt). The data is the pilot plant example from Box et al. (2005, pp. 177鈥?86). The response variable is Y = yield, while the three predictors (T = temp, C = concentration, K = catalyst) are at two levels.
a) Print out the output but do not turn in the first page. b) Do the residual and response plots look ok?","Diagnostic, Hypothesis Testing,Computational statistics"
LR_DO_ch8_P10,"Get the data for this problem. The data is the pilot plant example from Box et al. (2005, pp. 177鈥?86) examined in Problem 8.9. Minitab needs the levels for the factors and the interactions.
Highlight the data and use the menu commands 鈥淓dit>Copy.鈥?In Minitab, use the menu command 鈥淓dit>PasteCells.鈥?After a window appears, click on ok.
Below C1 type 鈥淎鈥? below C2 type 鈥淏鈥? below C3 type 鈥淐鈥?and below C8 type 鈥測ield.鈥?a) Use the menu command 鈥淪TAT>ANOVA>Balanced Anova鈥?put 鈥測ield鈥?in the responses box and
A|B|C
in the Model box. Click on 鈥淪torage.鈥?When a window appears, click on 鈥淔its鈥?and 鈥淩esiduals.鈥?Then click on 鈥淥K鈥? This window will disappear. Click on 鈥淥K.鈥?b) Next highlight the bottom 8 lines and use the menu commands 鈥淓dit>Delete Cells鈥? Then the data set does not have replication. Use the menu command 鈥淪TAT>ANOVA>Balanced Anova鈥?put 鈥測ield鈥?in the re- sponses box and
A B C A*C
in the Model box. Click on 鈥淪torage.鈥?When a window appears, click on 鈥淔its鈥?and 鈥淩esiduals.鈥?Then click on 鈥淥K鈥? This window will disappear. Click on 鈥淥K.鈥?(The model A|B|C would have resulted in an error message, not enough data.) c) Print the output by clicking on the top window and then clicking on the printer icon.
d) Make a response plot with the menu commands 鈥淕raph>Plot鈥?with yield in the Y box and FIT2 in the X box. Print by clicking on the printer icon.
e) Make a residual plot with the menu commands 鈥淕raph>Plot鈥?with RESI2 in the Y box and FIT2 in the X box. Print by clicking on the printer icon.
f) Do the plots look ok?","Diagnostic, Hypothesis Testing,Computational statistics,Estimation"
LR_DO_ch8_P11,"et the R code and data for this problem from (http://lagrange.math.siu.edu/Olive/lreghw.txt). The data is the pilot plant
8.6 Problems 281
example from Box et al. (2005, pp. 177鈥?86) examined in Problems 8.9 and 8.10.
a) Copy and paste the code into R. Then copy and paste the output into Notepad. Print out the page of output.
b) The least squares estimate = coefficient for x1 is half the A effect. So what is the A effect?
","Diagnostic, Hypothesis Testing,Computational statistics,Estimation"
LR_DO_ch8_P12,"a) Obtain and the R program twocub from (http://lagrange.math.siu.edu/Olive/lregpack.txt). To get the effects, mean squares, and SE(effect) for the Box et al. (2005, p. 177) pilot plant data, type the following commands and include the output in Word.
  mns <- c(60,72,54,68,52,83,45,80)
  twocub(mns,m=2,MSE=8)
b) Which effects appear to be significant from the QQ plot? (Match the effects on the plot with the output on the screen.)","Diagnostic, Hypothesis Testing,Computational statistics"
LR_DO_ch8_P13,"Box et al. (2005, p. 237) describe a 24鈭? fractional factorial design. IV
Assuming that you downloaded the twocub function in the previous problem, type the following R commands.
  mns <- c(20,14,17,10,19,13,14,10)
  twocub(mns,m=1)
a) Include the output in Word, print out the output and label the effects on the output with the corresponding effects from a 24鈭? fractional factorial
IV
design.
b) Include the QQ plot in Word. Print out the plot. Which effects (from
the fractional factorial design) seem to be significant?
","Diagnostic, Hypothesis Testing,Computational statistics"
LR_DO_ch8_P14,"a) Download lregpack into R, and type the following commands. mns <- c(14,16,8,22,19,37,20,38,1,8,4,10,12,30,13,30)
  twofourth(mns)
This is the Ledolter and Swersey (2007, p. 80) cracked pots 24 data and the response and residual plots are from the model without 3 and 4 factor interactions.
b) Copy the plots into Word and print the plots. Do the response and residual plots look ok?","Diagnostic, Hypothesis Testing,Computational statistics,Estimation"
LR_DO_ch8_P15,"Download lregpack into R. The data is the PB(12) example from
Box et al. (2005, p. 287).
a) Type the following commands. Copy and paste the QQ plot into Word
and print the plot.
  resp <- c(56,93,67,60,77,65,95,49,44,63,63,61)
  pb12(resp,k=5)
b) Copy and paste the output into Notepad and print the output.
c) As a 25 design, the effects B, D, BD, E, and DE were thought to be real. The PB(12) design works best when none of the interactions is significant. From the QQ plot and the output for the PB(12) design, which factors, if any, appear to be significant?
d) The output gives the A, B, C, D, and E effects along with the cor- responding least squares coefficients 尾藛1, ..., 尾藛5. What is the relationship between the coefficients and the effects?
For parts e) to g), act as if the PB(12) design with 5 factors is appropriate.
e) The full model has Y藛 = 尾藛0 + 尾藛1x1 + 尾藛2x2 + 尾藛3x3 + 尾藛4x4 + 尾藛5x5. The reduced model is Y藛 = 尾藛0 + 尾藛j xj where xj is the significant term found in c). Give the numerical formula for the reduced model.
f) Compute Y藛 using the full model if xi = 1 for i = 1,...,5. Then compute Y藛 using the reduced model if xj = 1.
g) If the goal of the experiment is to produce large values of Y , should xj = 1 or xj = 鈭? in the reduced model? Explain briefly.","Diagnostic, Hypothesis Testing,Computational statistics,Estimation"
LR_DO_ch9_P1,"The ANOVA table above is for the Montgomery (1984, pp. 386鈥?89) split plot data where the whole plots are assigned to factor A and to blocks in a completely randomized block design. The response variable is tensile strength of paper. Factor A is (preparation) method with 3 levels (1, 2, 3). Factor B is temperature with 4 levels (200, 225, 250, 275). The pilot plant can make 12 runs a day and the experiment is repeated each day, with days as blocks. A batch of pulp is made by one of the 3 preparation methods. Then the batch of pulp is divided into 4 samples, and each sample is cooked at one of the four temperatures.
a) Perform the test corresponding to A. b) Perform the test corresponding to B. c) Perform the test corresponding to AB.",Hypothesis Testing
LR_DO_ch9_P2,"The ANOVA table above is for the Kuehl (1994, pp. 473鈥?81) split plot data where the whole plots are assigned to factor A and to blocks in a completely randomized block design. The response variable is the average chlorophyll content (mg/gm of turf grass clippings). Factor A is nitrogen fertilizer with 4 levels (1, 2, 3, 4). Factor B is length of time that thatch was allowed to accumulate with 3 levels (2, 5, or 8 years).
There were 2 blocks of 4 whole plots to which the levels of factor A were assigned. The 2 blocks formed a golf green which was seeded with turf grass. The 8 whole plots were plots of golf green. Each whole plot had 3 subplots to which the levels of factor B were randomly assigned.
a) Perform the test corresponding to A. b) Perform the test corresponding to B. c) Perform the test corresponding to AB.",Hypothesis Testing
LR_DO_ch9_P3,"The ANOVA table above is for the Snedecor and Cochran (1967, pp. 369鈥?72) split plot data where the whole plots are assigned to factor A and to blocks in a completely randomized block design. Factor A = variety of alfalfa (ladak, cossack, ranger). Each field had two cuttings, with the second cutting on July 7, 1943. Factor B = date of third cutting (none, Sept. 1, Sept. 20, Oct. 7) in 1943. The response variable was yield (tons per acre) in 1944. The 6 blocks were fields of land divided into 3 plots of land, one for each variety. Each of these 3 plots was divided into 4 subplots for date of third cutting. So each block had 3 whole plots and 12 subplots.
a) Perform the test corresponding to A. b) Perform the test corresponding to B. c) Perform the test corresponding to AB.",Hypothesis Testing
LR_DO_ch9_P4,"Following Montgomery (1984, pp. 386鈥?89), suppose the response variable is tensile strength of paper. One factor is (preparation) method with 3 levels (1, 2, 3). Another factor is temperature with 4 levels (200, 225, 250, 275).
a) Suppose the pilot plant can make 12 runs a day and the experiment is repeated each day, with days as blocks. A batch of pulp is made by one of the 3 preparation methods. Then the batch of pulp is divided into 4 samples, and each sample is cooked at one of the four temperatures. Which factor, method, or temperature is assigned to subplots?
b) Suppose the pilot plant could make 36 runs in one day. Suppose that 9 batches of pulp are made, that each batch of pulp is divided into 4 samples, and each sample is cooked at one of the four temperatures. How should the 9 batches be allocated to the three preparation methods, and how should the 4 samples be allocated to the four temperatures?
c) Suppose the pilot plant can make 36 runs in one day and that the units are 36 batches of material to be made into pulp. Each of the 12 method tem- perature combinations is to be replicated 3 times. What type of experimental design should be used? (Hint: not a split plot.)

",Hypothesis Testing
LR_DO_ch9_P5,"a) Download (http://lagrange.math.siu.edu/Olive/lregdata.txt) into R, and type the following commands. Then copy and paste the output into Notepad and print the output.
  attach(guay)
  out<-aov(plants~variety*treatment + Error(flats),guay)
  summary(out)
  detach(guay)
This split plot data is from Chambers and Hastie (1993, p. 158). There are 8 varieties of guayule (rubber plant) and 4 treatments were applied to seeds. The response was the rate of germination. The whole plots were greenhouse flats and the subplots were subplots of the flats. Each flat received seeds of one variety (A). Each subplot contained 100 seeds and was treated with one of the treatments (B). There were m = 3 replications so each variety was planted in 3 flats for a total of 24 flats and 4(24) = 96 observations.
b) Use the output to test whether the response depends on variety.","Computational statistics, Hypothesis Testing"
LR_DO_ch9_P6,"Download (http://lagrange.math.siu.edu/Olive/lregdata.txt) into R, and type the following commands. Then copy and paste the output into Notepad and print the output. This split plot steel data is from Box et al. (2005, p. 336). The whole plots are time slots to use a furnace, which can hold 4 steel bars at one time. Factor A = heat has 3 levels (360, 370, 380o F). Factor B = coating has 4 levels (4 types of coating: c1, c2, c3, and c4). The response was corrosion resistance.
a) Perform the test corresponding to A. b) Perform the test corresponding to B. c) Perform the test corresponding to AB.

  attach(steel)
  out<-aov(resistance~heat*coating + Error(wplots),steel)
  summary(out)
  detach(steel)","Computational statistics, Hypothesis Testing"
LR_DO_ch9_P7,"This is the same data as in Problem 9.6, using SAS. Copy and paste the SAS program from (http://lagrange.math.siu.edu/Olive/lrsashw.txt) into SAS, run the program, then print the output. Only include the second page of output.
To get the correct F statistic for heat, you need to divide MS heat by MS wplots.
","Computational statistics, Hypothesis Testing"
LR_DO_ch9_P8,"a) Copy and paste the SAS program from (http://lagrange.math.siu.edu/Olive/lrsashw.txt) into SAS, run the pro- gram, then print the output. Only include the second page of output.
This data is from the SAS Institute (1985, pp. 131鈥?32). The B and AB ANOVA table entries are correct, but the correct entry for A is the last line of output where Block*A is used as the error.
b) Perform the test corresponding to A. c) Perform the test corresponding to B. d) Perform the test corresponding to AB.","Computational statistics, Hypothesis Testing"
LR_DO_ch10_P1,"a) Find the distribution of X2.
b) Find the distribution of (X1,X3)T.
c) Which pairs of random variables Xi and Xj are independent? d) Find the correlation 蟻(X1,X3).

","Probablity distribution, Linear Algebra"
LR_DO_ch10_P2,"Recall that if X 鈭?Np(渭, 危), then the conditional distribution of
X given that X = x is multivariate normal with mean 渭 +危 危鈭?(x 鈭?1 2 2 1 12 22 2
渭 ) and covariance matrix 危 鈭?危 危鈭?危
.
2
11 122221
Let 蟽12 = Cov(Y,X) and suppose Y and X follow a bivariate normal distribution a) If 蟽12 = 0, find Y |X. Explain your reasoning. b)If蟽12 =10findE(Y|X).
c) If 蟽12 = 10, find Var(Y |X).

","Probablity distribution, Linear Algebra"
LR_DO_ch10_P3,"a)If蟽12 =10findE(Y|X).
b) If 蟽12 = 10, find Var(Y |X).
c) If 蟽12 = 10, find 蟻(Y,X), the correlation between Y and X.","Probablity distribution, Linear Algebra"
LR_DO_ch10_P4,"where c > 0 and 0 < 纬 < 1. Following Example 10.2, show that X has an elliptically contoured distribution assuming that all relevant expectations exist.

","Probablity distribution, Linear Algebra"
LR_DO_ch10_P5,"In Proposition 10.5b, show that if the second moments exist, then 危 can be replaced by Cov(X).","Probablity distribution, Linear Algebra"
LR_DO_ch10_P6,"The table (W) above represents 3 head measurements on 6 people and one ape. Let X1 = cranial capacity, X2 = head length, and X3 = head height. Let x = (X1, X2, X3)T . Several multivariate location estimators, in- cluding the coordinatewise median and sample mean, are found by applying a univariate location estimator to each random variable and then collecting the results into a vector. a) Find the coordinatewise median MED(W).","Probablity distribution, Linear Algebra"
LR_DO_ch10_P7,"Using the notation in Proposition 10.6, show that if the second
moments exist, then","Probablity distribution, Linear Algebra"
LR_DO_ch10_P8,"Using the notation under Lemma 10.4, show that if X is elliptically contoured, then the conditional distribution of X1 given that X2 = x2 is also elliptically contoured.

","Probablity distribution, Linear Algebra,Multiple Linear Regression"
LR_DO_ch10_P9,"Suppose Y 鈭?Nn(X尾, 蟽2I). Find the distribution of (XTX)鈭?XTY if X is an n脳p full rank constant matrix and 尾 is a p脳1 constant vector.","Probablity distribution, Linear Algebra"
LR_DO_ch10_P10,"Recall that Cov(X, Y ) = E[(X 鈭扙(X))(Y 鈭扙(Y ))T ]. Using the notation of Proposition 10.6, let (Y, XT )T be ECp+1(渭, 危, g) where Y is a random variable. Let the covariance matrix of (Y, XT ) be: where c is some positive constant. Show that E(Y |X) = 伪 + 尾T X where","Probablity distribution, Linear Algebra"
LR_DO_ch10_P11,"Let X be a p脳1 random vector with E(X) = 0 and Cov(X) = 危. Let B be any constant full rank p 脳 r matrix where 1 鈮?r 鈮?p. Suppose that for all such conforming matrices B,
E(X|BT X)=MBBTX
where M B a p 脳 r constant matrix that depend on B.
Using the fact that 危B = Cov(X,BTX) = E(XXTB) =
E[E(XXTB|BTX)], compute 危B and show that MB = 危B(BT 危B)鈭?. Hint: what acts as a constant in the inner expectation?

","Probablity distribution, Linear Algebra"
LR_DO_ch10_P12,"a) Download the maha function that creates the classical Maha-
lanobis distances.
b) Enter the following commands and check whether observations 1鈥?0 look like outliers.
  simx2 <- matrix(rnorm(200),nrow=100,ncol=2)
  outx2 <- matrix(10 + rnorm(80),nrow=40,ncol=2)
  outx2 <- rbind(outx2,simx2)
  maha(outx2)","Computational Statistics, Diagnostic, Linear Algebra"
LR_DO_ch10_P13,"a) Assuming that you have done the two source commands above Problem 10.12 (and the library(MASS) command), type the command ddcomp(buxx). This will make 4 DD plots based on the DGK, FCH, FMCD, and median ball estimators. The DGK and median ball estimators are the two attractors used by the FCH estimator. With the leftmost mouse button, move the cursor to each outlier and click. This data is the Buxton (1920) data and cases with numbers 61, 62, 63, 64, and 65 were the outliers with head lengths near 5 feet. After identifying the outliers in each plot, hold the rightmost mouse button down and click on Stop to advance to the next plot. When done, hold down the Ctrl and c keys to make a copy of the plot. Then paste the plot in Word.
b) Repeat a) but use the command ddcomp(cbrainx). This data is the Gladstone (1905) data and some infants are multivariate outliers.
c) Repeat a) but use the command ddcomp(museum[,-1]). This data is the Schaaffhausen (1878) skull measurements and cases 48鈥?0 were apes while the first 47 cases were humans.

","Computational Statistics, Diagnostic"
LR_DO_ch11_P1,"Suppose Yi = xTi 尾 + 蔚i where the errors are independent N(0,蟽2). Then the likelihood function is
魪苯 鈭? 魪本 L(尾, 蟽2) = (2蟺蟽2)鈭抧/2 exp 2蟽2 鈭 鈭?X尾鈭?
.
 a) Since the least squares estimator 尾藛 minimizes 鈭 鈭?X尾鈭?, show that 尾藛 is the MLE of 尾.
b) Then find the MLE 蟽藛2 of 蟽2.","Estimation, Probability distribution,Multiple Linear Regression"
LR_DO_ch11_P2,"Suppose Yi = xTi 尾 + ei where the errors are iid double exponential
(0, 蟽) where 蟽 > 0. Then the likelihood function is 魪被魪备n 魪奔
L ( 尾 , 蟽 ) = 1 1 e x p 鈭?1 | Y i 鈭?x Ti 尾 | . 2n 蟽n 蟽 i=1
   Suppose that 尾 虄 is a minimizer of 魪北ni=1 |Yi 鈭?xTi 尾|.
a) By direct maximization, show that 尾 虄 is an MLE of 尾 regardless of the
value of 蟽.
b) Find an MLE of 蟽 by maximizing
魪被魪备n 魪奔 L ( 蟽 ) 鈮?L ( 尾 虄 , 蟽 ) = 1 1 e x p 鈭?1 | Y i 鈭?x Ti 尾 虄 | .","Estimation, Probability distribution,Multiple Linear Regression"
LR_DO_ch11_P3,"Suppose Yi = xTi 尾 + 蔚i where the errors are independent N(0,蟽2/wi) where wi > 0 are known constants. Then the likelihood func- tion is a) Suppose that 尾藛W minimizes 魪北ni=1 wi(yi 鈭抶Ti 尾)2. Show that 尾藛W is the
MLE of 尾.
b) Then find the MLE 蟽藛2 of 蟽2.","Estimation, Probability distribution,Multiple Linear Regression"
LR_DO_ch11_P4,"Suppose Y 鈭?Nn(X尾, 蟽2V ) for known positive definite n 脳 n ma- trix V . Then the likelihood function is
魪苯1魪本n 1 1 魪苯鈭? 魪本 L(尾,蟽2)= 鈭?蟺 |V|1/2 蟽n exp 2蟽2(y鈭扻尾)TV鈭?(y鈭扻尾) .
a) Suppose that 尾藛G minimizes (y 鈭?X尾)T V 鈭?(y 鈭?X尾). Show that 尾藛G is the MLE of 尾.
b) Find the MLE 蟽藛2 of 蟽2.","Estimation, Probability distribution,Multiple Linear Regression"
LR_DO_ch11_P5,"Find the vector a such that aT Y is an unbiased estimator for E(Yi)
     if the usual linear model holds.","Estimation, Probability distribution,Linear Algebra"
LR_DO_ch11_P6,"Write the following quantities as bT Y or Y T AY or AY .
a)Y, b)魪北(Yi鈭扽藛i)2, c)魪北(Y藛i)2, d)尾藛, e)Y藛 ii
 ","Estimation, Probability distribution,Linear Algebra"
LR_DO_ch11_P7,"Show that I 鈭?H = I 鈭?X(XT X)鈭?XT is idempotent, that is, showthat(I鈭扝)(I鈭扝)=(I鈭扝)2 =I鈭扝.",Linear Algebra
LR_DO_ch11_P8,"LetY 鈭糔(渭,蟽2)sothatE(Y)=渭andVar(Y)=蟽2 =E(Y2)鈭?[E(Y )]2. If k 鈮?2 is an integer, then
E(Y k) = (k 鈭?1)蟽2E(Y k鈭?) + 渭E(Y k鈭?).
Let Z = (Y 鈭捨?/蟽 鈭?N(0,1). Hence 渭k = E(Y 鈭捨?k = 蟽kE(Zk). Use this fact and the above recursion relationship E(Zk) = (k 鈭?1)E(Zk鈭?) to find a) 渭3 and b) 渭4.

","Estimation, Probability distribution,Linear Algebra"
LR_DO_ch11_P9,"Let A and B be matrices with the same number of rows. If C is another matrix such that A = BC, is it true that rank(A) = rank(B)? Prove or give a counterexample.",Linear Algebra
LR_DO_ch11_P10,"Let x be an n脳1 vector and let B be an n脳n matrix. Show that xTBx = xTBTx.
(The point of this problem is that if B is not a symmetric n 脳 n matrix, T T B+BT
thenx Bx=x AxwhereA= 2 isasymmetricn脳nmatrix.)",Linear Algebra
LR_DO_ch11_P11,"Consider the model Yi = 尾1 +尾2Xi,2 +路 路 路+尾pXi,p +ei = xTi 尾+ei.
 The least squares estimator 尾藛 minimizes 魪备n Q O L S ( 畏 ) =
( Y i 鈭?x Ti 畏 ) 2 and the weighted least squares estimator minimizes where the wi,Yi and xi are known quantities. Show that

","Linear Algebra, Estimation, Multiple Linear regression"
LR_DO_ch11_P12,"Suppose that X is an n脳p matrix but the rank of X < p < n. Then the normal equations XT X尾 = XT Y have infinitely many solutions. Let 尾藛 be a solution to the normal equations. So XTX尾藛 = XTY. Let G = (XT X)鈭?be a generalized inverse of (XT X). Assume that E(Y ) = X尾 and Cov(Y ) = 蟽2I. It can be shown that all solutions to the normal equations have the form bz given below.
a)Showthatbz=GXTY +(GXTX鈭扞)zisasolutiontothenormal equations where the p 脳 1 vector z is arbitrary.
b) Show that E(bz) 谈= 尾.
(Hence some authors suggest that bz should be called a solution to the normal equations but not an estimator of 尾.)
c) Show that Cov(bz) = 蟽2GXT XGT .
d) Although G is not unique, the pro jection matrix P = X GX T onto C(X) is unique. Use this fact to show that Y藛 = Xbz does not depend on G or z.
e) There are two ways to show that aT 尾 is an estimable function. Either show that there exists a vector c such that E(cT Y ) = aT 尾, or show that a 鈭?C(XT ). Suppose that a = XT w for some fixed vector w. Show that E(aT bz) = aT 尾.
(Hence aT 尾 is estimable by aT bz where bz is any solution of the normal equations.)
f) Suppose that a = XTw for some fixed vector w. Show that Var(aTbz) = 蟽2wTPw.","Linear Algebra, Estimation, Multiple Linear regression"
LR_DO_ch12_P1,"Consider the Hotelling Lawley test statistic. Let T(W)=n [vec(LB藛)]T[危藛鈭? 鈯?LWLT)鈭?][vec(LB藛)].
蔚 S h o w T ( W藛 ) = [ v e c ( L B藛 ) ] T [ 危藛 鈭?1 鈯?( L ( X T X ) 鈭?1 L T ) 鈭?1 ] [ v e c ( L B藛 ) ] .

",Linear Algebra
LR_DO_ch12_P2,"Consider the Hotelling Lawley test statistic. Let T =
[vec(LB藛 )]T [危藛 鈭? 鈯?(L(XT X)鈭?LT )鈭?][vec(LB藛 )]. 蔚 Let L = Lj = [0,...,0,1,0,...,0] have a 1 in the jth position. Let b藛Tj = LB藛
be the jth row of B藛 . Let dj = Lj(XT X)鈭?LTj = (XT X)鈭?, the jth diagonal jj
entry of (X T X )鈭? . Then T = 1 b藛T 危藛 鈭? b藛 . The Hotelling Lawley statistic j dj j 蔚 j
 U = tr([(n 鈭?p)危藛 蔚]鈭?B藛 T LT [L(XT X)鈭?LT ]鈭?LB藛 ]). HenceifL=L,thenU = 1 tr(危藛鈭?b藛b藛T).
j jdj(n鈭抪)蔚jj
Using tr(ABC) = tr(CAB) and tr(a) = a for scalar a, show that
 (n鈭抪)Uj =Tj.","Linear Algebra, Hypothesis Testing"
LR_DO_ch12_P3,"Consider the Hotelling Lawley test statistic. Using the Searle (1982,
p. 333) identity
tr(AGT DGC) = [vec(G)]T [CA 鈯?DT ][vec(G)],
show (n 鈭?p)U(L) = tr[危藛 鈭?B藛 TLT[L(XTX)鈭?LT]鈭?LB藛 ] 蔚
= [vec(LB藛 )]T [危藛 鈭? 鈯?(L(XT X)鈭?LT )鈭?][vec(LB藛 )] by identifying A, G, D, 蔚
and C.","Computational Statistics, Hypothesis Testing"
LR_DO_ch12_P4,"The above output is for the R Seatbelts data set where Y1 = drivers = number of drivers killed or seriously injured, Y2 = front = number of front seat passengers killed or seriously injured, and Y3 = back = num- ber of back seat passengers killed or seriously injured. The predictors were x2 = kms = distance driven, x3 = price = petrol price, x4 = van = number of van drivers killed, and x5 = law = 0 if the law was in effect that month and 1 otherwise. The data consists of 192 monthly totals in Great Britain from January 1969 to December 1984, and the compulsory wearing of seat belts law was introduced in February 1983.
a) Do the MANOVA F test. b) Do the F4 test.","Linear Algebra, Hypothesis Testing"
LR_DO_ch12_P5,"Sketch a DD plot of the residual vectors 藛蔚i for the multivariate linear regression model if the error vectors 蔚i are iid from a multivariate normal distribution.
  y<-USJudgeRatings[,c(9,10,12)]
  x<-USJudgeRatings[,-c(9,10,12)]
  mltreg(x,y,indices=c(2,5,6,7,8))
  $partial
       partialF      Pval
  [1,] 1.649415 0.1855314
  $MANOVA
        MANOVAF         pval
  [1,] 340.1018 1.121325e-14","Multiple Linear Regression, Hypothesis Testing"
LR_DO_ch12_P6,"The above output is for the R judge ratings data set consisting of lawyer ratings for n = 43 judges. Y1 = oral = sound oral rulings, Y2 = writ = sound written rulings, and Y3 = rten = worthy of retention. The predictors were x2 = cont = number of contacts of lawyer with judge, x3 = intg = judicial integrity, x4 = dmnr = demeanor, x5 = dilg = diligence, x6 = cfmg = case flow managing, x7 = deci = prompt decisions, x8 = prep = preparation for trial, x9 = fami = familiarity with law, and x10 = phys = physical ability.
a) Do the MANOVA F test.
b) Do the MANOVA partial F test for the reduced model that deletes
x2, x5, x6, x7, and x8.","Linear Algebra, Hypothesis Testing, Computational Statistics"
LR_DO_ch12_P7,Let 尾i be p 脳 1 and suppose,Probability distribution
LR_DO_ch12_P8,"This problem examines multivariate linear regression on the Cook and Weisberg (1999a) mussels data with Y1 = log(S) and Y2 = log(M) where S is the shell mass and M is the muscle mass. The predictors are X2 = L, X3 = log(W), and X4 = H: the shell length, log(width), and height.
a) The R command for this part makes the response and residual plots for each of the two response variables. Click the rightmost mouse button and highlight Stop to advance the plot. When you have the response and residual plots for one variable on the screen, copy and paste the two plots into Word. Do this two times, once for each response variable. The plotted points fall in roughly evenly populated bands about the identity or r = 0 line.
b) Copy and paste the output produced from the R command for this part from $partial on. This gives the output needed to do the MANOVA F test, MANOVA partial F test, and the Fj tests.
c) The R command for this plot makes a DD plot of the residuals and adds the lines corresponding to the three prediction regions of Section 12.3. The robust cutoff is larger than the semiparametric cutoff. Place the plot in Word. Do the residuals appear to follow a multivariate normal distribution?
d) Do the MANOVA partial F test where the reduced model deletes X3 and X4.
e) Do the F2 test.
f) Do the MANOVA F test.

","Diagnostic,Computational Statistics"
LR_DO_ch12_P9,"This problem examines multivariate linear regression on the SAS Institute (1985, p. 146) Fitness Club Data data with Y1 = chinups, Y2 = situps, and Y3 = jumps. The predictors are X2 = weight, X3 = waist, and X4 = pulse.
a) The R command for this part makes the response and residual plots for each of the three variables. Click the rightmost mouse button and highlight Stop to advance the plot. When you have the response and residual plots for one variable on the screen, copy and paste the two plots into Word. Do this three times, once for each response variable. Are there any outliers?
b) The R command for this part makes a DD plot of the residuals and adds the lines corresponding to the three prediction regions of Section 12.3. The robust cutoff is larger than the semiparametric cutoff. Place the plot in Word. Are there any outliers? (Right click Stop once.)

"," Hypothesis Testing, Computational Statistics"
LR_DO_ch12_P10,"This problem uses the lregpack function mregsim to simulate the Wilks鈥?螞 test, Pillai鈥檚 trace test, Hotelling Lawley trace test, and Roy鈥檚 largest root test for the Fj tests and the MANOVA F test for multivariate linear regression. When mnull = T the first row of B is 1T while the remaining rows are equal to 0. Hence the null hypothesis for the MANOVA F test is true. When mnull = F the null hypothesis is true for p = 2, but false for p>2.NowthefirstrowofBis1T andthelastrowofBis0.Ifp>2, then the second to last row of B is (1, 0, ..., 0), the third to last row is (1, 1, 0, ..., 0) et cetera as long as the first row is not changed from 1T. First m iid errors zi are generated such that the m errors are iid with variance 蟽2. Then 蔚i = Azi so that 危藛 蔚 = 蟽2AAT = (蟽ij ) where the diagonal entries 蟽ii = 蟽2[1 + (m 鈭?1)蠄2] and the off diagonal entries 蟽ij = 蟽2[2蠄 + (m 鈭?2)蠄2] where 蠄 = 0.10. Terms like Wilkcov give the percentage of times the Wilks鈥?test rejected the F1, F2, . . . , Fp tests. The $mancv wcv pcv hlcv rcv fcv output gives the percentage of times the 4 test statistics reject the MANOVA F test. Here hlcov and fcov both correspond to the Hotelling Lawley test using the formulas in Problem 12.3.
5000 runs will be used so the simulation may take several minutes. Sample sizes n = (m+p)2,n = 3(m+p)2, and n = 4(m+p)2 were interesting. We want coverage near 0.05 when H0 is true and coverage close to 1 for good power when H0 is false. Multivariate normal errors were used in a) and b) below.
a) Copy the coverage parts of the output produced by the R commands forthispartwheren=20,m=2,andp=4.HereH0 istrueexceptfor the F1 test. Wilks鈥?and Pillai鈥檚 tests had low coverage < 0.05 when H0 was false. Roy鈥檚 test was good for the Fj tests, but why was Roy鈥檚 test bad for the MANOVA F test?
b) Copy the coverage parts of the output produced by the R commands forthispartwheren=20,m=2,andp=4.HereH0 isfalseexceptforthe F4 test. Which two tests seem to be the best for this part?","Linear Algebra, Hypothesis Testing, Computational Statistics"
LR_DO_ch12_P11,"This problem uses the lregpack function mpredsim to simulate the prediction regions for yf given xf for multivariate regression. With 5000 runs this simulation may take several minutes. The R command for this problem generates iid lognormal errors then subtracts the mean, producing zi. Then the 蔚i = Azi are generated as in Problem 12.10 with n=100, m=2, and p=4. The nominal coverage of the prediction region is 90%, and 92% of the training data is covered. The ncvr output gives the coverage of the nonparametric region. What was ncvr?","Linear Algebra, Hypothesis Testing, Computational Statistics"
LR_DO_ch13_P1,"Consider trying to estimate the proportion of males from a popu- lation of males and females by measuring the circumference of the head. Use the above logistic regression output to answer the following problems.
a) Predict 蟻藛(x) if x = 550.0. b) Find a 95% CI for 尾.
c) Perform the 4 step Wald test for Ho : 尾 = 0.

","Prediction, Hypothesis Testing, Inference"
LR_DO_ch13_P2,"Now the data is as in Problem 13.1, but try to estimate the pro- portion of males by measuring the circumference and the length of the head. Use the above logistic regression output to answer the following problems.
a) Predict 蟻藛(x) if circumference = x1 = 550.0 and length = x2 = 200.0. b) Perform the 4 step Wald test for Ho : 尾1 = 0.
c) Perform the 4 step Wald test for Ho : 尾2 = 0.

",Logistic Regression
LR_DO_ch13_P3,"A museum has 60 skulls of apes and humans. Lengths of the lower jaw, upper jaw, and face are the explanatory variables. The response variable is ape (= 1 if ape, 0 if human). Using the output above, perform the four step deviance test for whether there is an LR relationship between the response variable and the predictors.","Prediction, Hypothesis Testing, Inference"
LR_DO_ch13_P4,"Suppose the full model is as in Problem 13.3, but the reduced model omits the predictor face length. Perform the 4 step change in deviance test to examine whether the reduced model can be used.
The following three problems use the possums data from Cook and Weis- berg (1999a).

","Prediction, Hypothesis Testing, Inference"
LR_DO_ch13_P5,"Use the above output to perform inference on the number of pos- sums in a given tract of land. The output is from a Poisson regression.
a) Predict 渭藛(x) if habitat = x1 = 5.8 and stags = x2 = 8.2. b) Perform the 4 step Wald test for Ho : 尾1 = 0.
c) Find a 95% confidence interval for 尾2.

","Prediction, Hypothesis Testing, Inference,Poisson distribution"
LR_DO_ch13_P6,"Perform the 4 step deviance test for the same model as in Prob-
lem 13.5 using the output above.

",Diagnostic
LR_DO_ch13_P7,Let the reduced model be as in Problem 13.5 and use the output for the full model be shown above. Perform a 4 step change in deviance test.,"Diagnostic, Hypothesis Testing"
LR_DO_ch13_P8,"The above table gives summary statistics for 4 models considered as final submodels after performing variable selection. (Several of the predic- tors were factors, and a factor was considered to have a bad Wald p-value > 0.05 if all of the dummy variables corresponding to the factor had p-values > 0.05. Similarly the factor was considered to have a borderline p-value with 0.01 鈮?p-value 鈮?0.05 if none of the dummy variables corresponding to the factor had a p-value < 0.01 but at least one dummy variable had a p-value between 0.01 and 0.05.) The response was binary and logistic regression was used. The response plot for the full model B1 was good. Model B2 was the minimum AIC model found. There were 1000 cases: for the response, 300 were 0s and 700 were 1s.a) For the change in deviance test, if the p-value 鈮?0.07, there is little evidence that Ho should be rejected. If 0.01 < p-value < 0.07, then there is moderate evidence that Ho should be rejected. If p-value 鈮?0.01, then there is strong evidence that Ho should be rejected. For which models, if any, is there strong evidence that 鈥淗o: reduced model is good鈥?should be rejected.
446 13 GLMs and GAMs b) For which plot is 鈥渃orr(B1:ETA鈥橴,Bi:ETA鈥橴)鈥?(using notation from
Arc: 畏T u instead of 尾T x) relevant?
c) Which model should be used as the final submodel? Explain briefly why
each of the other 3 submodels should not be used.
",Diagnostic
LR_DO_ch13_P9,"Activate the banknote.lsp dataset with the menu commands
鈥淔ile > Load > Data > banknote.lsp.鈥?Scroll up the screen to read the data description. Twice you will fit logistic regression models and include the coefficients in Word. Print out this output when you are done and include the output with your homework.
From Graph&Fit select Fit binomial response. Select Top as the predictor, Status as the response, and ones as the number of trials.
a) Include the output in Word.
b) Predict 蟻藛(x) if x = 10.7.
c) Find a 95% CI for 尾.
d) Perform the 4 step Wald test for Ho : 尾 = 0.
e) From Graph&Fit select Fit binomial response. Select Top and Diagonal as predictors, Status as the response, and ones as the number of trials. Include the output in Word.
f) Predict 蟻藛(x) if x1 = Top = 10.7 and x2 = Diagonal = 140.5. g) Find a 95% CI for 尾1.
h) Find a 95% CI for 尾2.
i) Perform the 4 step Wald test for Ho : 尾1 = 0.
j) Perform the 4 step Wald test for Ho : 尾2 = 0.","Hypothesis Testing, Inference, Prediction, Computational Statistics"
LR_DO_ch13_P10,"Activate banknote.lsp in Arc. with the menu commands
鈥淔ile > Load > Data > banknote.lsp.鈥?Scroll up the screen to read the data description. From Graph&Fit select Fit binomial response. Select Top and Diagonal as predictors, Status as the response, and ones as the number of trials.
a) Include the output in Word.
13.10 Problems 447
b) From Graph&Fit select Fit linear LS. Select Diagonal and Top for pre- dictors, and Status for the response. From Graph&Fit select Plot of and select L2:Fit-Values for H, B1:Eta鈥橴 for V, and Status for Mark by. Include the plot
in Word. Is the plot linear? How are 伪藛OLS +尾藛TOLSx and 伪藛logistic +尾藛Tlogisticx related (approximately)?

","Multiple Linear Regression,Computational Statistics, "
LR_DO_ch13_P11,"Activate possums.lsp in Arc with the menu commands
鈥淔ile > Load > Data > possums.lsp.鈥?Scroll up the screen to read the data description.
a) From Graph&Fit select Fit Poisson response. Select y as the response and select acacia, bark, habitat, shrubs, stags, and stumps as the predictors. Include the output in Word. This is your full model.
b) Response plot: From Graph&Fit select Plot of. Select P1:Eta鈥橴 for the H box and y for the V box. From the OLS popup menu select Poisson and move the slider bar to 1. Move the lowess slider bar until the lowess curve tracks the exponential curve well. Include the response plot in Word.
c) From Graph&Fit select Fit Poisson response. Select y as the response and select bark, habitat, stags, and stumps as the predictors. Include the output in Word.
d) Response plot: From Graph&Fit select Plot of. Select P2:Eta鈥橴 for the H box and y for the V box. From the OLS popup menu select Poisson and move the slider bar to 1. Move the lowess slider bar until the lowess curve tracks the exponential curve well. Include the response plot in Word.
e) Deviance test. From the P2 menu, select Examine submodels and click on OK. Include the output in Word and perform the 4 step deviance test.
f) Perform the 4 step change of deviance test.
g) EE plot. From Graph&Fit select Plot of. Select P2:Eta鈥橴 for the H box and P1:Eta鈥橴 for the V box. Move the OLS slider bar to 1. Click on the Options popup menu and type 鈥測=x鈥? Include the plot in Word. Is the plot linear?","Logistic Regression,Computational Statistics"
LR_DO_ch13_P12,"In this problem you will find a good submodel for the possums data.
Activate possums.lsp in Arc with the menu commands
鈥淔ile > Load > Data > possums.lsp.鈥?Scroll up the screen to read the data description.
From Graph&Fit select Fit Poisson response. Select y as the response and select Acacia, bark, habitat, shrubs, stags, and stumps as the predictors.
In Problem 13.11, you showed that this was a good full model.a) Using what you have learned in class find a good submodel and include
the relevant output in Word.
(Hints: Use forward selection and backward elimination and find the model Imin with the smallest AIC. Let 螖(I) = AIC(I)鈭扐IC(Imin). Then find the model II with the fewest number of predictors such that 螖(II) 鈮?2. Then submodel II is the initial submodel to examine. Fit model II and look at the Wald test p鈥搗alues. Try to eliminate predictors with large p鈥搗alues but make sure that the deviance does not increase too much. Also examine submodels I with fewer predictors than II with 螖(I) 鈮?7. You may have several models, say P2, P3, P4, and P5 to look at. Make a scatterplot matrix of the Pi:ETA鈥橴 from these models and from the full model P1. Make the EE and response plots for each model. The correlation in the EE plot should be at least 0.9 and preferably greater than 0.95. As a very rough guide for Poisson regression, the number of predictors in the full model should be less than n/5 and the number of predictors in the final submodel should be less than n/10.)
b) Make a response plot for your final submodel, say P2. From Graph&Fit select Plot of. Select P2:Eta鈥橴 for the H box and y for the V box. From the OLS popup menu select Poisson and move the slider bar to 1. Move the lowess slider bar until the lowess curve tracks the exponential curve well. Include the response plot in Word.
c) Suppose that P1 contains your full model and P2 contains your final submodel. Make an EE plot for your final submodel: from Graph&Fit select Plot of. Select P1:Eta鈥橴 for the V box and P2:Eta鈥橴, for the H box. After the plot appears, click on the options popup menu. A window will appear. Type y = x and click on OK. This action adds the identity line to the plot. Also move the OLS slider bar to 1. Include the plot in Word.
d) Using a), b), c), and any additional output that you desire (e.g., AIC(full), AIC(Imin), AIC(II), and AIC(final submodel), explain why your final submodel is good.
Warning: The following 5 problems use data from the book鈥檚 webpage. Save the data files on a flash drive. Get in Arc and use the menu commands 鈥淔ile > Load鈥?and a window with a Look in box will appear. Click on the black triangle and then on Removable Disk (G:). Then click twice on the data set name.
","Diagnostic,Computational Statistics,Logistic Regression"
LR_DO_ch13_P13,"(Response Plot): Activate cbrain.lsp in Arc with the menu com- mands 鈥淔ile > Load > Removable Disk (G:) > cbrain.lsp.鈥?Scroll up the screen to read the data description. From Graph&Fit select Fit binomial response. Select brnweight, cephalic, breadth, cause, size, and headht as pre- dictors, sex as the response, and ones as the number of trials. Perform the logistic regression and from Graph&Fit select Plot of. Place sex on V and B1:Eta鈥橴 on H. From the OLS popup menu, select Logistic and move the
13.10 Problems 449
slider bar to 1. From the lowess popup menu select SliceSmooth and move the slider bar until the fit is good. Include your plot in Word. Are the slice means (observed proportions) tracking the logistic curve (fitted proportions) very well? Use lowess if SliceSmooth does not work.
","Diagnostic,Computational Statistics"
LR_DO_ch13_P14,"Suppose that you are given a data set, told the response, and asked to build a logistic regression model with no further help. In this prob- lem, we use the cbrain data to illustrate the process.
a) Activate cbrain.lsp in Arc with the menu commands
鈥淔ile > Load > Removable Disk (G:) > cbrain.lsp.鈥?Scroll up the screen to read the data description. From Graph&Fit select Scatterplot-matrix of. Place sex in the Mark by box. Then select age, breadth, cause, cephalic, circum, headht, height, length, size, and sex. Include the scatterplot matrix in Word.
b) Use the menu commands 鈥渃brain>Make factors鈥?and select cause. This makes cause into a factor with 2 degrees of freedom. Use the menu commands 鈥渃brain>Transform鈥?and select age and the log transformation.
Why was the log transformation chosen?
c) From Graph&Fit select Plot of and select size in H. Also place sex in the Mark by box. A plot will come up. From the GaussKerDen menu (the triangle to the left) select Fit by marks, move the sliderbar to 0.9, and include the plot in Word.
d) Use the menu commands 鈥渃brain>Transform鈥?and select size and the log transformation. From Graph&Fit select Fit binomial response. Select age, log(age), breadth, {F}cause, cephalic, circum, headht, height, length, size, and log(size) as predictors, sex as the response, and ones as the number of trials. This is the full model B1. Perform the logistic regression and include the relevant output for testing in Word.
e) From Graph&Fit select Plot of. Place sex on V and B1:Eta鈥橴 on H. From the OLS popup menu, select Logistic and move the slider bar to 1. From the lowess popup menu select SliceSmooth and move the slider bar until the fit is good. Include your plot in Word. Are the slice means (observed proportions) tracking the logistic curve (fitted proportions) fairly well? (Use lowess if SliceSmooth does not work.)
f) From B1 select Examine submodels and select Add to base model (For- ward Selection). Include the output with the header 鈥淏ase terms: ...鈥?and from 鈥淎dd: length 259鈥?to 鈥淎dd: {F}cause 258鈥?in Word.
g) From B1 select Examine submodels and select Delete from full model (Backward Elimination). Include the output with df corresponding to the minimum AIC model in Word. What predictors does this model use?
450 13 GLMs and GAMs
h) As a final submodel B2, use the model from f): from Graph&Fit select Fit binomial response. Select age, log(age), circum, height, length, size, and log(size) as predictors, sex as the response, and ones as the number of trials. Perform the logistic regression and include the relevant output for testing in Word.
i) Put the EE plot H B2:Eta鈥橴 versus V B1:Eta鈥橴 in Word. Is the plot linear?
j) From Graph&Fit select Plot of. Place sex on V and B2:Eta鈥橴 on H. From the OLS popup menu, select Logistic and move the slider bar to 1. From the lowess popup menu select SliceSmooth and move the slider bar until the fit is good. Include your plot in Word. Are the slice means (observed proportions) tracking the logistic curve (fitted proportions) fairly well? (Use lowess if SliceSmooth does not work.)
k) Perform the 4 step change in deviance test using the full model in d) and the reduced submodel in h).
Now act as if the final submodel is the full model.
l) From B2 select Examine submodels, click OK, and include the output in
Word. Then use the output to perform a 4 step deviance test on the submodel.
m) From Graph&Fit select Inverse regression. Select age, log(age), circum, height, length, size, and log(size) as predictors, and sex as the response. From Graph&Fit select Plot of. Place I3:SIR.p1 on the H axis and B2:Eta鈥橴 on the V axis. Include the plot in Word. Is the plot linear?","Diagnostic,Computational Statistics,Multiple Linear Regression"
LR_DO_ch13_P15,"In this problem you will find a good submodel for the ICU data obtained from STATLIB or the text鈥檚 website. This data set will violate some of the rules of thumb: the model II does not have enough predictors to make a good EE plot. See Example 13.13.
a) Activate ICU.lsp in Arc with the menu commands
鈥淔ile > Load >.鈥?Then use the upper box to navigate to where ICU.lsp is stored, for example Removable Disk (G:). Scroll up the screen to read the data description.
b) Use the menu commands 鈥淚CU>Make factors鈥?and select loc and race.
c) From Graph&Fit select Fit binomial response. Select STA as the re- sponse and ones as the number of trials. The full model will use every predic- tor except ID, LOC, and RACE (the latter 2 are replaced by their factors): select AGE, Bic, CAN, CPR, CRE, CRN, FRA, HRA, INF, {F}LOC , PCO, PH, PO2 , PRE , {F}RACE , SER, SEX, SYS, and TYP as predictors.
13.10 Problems 451 Perform the logistic regression and include the relevant output for testing in
Word.
d) Make the response plot for the full model: from Graph&Fit select Plot of. Place STA on V and B1:Eta鈥橴 on H. From the OLS popup menu, select Logistic and move the slider bar to 1. From the lowess popup menu select SliceSmooth and move the slider bar until the fit is good. Use lowess if Slice- Smooth does not work. Include your plot in Word. Is the full model good?
e) Using what you have learned in class, find a good submodel and include the relevant output in Word.
[Hints: Use forward selection and backward elimination and find the model with the minimum AIC. Let 螖(I) = AIC(I) 鈭?AIC(Imin). Then find the model II with the fewest number of predictors such that 螖(II) 鈮?2. Then submodel II is the initial submodel to examine. Fit model II and look at the Wald test p鈥搗alues. Try to eliminate predictors with large p鈥搗alues but make sure that the deviance does not increase too much. Also examine submodels I with fewer predictors than II with 螖(I) 鈮?7. WARNING: do not delete part of a factor. Either keep all 2 factor dummy variables or delete all I-1=2 factor dummy variables. You may have several models, say B2, B3, B4, and B5 to look at. Make the EE and response plots for each model. WARNING: if a useful factor is in the full model but not the reduced model, then the EE plot may have I = 3 lines if the factor should be in the model. See part h) below.]
f) Make a response plot for your final submodel.
g) Suppose that B1 contains your full model and B5 contains your final submodel. Make an EE plot for your final submodel: from Graph&Fit select Plot of. Select B1:Eta鈥橴 for the V box and B5:Eta鈥橴, for the H box. After the plot appears, click on the options popup menu. A window will appear. Type y = x and click on OK. This action adds the identity line to the plot. Include the plot in Word.
If the EE plot is good, then the plotted points will cluster about the identity line. For model II, some points are far away from the identity line. At least one variable needs to be added to model II to get a good submodel and EE plot, violating the rule of thumb that submodels with more predictors than II should not be examined. Variable selection may be suggesting poor submodels because of clusters of cases that are given exact probabilities of 0 or 1. Try adding {F}RACE to the predictors in II .
h) Using e), f), g), and any additional output that you desire [e.g. AIC(full), AIC(Imin), AIC(II), and AIC(final submodel)], explain why your final submodel is good.","Diagnostic,Computational Statistics"
lr_gu_LR_p1.1,"1.1 Which of the following models are linear in the parameters, or variables, or both. Which of these models are LRMs?
a.
Yi = B1 + B2 (1/Xi) +ui (Reciprocal)
b.
Yi = B1 + B2 ln Xi +ui (Semilogarithmic)
c.
ln Yi = B1 + B2 Xi +ui (Inverse semilogarithmic)
d.
ln Yi = B1 + B2 ln Xi +ui (Double logarithmic)
e. ln Yi = B1 + B2 (1/Xi) +ui (Logarithmic reciprocal)
Note: = natural log and ui is the regression error term.",Simple Linear Regression
lr_gu_LR_p1.2,"1.2 Are the following models LRMs? Why or why not?
a.
Yi=eB1+B2Xi+ui
b.
Yi=11+eB1+B2Xi+ui
c.
lnYi=B1+B2(1/Xi)+ui
d.
Yi=B1+(0.75−B1)e−B2(Xi−2)+ui
e.
Yi=B1+B23Xi+ui",Simple Linear Regression
lr_gu_LR_p1.3,"1.3 Consider the following regression model that has no explanatory variables.
Yi = B1+ ui
a.
Use OLS to estimate B1.
b.
How would you interpret B1 in this model?","Simple Linear Regression, Estimation"
lr_gu_LR_p1.4,"1.4 Consider the following simple two-variable, or bivariate, regression model.
Yi = B1+ B2 Xi + ui
a.
Using OLS, obtain the estimators of B1 and B2.
b.
How would you interpret the two regression coefficients?","Simple Linear Regression, Estimation"
lr_gu_LR_p1.5,"1.5
Prove: rY^Y2=R2, that is, the squared correlation coefficient between the actual Y and
the estimated Y from a regression model is equal to the coefficient of determination.","Simple Linear Regression, Estimation, Prediction, Remedy"
lr_gu_LR_p1.6,Let Byx be the slope coefficient in the regression of Y on X and BXY the slope coefficient in the regression of X on Y. Show that BYX BXY = r2.,"Simple Linear Regression, Estiamtion, Remedy"
lr_gu_LR_p1.7,"Show that ¯
X=1′x/n. Similarly, show that ¯
Y=1′y/n. Show that nΣ1Xi=1′x, where 1′ = (1, 1, . . . , 1)′ and x is an n-element column vector",Linear Algebra
lr_gu_LR_p1.8,"Prove the following in inequality, known as the Cauchy–Schwarz inequality:
[E(XY)]2 ≤ E(X2)E(Y2)
Use this inequality to show that r 2, the squared correlation coefficient, is such that 0 ≤ r2 ≤ 1.","Linear Algebra, Remedy"
lr_gu_LR_p1.9,"Show that
a.
¯e=0, that is, the average value of the residuals is zero.
b.
¯Y=b1+b2¯X2+b3¯X3+?+bk¯Xk, that is, the regression hyperplane passes through the sample mean values of Y and the Xs.
c.
The mean value of actual Y and estimated Y values are the same.","Multiple Linear Regression, Estimation, Prediction"
lr_gu_LR_p1.10,"Consider the following regression model: Yik=B1+B2Xi+uiY>0,k=aconstant
Consider the following values for k
k = 1, 2, 0.5, −0.5, −1
a.
For each of these k values, find the corresponding regression model. Which of these models are LRMs?
b.
Suppose k = 0. Can you estimate the regression model in this case?10","Generalized Linear Regression, Estimation"
lr_gu_LR_p1.11,"You are given 10 values for variables Y (the dependent variable) and X (the explanatory variable):
Y 70 65 90 95 110 115 120 140 155 150
X 80 100 120 140 160 180 200 220 240 260
Based on these values, estimate the following regression:
Yi=B1+B2Xi+uii=1,2,…,10
a.
Find (X′X),(X′Y)
b.
Estimate b=(X′X)-1 X′Y.
c.
Estimate R 2 for this example.","Simple Linear Regression, Estimation"
lr_gu_LR_p2.1,"Consider the bivariate regression: Yi = B1 + B2Xi +ui. Under the classical linear regression assumptions, show that a. cov(b1, b2) = − ¯ X σ 2 Σ(Xi − ¯ X) 2 b. cov( ¯ Y, b2) = 0","Simple Linear Regression, Estimation"
lr_gu_LR_p2.2,"Show that for the model in Exercise 2.1,
RSS =
Σxi
2
Σyi
2
− (Σxiyi
)
2
Σxi
2
where RSS is the residual sum of squares and
xi = (Xi −
¯
X); yi = (Yi −
¯
Y); xiyi = (Xi −
¯
X)(Yi −
¯
Y)","Simple Linear Regression, Estimation"
lr_gu_LR_p2.3,"Verify the following properties of OLS estimators:
a. The OLS regression line (plane) passes through the sample means of the
regressand and the regressors.
b. The mean values of the actual Y and the estimated Y( = ŷ) are the same.
c. In the CLRM with intercept, the mean value of the residuals (ē) is zero.
d. As a result of the preceding property, the k-variable sample CLRM can be
expressed as yi = b2x2i + b3x3i + ? + bkxk
i
+ ei
where
yi = (Yi −
¯
Y); xki = (Xki −
¯
Xk)","Simple Linear Regression Estimation, Prediction"
lr_gu_LR_p2.4,"Consider the following bivariate regression model:
Yi
*
= B1
*
+ B2
*Xi
*
+ ui
where
Yi
*
=
Yi −
¯
Y
sY
; Xi
*
=
Xi −
¯
X
sX
where sY and sX are the sample standard deviations of Y and X. Yi
*
and Xi
*
are known
as standardized variables, often known as Z scores. Since the units of measurement of
the Z scores in the numerator and the denominator are the same, they are called “pure”
or “unitless” numbers.
Show that a standardized variable has a zero mean and unit variance.
What are the formulas to estimate B1
*
and B2
*
?
What is the relationship between B1
*
and B1 and between B2
*
and B2? ","Simple Linear Regression, Estimation"
lr_gu_LR_p2.5,"The sample correlation coefficient between variables Y and X, rXY, is defined as
rXY =
Σxiyi
√Σxi
2
Σyi
2
where
xi = (Xi −
¯
X); yi = (Yi −
¯
Y)
If we standardize variables as in Exercise 2.4, does it affect the correlation coefficient
between X and Y? Show the necessary calculations. ","Simple Linear Regression, Estimation, Remedy"
lr_gu_LR_p2.6,"Consider variables X1, X2, and X3. Now consider the following correlation coefficients:
r12 = correlation coefficient betweenX1andX2
r13 = correlation coefficient betweenX1andX3
r23 = correlation coefficient betweenX2andX3
r12.3 =
r12 − r13r23
√1 − r
13
2
√1 − r
23
2

r12.3 is called the partial correlation coefficient between X1 and X2 holding the
influence of the variable X3. The concept of partial correlation is akin to the concept of a
partial regression coefficient.
a. What other partial correlation coefficients can you compute?
b. If we standardize the three variables as in Exercise 2.4, would the
correlation coefficients among the standardized variables be different from
the unstandardized variables?
c. Would partial correlation coefficients be affected by standardizing the
variables? Explain. ","Simple Linear Regression, Estimaion"
lr_gu_LR_p2.7,"Consider the following LRM:
Yi = B1 + B2X2
i
+ B3X3i + B4X4i + B5X5i + ui
How would you test the following hypotheses?
a. B2 = B3 = B4 = B5 = B, that is, all partial regression coefficients
are the same.
b. B2
= B3
and B4
= B5
c. B2 + B3 = 2B4","Multiple Linear Regression, Estimation, Remedy"
lr_gu_LR_p2.8,"Remember that the hat matrix, H, is expressed as
H = X(X ′ X)
− 1X
Show that the residual vector e can also be expressed as
e=(I−H)y ","Multiple Linear Regression, Estimation"
lr_gu_LR_p2.9,Prove that the matrices H and (I − H) are idempotent. ,Linear Algebra
lr_gu_LR_p2.10,"For the following matrix, compute its eigenvalues:
[
1
0
0
0
1
0
0
0
1 ]",Linear Algebra
lr_gu_LR_p2.11,"Consider the following regression model (see Chapter 7, Equation (7.30)):
Yi = B1 + B2Xi + B3Xi
2
+ ui
Models like this are called polynomial regression models, here a second-degree
polynomial.
a. Is this an LRM?
b. Can OLS be used to estimate the parameters of this model?
c. Since Xi
2
is the square of Xi
, does this model suffer from perfect collinearity? ",Generalized Linear Regression
lr_gu_LR_p2.12,"Consider the following model:
Yi = B1 + B2X2
i
+ B3X3i + B4X4i + ui
You are told that B2 = 1.
a. In this case, is it legitimate to estimate the following regression?
Yi = B1 + B2X2i + B3X3i + B4X4i + ui
This model is called a restricted linear regression, whereas the preceding
model is called an unrestricted linear regression (see Chapter 4,
Appendix 4A for further details).
b. How would you estimate the restricted regression, taking into account the
restriction that B2 = 1? ","Multiple Linear Regression, Estimation"
lr_gu_LR_p3.1,"Consider the binomial variable x discussed in the chapter.
a. If a random sample of n observations is drawn from this PDF, what is the
ML estimator of p and the variance of its sampling distribution?
b. What is the asymptotic variance of the ML estimator found in (b)?
c. *Does ^
p the estimator of p, attain the CRLB for the variance of an
unbiased estimator? ",Estimation
lr_gu_LR_p3.2,"A random variable x is said to follow the Poisson distribution if it has the following density
function:
f(x; λ) =
λ
x
e
− λ
x!
, λ > 0, x = 0, 1, 2, …
where λ is the parameter of the distribution. A sample of size n is drawn randomly from
this distribution.
a. Obtain the likelihood function for the sample.
b. Estimate the parameter λ. How would you interpret it?
c. What is the variance of this distribution?
d. Does it attain the CRLB? ",Probability Distribution
lr_gu_LR_p3.3,"X1,X2,…,Xn is a random sample from N(λ,1).
a. What is the ML estimator of λ
b. Let γ = e
λ
. What is the ML estimator of γ? (Hint: Invariance property of
ML estimator.) ",Estimation
lr_gu_LR_p4.1,"In the classical bivariate LRM,
Yi = B1 + B2Xi + ui
Show that
a.
cov(b1, b2) = −
¯
X(
σ
2
Σx
i
2)
and
b. cov(
¯
Y, b1) = 0
where
xi = (Xi −
¯
X).","Estimation, Linear Algebra"
lr_gu_LR_p4.2,"In the bivariate regression model of Exercise 4.1, show that
E(MESS) = σ
2
+ b2Σx1
2
, and
E(MRSS) = σ
2
where MESS = mean or average explained sum of squares and MRSS =
mean or average residual sum of squares. (Hint: Refer to Table 4.2.) ","Probability Distribution, Estimation"
lr_gu_LR_p4.3,"Consider the data in Tables 1 and 2 below:
Table 1
Y X2 X3
1 2 4
2 0 2
3 4 12
4 6 0
5 8 16
Table 2
Y X2 X3
1 2 4
2 0 2
3 4 0
4 6 12
5 8 16
The only difference between the two tables is that the third and fourth values of X3 are
interchanged.
a. Estimate the two regressions.
b. What conclusions do you draw from this exercise?
c. What may be the reason for the difference between the two regressions? ","Multiple Linear Regression, Estimation"
lr_gu_LR_p4.4,"Suppose in the model
Yi = B1 + B2X2i + B3X3i + ui
r23 the coefficient of correlation between X2 and X3 is zero. Therefore, someone
suggests that you estimate the following regressions:
Yi = C1 + C2X2i + u1i
Yi = D1 + D3X3i + u2i
a. Willc2 = b2 and d3 = b3? Why?
b. Will b1 equal to c1 or d1, or some combination thereof?
c. Will var(b2) = var(c2) and var(b3) = var(d3)? Explain. ","Multiple Linear Regression, Simple Linear Regression, Hypothesis Testing"
lr_gu_LR_p4.5,"Suppose that X1,X2,…,Xn is a random sample from N(θ,1) The ML estimator of γ is
γ =
¯
X.
What is the ML estimator of T = e
γ
?(Hint: Consistency property of ML estimators.) ","Estimation, Probability Distirbution"
lr_gu_LR_p4.6,"Suppose that X1,X2,…,Xn is a random sample from N(γ,σ2
). The ML estimator of θ is
^
θ =
¯
X.
Let
˜
θ,
the sample median, be an alternative estimator of θ. It can be shown that the ML
estimator satisfies10
√n(θn − θ) ~ N(0, σ
2
) (i.e., asymptotic normality)
It can also be shown that the median satisfies
√n(
˜
θ − θ) ~ N(
0, σ
2π
2 )
That is, it is also asymptotically normally distributed with the stated parameters.
Since both the estimators converge to the right value, what is the difference between the two? Which is
relatively more efficient? What is the practical importance of this finding? ","Probaility Distribution, Estimation"
lr_gu_LR_p5.1,"Consider the following model:15
Yi = BXi + ui
, i = 1, 2
where u1 ~ N(0, σ
2
) and u2 ~ N(0, 2σ
2
), and they are statistically independent. If X1 =
+1and X2 = –1, obtain
a. the weighted least-squares estimate of B.
b. the variance of the estimate. ","Simple Linear Regression, Estimation"
lr_gu_LR_p5.2,"Let Z = c+Dy, where y is a random vector, D is a fixed matrix, and c is a fixed vector.
Show that
a. E(Z) = c+D E(y)
and
b. cov(Z) = DΣyyD
′
where
Σyy
is the covariance of y. ","Probability Distribution, Linear Algebra"
lr_gu_LR_p5.3,"Consider the following model without the intercept:
Yi = BXi + ui
You are told that var(ui
) = σ
2Xi
2
. Show that   var(B) =
σ
2
ΣXi
4
(ΣXi
2
)
2",Probability Distribution
lr_gu_LR_p5.4,"Which of the following statements are true or false?
a. In regression analysis, the assumption that the error term u is normally
distributed is essential to validate the use of F and t tests.
b. H0: B1 = B2B3 is not a linear hypothesis.
c. The formula
E(b) = B + (X ′ X)
−
1X ′ X2B2
gives the bias effect on b of failing to include the terms X2B2 in the model. ","Probability Distribution, Hypothesis Testing"
lr_gu_LR_p5.5,"Suppose in the hypothetical savings–income regression discussed in the text, the error ui
has the following error variance structure:
E(ui
2
) = σ
2Xi
That is, the error variance is proportional to the level of income X. How would you transform the
savings–income regression so that in the transformed model the error variance is homoscedastic? ","Hypothesis Testing, Probability Distribution"
lr_gu_LR_p7.1,"Using the lin-log model of (7.26), we obtained the following regression results:
Yi = 0.9304 − 0.0777lnXi
t = (25.5836)( − 21.6482) R
2
= 0.3508
where Y = share of food expenditure in total expenditure and X = total expenditure. The
results are based on the data for 869 U.S. households in 1995.
a. How would you interpret this regression?
b. What is the interpretation of the slope coefficient of about −0.08?
c. How would you compute the elasticity of the share of food expenditure in
relation to the total expenditure?
(Hint : Elasticity =
dY
dX
X
Y
)","Generalized Linear Regression, Estimation"
lr_gu_LR_p7.2,"Using the same data as in Exercise 7.1, we estimated the reciprocal model as in Equation
(7.29) and obtained the following results:
Yi = 0.0772+1331.3380(
1
Xi)
t = (19.2595)(20.8161) R
2
= 0.3332
a. How would you interpret this regression?
b. What is the interpretation of the slope coefficient?
c. Is the rate of change in the food expenditure in relation to total expenditure
positive or negative throughout?
d. What would be the share of food expenditure in the total expenditure if the
total expenditure were to increase indefinitely? (Hint: the intercept) ","Generalized Linear Regression, Estimation, Hypothesis Testing"
lr_gu_LR_p7.3,"Suppose instead of estimating the quadratic trend model (7.31), we estimate the model
without the quadratic term. The results are as follows:
RGDPt = 1664.2180 + 186.9939timet
t = (12.6078) + (39.8717) R
2
= 0.9718
This model is known as the linear trend model.
a. How would you interpret this regression?
b. Between the quadratic trend model of Equation (7.32) and the linear trend
model, which would you choose? And why?
c. If the quadratic trend model is the “true” model, what type of specification
error is involved if you use the linear trend model? ",Generalized Linear Regression
lr_gu_LR_p7.4,"In the wage regression results given in Table 4.1, the dependent variable was w, hourly
wage rate in dollars. Suppose now we regress the log of the wage rate on the regressors
given in Table 4.1. The results of this regression are given in the following table (Note:
LW = natural log of W). In this regression, as in the results given in Table 4.1, FE (female),
NW (nonwhite), and UN (union) are qualitative or dummy variables; ED (education in
years) and EX (experience in years) are quantitative variables. The results of “log wage
regression” are as follows:
Dependent Variable: LW
Method: Least Squares
Date: 02/25/17 Time: 17:55
Sample: 1 1289
Included observations: 1289
Variable Coefficient Std. Error t-Statistic Prob.
C 0.905504 0.074175 12.20768 0.0000
FE −0.249154 0.026625 −9.357891 0.0000
NW −0.133535 0.037182 −3.591399 0.0003
UN 0.180204 0.036955 4.876316 0.0000
ED 0.099870 0.004812 20.75244 0.0000
EX 0.012760 0.001172 10.88907 0.0000
R-squared 0.345650 Mean dependent var 2.342416
Adjusted R-squared 0.343100 S.D. dependent var 0.586356
S.E. of regression 0.475237 Akaike info criterion 1.354639
Sum squared resid 289.7663 Schwarz criterion 1.378666
Log likelihood −867.0651 Hannan-Quinn criter. 1.363658
F-statistic 135.5452 Durbin-Watson stat 1.942506
Prob(F-statistic) 0.000000
a. How would you interpret these results?
b. Are the signs of the various coefficients in accord with prior expectations?
c. What is the semielasticity of the wage rate with respect to education? And
with respect to years of experience?
d. Is it possible to compute the wage elasticity with respect to the dummy
variables?28
e. Is the R
2
value given in the preceding table comparable with the R
2
value
given in Table 4.1? Why or why not? ","Multiple Linear Regression, Estiamtion, Remedy"
