{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "from gensim.utils import simple_preprocess\n",
    "from smart_open import smart_open\n",
    "import os\n",
    "\n",
    "import warnings;\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)], [(4, 4)]]\n"
     ]
    }
   ],
   "source": [
    "# List with 2 sentences\n",
    "my_docs = [\"Who let the dogs out?\",\n",
    "           \"Who? Who? Who? Who?\"]\n",
    "\n",
    "# Tokenize the docs\n",
    "tokenized_list = [simple_preprocess(doc) for doc in my_docs]\n",
    "\n",
    "# Create the Corpus\n",
    "mydict = corpora.Dictionary()\n",
    "mycorpus = [mydict.doc2bow(doc, allow_update=True) for doc in tokenized_list]\n",
    "pprint(mycorpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load textbook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "\n",
    "def convert_pdf_to_txt(path):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, laparams=laparams)\n",
    "    fp = open(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "\n",
    "    for PageNumer,page in enumerate(PDFPage.get_pages(fp, pagenos , password=password,caching=caching, check_extractable=True)):\n",
    "        interpreter.process_page(page)\n",
    "\n",
    "    text = retstr.getvalue()\n",
    "\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "DATA_DIR = \"C:/Users/jry5/OneDrive/Desktop/1/\"\n",
    "\n",
    "# Grab the files\n",
    "books = [pdf for pdf in listdir(DATA_DIR) if isfile(join(DATA_DIR, pdf))]\n",
    "text = []\n",
    "\n",
    "for book in books:\n",
    "    path = DATA_DIR + book\n",
    "    content = convert_pdf_to_txt(path)\n",
    "    text.append(content)\n",
    "    print (\"\\n----- Contents of %s -----\\n\" % book)\n",
    "    print (content[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = ['\\n\\n'.join(text)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find each paragraph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = re.compile(r'(?s)((?:[^\\n][\\n]?)+)', re.M)\n",
    "text1 = pat.findall(text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# stop_words = stopwords.words('english')\n",
    "\n",
    "def clean_text(\n",
    "    string: str, \n",
    "    punctuations=r'''!()-[]{};:'\"\\,<>./?@#$%^&*_~''',\n",
    "    stop_words=['the', 'a', 'and', 'is', 'be', 'will']) -> str:\n",
    "    \"\"\"\n",
    "    A method to clean text \n",
    "    \"\"\"\n",
    "    # Cleaning the urls\n",
    "    string = re.sub(r'https?://\\S+|www\\.\\S+', '', string)\n",
    "\n",
    "    # Cleaning the html elements\n",
    "    string = re.sub(r'<.*?>', '', string)\n",
    "\n",
    "    # Removing the punctuations\n",
    "    for x in string.lower(): \n",
    "        if x in punctuations: \n",
    "            string = string.replace(x, \"\") \n",
    "            \n",
    "    # remove backslash-apostrophe \n",
    "    string = re.sub(\"\\'\", \"\", string) \n",
    "    \n",
    "    # remove everything except alphabets \n",
    "    string = re.sub(\"[^a-zA-Z]\",\" \",string) \n",
    "    \n",
    "    # Converting the text to lower\n",
    "    string = string.lower()\n",
    "\n",
    "    # Removing stop words\n",
    "    string = ' '.join([word for word in string.split() if word not in stop_words])\n",
    "\n",
    "    # Cleaning the whitespaces\n",
    "    string = re.sub(r'\\s+', ' ', string).strip()\n",
    "\n",
    "    return string        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "count = 0\n",
    "for i in range(len(text1)):\n",
    "    data.append(clean_text(text1[i]).split())\n",
    "for i in range(len(data)):\n",
    "    count += data[i].count('bias')\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Bag of words:\n",
    "## Build word2vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-819dd80e25c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmultiprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcpu_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from multiprocessing import cpu_count\n",
    "import gensim.downloader as api\n",
    "model = Word2Vec(data, min_count = 10, workers=cpu_count()).model.save('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.51311306e-02, -6.28145397e-01,  1.77192256e-01, -1.04333684e-01,\n",
       "        1.20230116e-01, -3.48853283e-02, -4.54511136e-01,  3.12276721e-01,\n",
       "       -2.04952136e-01, -7.39401281e-01, -4.78009343e-01,  9.82072875e-02,\n",
       "        3.75785202e-01,  2.22154334e-02, -2.92884737e-01, -4.83743995e-01,\n",
       "        9.09047648e-02, -4.87612873e-01, -2.96278805e-01,  1.20562661e+00,\n",
       "        2.69648761e-01,  6.85722649e-01, -3.57671171e-01, -4.89650935e-01,\n",
       "       -4.91406530e-01, -4.78023291e-01, -1.58647209e-01,  1.02819093e-01,\n",
       "        1.03273980e-01,  3.28905553e-01, -3.40806514e-01,  9.04548243e-02,\n",
       "       -1.74415052e-01, -4.57453847e-01, -5.95559878e-03,  2.05607668e-01,\n",
       "       -1.22218452e-01,  1.57815918e-01, -8.13616931e-01,  4.90259640e-02,\n",
       "        1.37814224e+00,  6.46468177e-02,  7.46606350e-01,  4.06920344e-01,\n",
       "       -1.37573314e+00, -4.71521527e-01, -3.76775712e-02, -7.98538566e-01,\n",
       "        7.91397333e-01, -6.65266290e-02,  1.36889115e-01,  3.35829526e-01,\n",
       "        1.95567504e-01, -8.48331153e-01, -4.55819547e-01,  4.32484686e-01,\n",
       "       -8.56586993e-01,  7.51327304e-03,  6.44548774e-01,  4.52349722e-01,\n",
       "        6.41869366e-01,  5.57872236e-01,  7.79797062e-02, -1.21311092e+00,\n",
       "        1.70070808e-02, -1.41383600e+00,  7.44834304e-01,  6.37065470e-02,\n",
       "       -2.58895755e-01, -1.53418913e-01,  2.73570985e-01,  4.96572107e-01,\n",
       "       -4.29775357e-01,  4.15479481e-01,  1.11296833e+00,  2.61335913e-02,\n",
       "       -8.25846732e-01,  5.48906147e-01, -4.49128568e-01,  9.87503052e-01,\n",
       "        3.19727480e-01, -2.56348819e-01, -6.63169503e-01, -9.42310154e-01,\n",
       "        7.54606426e-01,  1.31283432e-01,  2.85292920e-02,  6.89498242e-03,\n",
       "        1.46096498e-01,  6.22072577e-01,  4.42585707e-01,  7.08490668e-04,\n",
       "        3.56720909e-02, -5.66597879e-01,  8.03407133e-02,  6.96590066e-01,\n",
       "       -9.33778808e-02,  6.04987554e-02, -8.50409195e-02, -8.92793536e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['regression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('multiple', 0.9707800149917603),\n",
       " ('linear', 0.9389125108718872),\n",
       " ('multivariate', 0.9378404021263123),\n",
       " ('regression', 0.9290093183517456),\n",
       " ('special', 0.913578450679779),\n",
       " ('additive', 0.894995927810669),\n",
       " ('generalized', 0.8923579454421997),\n",
       " ('doi', 0.8839783668518066),\n",
       " ('alternatives', 0.8582957983016968),\n",
       " ('logistic', 0.850387692451477)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('linear', 0.9877618551254272),\n",
       " ('multiple', 0.9752734899520874),\n",
       " ('multivariate', 0.9679512977600098),\n",
       " ('models', 0.929009199142456),\n",
       " ('doi', 0.8791576623916626),\n",
       " ('poisson', 0.8545827865600586),\n",
       " ('binomial', 0.8364201784133911),\n",
       " ('generalized', 0.8351503014564514),\n",
       " ('logistic', 0.8285643458366394),\n",
       " ('additive', 0.8254982233047485)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sample'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(['logistic', 'sample', 'simple', 'multiple'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9707800149917603\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(float(cosine_similarity(model[['models']], model[['multiple']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('multiple', 0.9707800149917603),\n",
       " ('linear', 0.9389125108718872),\n",
       " ('multivariate', 0.9378404021263123),\n",
       " ('regression', 0.9290093183517456),\n",
       " ('special', 0.913578450679779),\n",
       " ('additive', 0.894995927810669),\n",
       " ('generalized', 0.8923579454421997),\n",
       " ('doi', 0.8839783668518066),\n",
       " ('alternatives', 0.8582957983016968),\n",
       " ('logistic', 0.850387692451477)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wikipedia2vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
