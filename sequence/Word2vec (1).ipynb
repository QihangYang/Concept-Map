{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "from gensim.utils import simple_preprocess\n",
    "from smart_open import smart_open\n",
    "import os\n",
    "\n",
    "import warnings;\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)], [(4, 4)]]\n"
     ]
    }
   ],
   "source": [
    "# List with 2 sentences\n",
    "my_docs = [\"Who let the dogs out?\",\n",
    "           \"Who? Who? Who? Who?\"]\n",
    "\n",
    "# Tokenize the docs\n",
    "tokenized_list = [simple_preprocess(doc) for doc in my_docs]\n",
    "\n",
    "# Create the Corpus\n",
    "mydict = corpora.Dictionary()\n",
    "mycorpus = [mydict.doc2bow(doc, allow_update=True) for doc in tokenized_list]\n",
    "pprint(mycorpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load textbook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "\n",
    "def convert_pdf_to_txt(path):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, laparams=laparams)\n",
    "    fp = open(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "\n",
    "    for PageNumer,page in enumerate(PDFPage.get_pages(fp, pagenos , password=password,caching=caching, check_extractable=True)):\n",
    "        interpreter.process_page(page)\n",
    "\n",
    "    text = retstr.getvalue()\n",
    "\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Contents of 2017_Book_LinearRegression.pdf -----\n",
      "\n",
      "David J. Olive\n",
      "\n",
      "Linear \n",
      "Regression\n",
      "\n",
      "\f",
      "Linear Regression\n",
      "\n",
      "\f",
      "David J. Olive\n",
      "\n",
      "Linear Regression\n",
      "\n",
      "123\n",
      "\n",
      "\f",
      "Da\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "DATA_DIR = \"C:/Users/jry5/OneDrive/Desktop/1/\"\n",
    "\n",
    "# Grab the files\n",
    "books = [pdf for pdf in listdir(DATA_DIR) if isfile(join(DATA_DIR, pdf))]\n",
    "text = []\n",
    "\n",
    "for book in books:\n",
    "    path = DATA_DIR + book\n",
    "    content = convert_pdf_to_txt(path)\n",
    "    text.append(content)\n",
    "    print (\"\\n----- Contents of %s -----\\n\" % book)\n",
    "    print (content[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = ['\\n\\n'.join(text)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find each paragraph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pat = re.compile(r'(?s)((?:[^\\n][\\n]?)+)', re.M)\n",
    "text1 = pat.findall(text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# stop_words = stopwords.words('english')\n",
    "\n",
    "def clean_text(\n",
    "    string: str, \n",
    "    punctuations=r'''!()-[]{};:'\"\\,<>./?@#$%^&*_~''',\n",
    "    stop_words=['the', 'a', 'and', 'is', 'be', 'will']) -> str:\n",
    "    \"\"\"\n",
    "    A method to clean text \n",
    "    \"\"\"\n",
    "    # Cleaning the urls\n",
    "    string = re.sub(r'https?://\\S+|www\\.\\S+', '', string)\n",
    "\n",
    "    # Cleaning the html elements\n",
    "    string = re.sub(r'<.*?>', '', string)\n",
    "\n",
    "    # Removing the punctuations\n",
    "    for x in string.lower(): \n",
    "        if x in punctuations: \n",
    "            string = string.replace(x, \"\") \n",
    "            \n",
    "    # remove backslash-apostrophe \n",
    "    string = re.sub(\"\\'\", \"\", string) \n",
    "    \n",
    "    # remove everything except alphabets \n",
    "    string = re.sub(\"[^a-zA-Z]\",\" \",string) \n",
    "    \n",
    "    # Converting the text to lower\n",
    "    string = string.lower()\n",
    "\n",
    "    # Removing stop words\n",
    "    string = ' '.join([word for word in string.split() if word not in stop_words])\n",
    "\n",
    "    # Cleaning the whitespaces\n",
    "    string = re.sub(r'\\s+', ' ', string).strip()\n",
    "\n",
    "    return string        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "count = 0\n",
    "for i in range(len(text1)):\n",
    "    data.append(clean_text(text1[i]).split())\n",
    "for i in range(len(data)):\n",
    "    count += data[i].count('bias')\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Bag of words:\n",
    "## Build word2vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from multiprocessing import cpu_count\n",
    "import gensim.downloader as api\n",
    "model = Word2Vec(data, min_count = 10, workers=cpu_count())\n",
    "model.save('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.47447598, -0.50714296,  0.5753656 , -0.03402466,  0.44752416,\n",
       "        0.72413677, -0.33330336,  0.51516443,  1.0497626 ,  0.8192747 ,\n",
       "        0.09662783,  0.39593318, -0.47896796,  0.33325276,  0.94673544,\n",
       "        0.19670576, -0.9646793 , -0.06000009,  0.19711249,  0.48845363,\n",
       "        0.00256744, -0.37924588, -0.10640869,  0.3473871 ,  0.9649055 ,\n",
       "       -0.6942536 ,  0.24901143, -0.76838386, -0.28010854, -0.30402097,\n",
       "       -0.79135585,  0.8440176 ,  0.04658692,  0.28777543, -0.7958924 ,\n",
       "       -0.12693746, -0.00620345, -0.0391525 ,  0.07302649,  0.5535461 ,\n",
       "        0.05857614, -1.0168226 ,  0.13907485,  0.27156174,  0.3206015 ,\n",
       "       -0.8867621 , -0.24572295, -0.7150718 ,  0.19871452, -0.10811483,\n",
       "       -0.01955325, -0.3552129 , -0.7601704 ,  0.6217304 , -0.7315517 ,\n",
       "       -0.6078159 ,  0.70880824, -0.38100582, -0.1633411 ,  0.58503085,\n",
       "       -0.82559514, -0.11237887, -0.24963054, -0.01607132, -0.00995457,\n",
       "        0.38562232,  0.83340436, -0.5828035 , -0.23919709, -1.263189  ,\n",
       "       -0.5526318 ,  0.45356473,  0.37955   , -0.2610286 , -0.0018894 ,\n",
       "       -0.23305929,  0.0853108 ,  0.28396922,  0.17216922,  0.5294308 ,\n",
       "        0.99219406,  0.48391366, -0.21502091,  0.00784887, -0.08692431,\n",
       "       -0.44639784, -0.2378061 , -0.44817528, -0.0275062 ,  0.6954853 ,\n",
       "       -0.532645  ,  0.44290522, -0.28712365,  0.35913956, -0.21084008,\n",
       "       -0.06351797,  0.38529253, -0.11935964, -0.20429431,  0.7268821 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['regression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('multiple', 0.9537561535835266),\n",
       " ('multivariate', 0.9035055637359619),\n",
       " ('regression', 0.9006812572479248),\n",
       " ('special', 0.897425651550293),\n",
       " ('linear', 0.8899581432342529),\n",
       " ('generalized', 0.8808688521385193),\n",
       " ('analysis', 0.875869870185852),\n",
       " ('additive', 0.8687137365341187),\n",
       " ('alternatives', 0.8665547370910645),\n",
       " ('for', 0.8631240129470825)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('linear', 0.9908047318458557),\n",
       " ('multiple', 0.9712517261505127),\n",
       " ('multivariate', 0.9667948484420776),\n",
       " ('models', 0.9006812572479248),\n",
       " ('poisson', 0.8598709106445312),\n",
       " ('doi', 0.8507057428359985),\n",
       " ('generalized', 0.8192943334579468),\n",
       " ('logistic', 0.8156444430351257),\n",
       " ('binary', 0.8149569034576416),\n",
       " ('simple', 0.8042111396789551)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sample'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(['logistic', 'sample', 'simple', 'multiple'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9537561535835266\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(float(cosine_similarity(model[['models']], model[['multiple']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('multiple', 0.9537561535835266),\n",
       " ('multivariate', 0.9035055637359619),\n",
       " ('regression', 0.9006812572479248),\n",
       " ('special', 0.897425651550293),\n",
       " ('linear', 0.8899581432342529),\n",
       " ('generalized', 0.8808688521385193),\n",
       " ('analysis', 0.875869870185852),\n",
       " ('additive', 0.8687137365341187),\n",
       " ('alternatives', 0.8665547370910645),\n",
       " ('for', 0.8631240129470825)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
