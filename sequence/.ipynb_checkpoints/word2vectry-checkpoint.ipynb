{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from gensim.models import Word2Vec \n",
    "import warnings;\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    lr_final_2016_ p1,Consider a joint distributio...\n",
      "1    lr_final_2016_ p2,If we take the residuals fro...\n",
      "2    lr_final_2016_ p3,If we take the residuals fro...\n",
      "3    lr_final_2016_ p4,The coefficient of multiple ...\n",
      "4    lr_final_2016_ p5,Let maxj r2j and minj r2j de...\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "problem_df=pd.read_excel(\"problem.xlsx\")\n",
    "problem_df.head()\n",
    "problem_df.dropna(inplace=True)\n",
    "problem_df.shape\n",
    "problem_df_2=problem_df.apply(lambda x:','.join(x.astype(str)),axis=1)\n",
    "print(problem_df_2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID=problem_df[\"Problem Set ID\"].unique().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118,)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split into train and test data set\n",
    "random.shuffle(ID)\n",
    "train=[ID[i] for i in range(round(0.9*131))]\n",
    "\n",
    "train_df=problem_df[problem_df[\"Problem Set ID\"].isin(train)]\n",
    "test_df=problem_df[~problem_df[\"Problem Set ID\"].isin(train)]\n",
    "train_df=train_df.apply(lambda x:','.join(x.astype(str)),axis=1)\n",
    "train_df.head()\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['lr_final_2016_ p1', 'Consider a joint distribution in which E[log2 Y |X = x] = β0 +β1 log2 x. Under this model', ' doubling the value of x produces a 4β1 multiplicative effect on the expected value of Y given X = x. True False', 'probability distribution'], ['lr_final_2016_ p3', 'If we take the residuals from the regression of y on x1', ' and plot them versus the residuals from regressing x2 on x1', ' the intercept for the least squares regression line on this plot will be exactly equal to zero. True False', 'Multiple Linear Regression', ' residual plots'], ['lr_final_2016_ p4', 'The coefficient of multiple determination', ' denoted R2', ' gives the proportionate reduction to the total variation in the response variable Y that is achieved by accounting for the linear association between Y and the predictor variables in a regression. True False\\n', 'Multiple Linear Regression', ' r square'], ['lr_final_2016_ p5', 'Let maxj r2j and minj r2j denote the maximum and minimum values of the squared correlation coefficients between the response variable Y and the predictor variables X1', ' . . . ', ' Xp−1; then the coefficient of multiple determination R2 satisfies minj r2 j ≤ R2 ≤ maxj r2. True False', 'Multiple Linear Regression', ' r square'], ['lr_final_2016_ p6', 'In a multiple linear regression with p − 1 predictor variables and an intercept term', ' and assuming a constant variance', ' an unbiased estimator of that variance is the mean squared error', ' MSE = 1 n−pPni=1(yi − yˆi)2. True False', 'Multiple Linear Regression', ' estimator'], ['lr_final_2016_ p7', 'Changing the order in which terms are entered into a multiple linear regression model will change the decomposition of the regression sum of squares', ' but not their sum. With two predictors', ' for example', ' although in general SSR(X1) 6= SSR(X1|X2)', ' and SSR(X2|X1) 6= SSR(X2)', ' it will still be the case that SSR(X1) + SSR(X2|X1) = SSR(X2) + SSR(X1|X2) = SSR(X1', ' X2) . True False', 'Multiple Linear Regression', ' residual'], ['lr_final_2016_ p8', 'Suppose we regress Y on X1', ' save the residuals as e(Y |X1); and likewise regress X2 on X1', ' save those residuals as e(X2|X1). Then the squared correlation between these two sets of residuals will equal R2Y 2|1 =SSR(X2|X1)SSE(X1)', ' the coefficient of partial determination for X2', ' accounting for X1. True False', 'Multiple Linear Regression', ' residual'], ['lr_final_2016_ p9', 'The coefficient of partial determination R2Y 2|1 will not in general be equal to the corresponding simple coefficient of determination R2Y 2', ' but the decomposition of the coefficient of multiple determination holds that R2 Y 1 + R2Y 2|1 = R2Y 2 + R2Y 1|2. True False', 'Multiple Linear Regression', ' r square'], ['lr_final_2016_ p10', 'Adding a quadratic term to a simple linear regression model may be appropriate when the optimal response value is attained at an intermediate value of the predictor', ' that is', ' when the dose-response relationship is not monotone. The additional term in the mean function may have the effect of reducing R2 ', ' but that sacrifice is justified by the improved realism of the model. True False', 'Simple Linear Regression', ' r square'], ['lr_final_2016_ p11', 'Consider a regression problem with two quantitative predictors. The mean function for the general second-order model (quadratic in both terms', ' plus interaction modeled as the product x1x2) will have five parameters. True False\\n', 'higher order regression models', ' parameter'], ['lr_final_2016_ p12', 'Consider a regression problem with two predictor variables', ' both categorical: one has four levels', ' and the other has five levels. The mean function for the main effects model (no interaction) will have eight parameters. True False', 'higher order regression models', ' parameter'], ['lr_final_2016_ p13', 'Consider a regression problem with two predictor variables', ' both categorical: one has four levels', ' and the other has five levels. The mean function for the second order model (main effects and interaction) will have 21 parameters. True False', 'higher order regression models', ' parameter'], ['lr_final_2016_ p14', 'Consider a regression problem with two predictor variables', ' one is categorical with four levels', ' and the other is continuous. The mean function for the parallel linear regressions (first order) model will have five parameters.True  False', 'higher order regression models', ' parameter'], ['lr_final_2016_ p15', 'Consider a regression problem with two predictor variables', ' one is categorical with four levels', ' and the other is continuous. The mean function for the separate linear regressions (first order plus interaction) model will have 10 parameters. True False', 'higher order regression models', ' parameter'], ['lr_final_2016_ p16', 'Consider a linear regression on two predictor variables x1 and x2. Other things being equal', ' the greater the difference in their average values', ' i.e.', ' the greater is |x? - x?|', ' the greater is the variance of the least squares estimates of both their coefficients. True False', 'Multiple Linear Regression', ' least square estimation'], ['lr_final_2016_ p18', 'Consider a linear regression on two predictor variables. If the predictor variables are perfectly correlated (collinear)', \" then the least squares estimation of their coefficients won't even have a unique solution. True False\", 'Multiple Linear Regression', ' least square estimation', ' collinearity'], ['lr_final_2016_ p19', 'Consider a linear regression on p - 1 predictors', ' plus intercept. The variance inflation factor for the jth predictor xj is a function of the coefficient of multiple determination from regressing xj on x1', ' . . . ', ' xj-1; thus the variance inflation factors for a set of predictors will depends on what order the terms are listed in the model specification. True False', 'Multiple Linear Regression', ' Prediction'], ['lr_final_2016_ p20', 'If the Schwartz Bayesian Criterion', ' or BIC', ' selects a different set of predictors than does AIC', ' it will generally be the case that the model favored by BIC is simpler (contains fewer terms); this follows since AIC contains a more severe penalty for model complexity. True False', 'variable selection'], ['lr_final_2016_ p21', 'If data are available for P - 1 predictor variables', ' there are 2P -1 potential first-order models containing an intercept term. For five predictors (25 = 32) or even ten (210 = 1024) it may not be unreasonable for a computer routine to search them all. Much more than that', ' however', ' and some sort of search strategy (e.g.', ' stepwise regression) will be necessary (or at least desirable). True False', 'higher order regression models', ' parameter'], ['lr_final_2016_ p22', 'Depending on the number of terms in the optimal subset of predictor variables', ' one of backward selection and forward elimination will be more efficient than the other. But either algorithm will identify and select this \"best model\" eventually. True False', 'Multiple Linear Regression', ' variable selection'], ['lr_final_2016_ p23', 'A simplistic but useful approach to testing for lack of fit in multiple linear regression is to consider the model E[Y |X = x] = 0 + 1x1 + ???+ p-1xp-1 + jjx2j and test H0 : jj = 0 versus HA : jj = 0. Do this for each j = 1', ' . . . ', ' p - 1. True False', 'Multiple Linear Regression', ' Hypothesis Testing'], ['lr_final_2016_ p24', 'If we conduct the tests for curvature described above', \" as well as Tukey's test for nonadditivity\", ' and fail to reject H0 in every single instance', ' then we can safely conclude that the first order (linear) mean function is correct. True False', 'Multiple Linear Regression', ' Hypothesis Testing'], ['lr_final_2016_ p25', 'In the Breusch-Pagan test for nonconstant variance', ' the alternative hypothesis holds that the variance function depends on a linear combination of the predictor variables', ' specifically that Var[Y |X = x] = exp {0 + 1x1 + ???+ p-1xp-1} . The null hypothesis holds that 1 = ???= p-1 = 0.True False', 'Multiple Linear Regression', ' Hypothesis Testing'], ['lr_final_2016_ p26', 'Suppose we reject H0 in a Breusch-Pagan test', ' and conclude that the assumption of constant variance does not hold for our data. Some possible courses of action (not an exhaustive list) would be (1) investigate the need to transform the response variable; (2) consider weighted least squares; and (3) do nothing: ordinary least squares estimation of the mean function parameters will still be unbiased (if no longer optimal)', ' and inferences will still be approximately valid. True False', 'Multiple Linear Regression', ' Hypothesis Testing'], ['lr_final_2016_ p27', 'In a regression problem with response variable y', ' and x a vector of predictors', ' an outlier is characterized by an unusual y-value', \" given its x-value; it is quite possible that yj sits at or near the center of the marginal distribution of the yi's\", ' but case j could still be an outlier (depending on the value of xj). True False', 'Multiple Linear Regression', ' outlier'], ['lr_final_2016_ p28', 'In a regression problem with response variable y', ' and x a vector of predictors', ' a high-leverage case is characterized by an unusual x-value. In fact', ' the formula for computing the \"hat value\" hjj (the usual measure of case j\\'s leverage) does not depend on yj at all. True False', 'Multiple Linear Regression', ' high-leverage'], ['lr_final_2016_ p29', 'One approach to assessing the \"outlierness\" of case j in a linear regression problem involves fitting the model Yi = xi + j I{i=j} + i for i = 1', ' . . . ', ' n. Since the estimation of  is unchanged by the additional term in the above model', ' the estimate of j is just the case j residual', ' ej = yj - xjb. True False', 'Multiple Linear Regression', ' outlier'], ['lr_final_2016_ p30', 'When we conduct an outlier t-test for only the most \"outlier-ish\" case in a data set', ' we have implicitly conducted n such tests', ' and selectively reported only the most striking result. A valid (in fact conservative) adjustment for this multiple testing issue is the so-called Bonferroni correction', ' in which we simply multiply the p-value by n (capping at 1 of course). True False', 'Hypothesis Testing', ' outlier'], ['lr_final_2016_ p31', 'It is mathematically possible (though rarely encountered in practice) for a case whose residual and hat-values are both close to zero to still be an influential case', \" in the sense of having the highest (or near highest) Cook's distance in a data set. True False\", 'residual'], ['lr_final_2016_ p32', 'Under the statistical model Yi  indep Normal xi', ' 2 for i = 1', ' . . . ', ' n', ' the maximum likelihood estimator of  is the value which minimizesnQ () =yi - xi 2 ;i=1 that is', ' under normality', ' maximum likelihood and least squares are equivalent criteria. True False', 'Multiple Linear Regression', ' maximum likelihood estimation', ' least square estimation'], ['lr_final_2016_ p33', 'Under the statistical model Yi  indep Normal (xi', ' k/wi) for i = 1', ' . . . ', ' n', ' where the wi are known and not all equal', ' the maximum likelihood estimator of  is the value which minimizesnQ () =yi - xi 2 ;i=1 that is', ' under normality', ' even with unequal variance', ' maximum likelihood and ordinary least squares are equivalent criteria. True False', 'Multiple Linear Regression', ' maximum likelihood estimation', ' least square estimation'], ['lr_final_2016_ p34', 'One of the strengths of the weighted least squares approach to regression estimation is that it requires no assumption about the relative variance for different observations of the response variable. True False', 'weighted least squares'], ['lr_final_2016_ p37', 'Researchers wishing to study teenage gambling in Great Britain took a survey of n = 47 British teenagers', \" then fit the regression of\\ny = ln (expenditure on gambling in pounds per year)\\non x1 = socioeconomic status score (based on parents' occupation)\", ' x2 = ln(income) (in pounds per week)', ' x3 = verbal score in words out of 12 correctly defined', ' and x4 = sex (0=male', ' 1=female). The estimated regression coefficients were:\\n\\n(a) Consider two British teenagers: a girl with status = 30 and verbal = 4', ' and a boy with status = 60 and verbal = 6 (assume their respective incomes are equal). Use the model fit above to make a prediction of which child gambles more and by how much. Provide as precise an answer as you can', ' for a client who does not understand logarithms.\\n\\n(b) Can you compute a standard error for your estimate in part (a)? A portion of the estimated variance-covariance matrix C^ov(b)', ' from the R function vcov()', ' is given below.\\n\\n', 'Multiple Linear Regression', ' Parameter', ' prediction'], ['lr_final_2016_ p38', 'A national insurance organization wanted to study the consumption pattern of cigarettes in all 50 states and the District of Columbia. They fit a linear regression of y = packs of cigarettes sold per capita in each state on x1 = median age', ' x2 = per capita income', \" and x3 = average price of a pack of cigarettes. Index plots for Cook's distance\", ' outlier t-statistics', ' and hat values are given below.\\nDiagnostic Plots\\n\\n\\n(a) With a single sentence', ' explain to someone who has zero knowledge of or interest in statistical modeling: What is it about Nevada and New Hampshire (cases 29 and 30', \" respectively) that makes them stick out in these plots?\\n\\n(b) Carefully explain how you could generalize the idea of Cook's distance to compute a measure of the combined influence of cases 29 and 30. Can you foresee a practical difficulty in the assessment of this measure?\", ' Multiple Linear Regression', ' Diagnostic'], ['lr_midterm_2016_p1', '1. Given data {(xi\\n', ' yi) : i = 1', ' . . . ', ' n} and assuming the simple linear regression model Yi = β0 +β1xi + εi', ' it will generally be the case that Xni=1(yi − β0 − β1xi)2 <Xni=1(yi − b0 − b1xi)2 where b1 and b0 denote the least squares estimates of β1 and β0', ' respectively. True False\\n', 'Simple Linear Regression', ' least square estimation'], ['lr_midterm_2016_p2', 'The least squares estimates of the slope and intercept in simple linear regression are the values of b1 and b0 that minimize the sum of the squared vertical distances between the points (xi', ' yi) and the line y = b0 + b1x in a scatterplot of y versus x. True False', 'Simple Linear Regression', ' least square estimation'], ['lr_midterm_2016_p3', 'The least squares estimates of the slope and intercept in simple linear regression are found by solving the so-called normal equations', ' a nonlinear system with no closed-form solution', ' but readily solvable with modern computers', ' via an iterative algorithm. True False', 'Simple Linear Regression', ' least square estimation'], ['lr_midterm_2016_p5', 'In simple linear regression', ' the ANOVA F-test of no linear association', ' and the t-test of H0 : β1 = 0 versus Ha : β1 6= 0', ' will give the exact same result; in fact', ' the test statistics are related by F ∗ = (t ∗)2.\\nTrue False', 'Simple Linear Regression', ' Hypothesis Testing'], ['lr_midterm_2016_p6', 'One reason the assumption of constant variance is so crucial in simple linear regression is that the least squares estimators b1 and b0 are no longer unbiased for β1 and β0', ' respectively', ' if the assumption of constant variance is violated. True False', 'Simple Linear Regression', ' constant variance'], ['lr_midterm_2016_p7', 'If we think of SSTO = Pn i=1(yi − y¯) 2 as a measure of the total variation observed in our data set', ' and define the residual sum of squares for our model fit by SSE = Pni=1(yi − yˆi)2 then we can reasonably interpret SSE/SSTO as the proportion of that variation which is unexplained by the model which yielded the fitted values ˆyi. True False', 'residual'], ['lr_midterm_2016_p8', 'The residuals versus fitted values plot is a useful graphical tool for assessing the assumption that the error term and response variable are uncorrelated', ' that is', ' Cov(εi', ' Yi) = 0 for i = 1', ' . . . ', ' n. True False', 'residual plots'], ['lr_midterm_2016_p9', 'In simple linear regression', ' the residuals versus fitted values plot and the residuals versus predictor variable plot are equivalent', ' as the ˆyi and the xi are simple linear transformations of each other (of course the plot gets “flipped” if the least squares line has negative slope', ' b1 < 0). True False', 'Simple Linear Regression', ' residual plots'], ['lr_midterm_2016_p10', 'A residuals versus fitted values plot like the following would suggest a variance function that is not constant', ' but rather', ' increasing with the mean response. True False', 'residual plots'], ['lr_midterm_2016_p11', 'The usual ANOVA F-test of H0 : β1 = 0 versus Ha : β1 6= 0 in simple linear regression is actually a special case of a more general framework for testing H0 : Reduced model vs. Ha : Full  model based on the test statistic F =SSE(Reduced) − SSE(Full)dfReduced − dfFull ', 'SSE(Full)dfFull True False', 'Simple Linear Regression', ' Hypothesis Testing'], ['lr_midterm_2016_p12', 'The F-test for linear association and the F-test for lack of fit in simple linear regression have the same null (reduced) model', ' but different alternatives. True False', 'Simple Linear Regression', ' Hypothesis Testing'], ['lr_midterm_2016_p13', 'The alternative model in the F-test for lack of fit is not fully general', ' in that it assumes a normally distributed error term; the mean and variance functions', ' however', ' are completely arbitrary. True False', 'Hypothesis Testing'], ['lr_midterm_2016_p14', ' In the decomposition of SSE that underlies the F-test for lack of fit', ' the sum of squares for pure error', ' SSPE =PjPi(yij − y¯j )2', ' is that which could', ' in principle', ' be made zero by a “perfect-fitting” model. True False\\n', 'Hypothesis Testing'], ['lr_midterm_2016_p15', 'The basic idea underlying data transformations in simple linear regression is that if the model E(Y |X = x) = β0 + β1x and Var(Y |X = x) = σ2 fails to hold', ' perhaps there exist functions ψ(·) and T(·) such that E[T(Y )|X = x] = β0 + β1ψ(x) and Var[T(Y )|X = x] = σ2 does. True False\\n', 'Simple Linear Regression', '  power transformation '], ['lr_midterm_2016_p16', 'The great value of optimality criteria for power transformations is that they eliminate the need for subjective judgment on the part of the analyst: simply apply the power transformation that the algorithm suggested', ' and report that model fit. True False', ' power transformation '], ['lr_midterm_2016_p17', 'Given a joint 1−α confidence region for the simple linear regression parameters (β0', ' β1)', ' one can compute a 1 − α confidence band for the entire mean function', ' that is', ' E(Y |X = x) = β0 +β1x for all values of x. The Working-Hotelling method is an example. True False', 'Simple Linear Regression', ' confidence interval'], ['lr_midterm_2016_p18', 'A necessary and sufficient condition for the simple linear regression model to hold for a bivariate data set {(xi', ' yi) : i = 1', ' . . . ', ' n} is that the (xi', ' yi) be a random sample from a bivariate normal distribution. True False', 'Simple Linear Regression', ' probability distribution'], ['lr_midterm_2016_p19', 'This problem concerns a data set on 31 felled black cherry trees. Let xi denote\\nthe diameter of the ith tree (in inches)', ' and let yi denote the volume of timber in cubic feet.\\nFollowing are (i) a scatterplot of y versus x', ' overlaid with lowess and least squares lines for\\nthe regression of y on x; and (ii) a plot of residuals versus fitted values for the least squares\\nregression of y on x (with lowess curve overlaid).\\n(a) Without doing any calculation', ' predict the volume of a tree known to have a trunk diameter\\nof 14 inches. Indicate whether you’re using lowess or least squares', ' and defend your choice.\\n(b) Do the assumptions of the normal simple linear regression model seem to hold for these\\ndata? Support your answer with specific reference to the information provided by the\\nabove plots.\\n(c) If you had the raw data and a computer', ' what would be your next step to try to improve\\nyour answer to part (a)? Recall that the volume of a cylinder with height h and radius r\\nis V = πr2h. (Data on the heights of the 31 trees are not available.)', 'Simple Linear Regression', ' least square estimation', ' residual plots', ' Prediction'], ['lr_midterm_2016_p20', 'Consider an agricultural experiment with n = 24 replications (on 24 different plots\\nof land). Let xi denote the quantity of phosphorous fertilizer', ' and yi the total yield', ' for the ith\\nplot', ' both measured in kilograms per hectare.\\nThere were three replications at each of x = 5', ' 10', ' 15', ' 20', ' 30', ' and 50 kg/ha', ' and six replications\\nat x = 40 kg/ha.\\nConsider the simple linear regression model\\nYi = β0 + β1xi + εi for i = 1', ' . . . ', ' n\\nwhere\\nε1', ' . . . ', ' εn ∼ iid Normal(0', ' σ2\\n)\\nand suppose we wish to conduct a test of lack of fit.\\n(a) Clearly state the null and alternative hypotheses for this test. You should be explicit\\nabout the mean function', ' error variance', ' and error distribution under both your reduced\\nand full models.\\n(b) For these data we obtain F\\n∗ =\\nMSLF\\nMSPE = 0.42. Express the P-value for the lack-of-fit test as\\na probability P(X > c)', ' specifying the numeric value of c and the probability distribution\\nof the random variable X.', 'Simple Linear Regression', ' Hypothesis Testing'], ['lr_midterm_2016_p22', 'Continue with the regression of legume yield on amount of phosphorous fertilizer', '\\nboth measured in kg/ha', ' from the previous problem.\\nAnalysis of Variance Table\\nResponse: y\\nDf Sum Sq Mean Sq F value Pr(>F)\\nx 1 10704014 10704014 58.748 1.193e-07\\nResiduals 22 4008485 182204\\n> predict(m1', ' data.frame(x=25)', ' interval=\"confidence\")\\nfit lwr upr\\n1 3896.21 3714.897 4077.522\\n> qt(.975', ' df=22)\\n[1] 2.073873\\nA 95% confidence interval for the mean yield with phosphorous level x = 25 is [3714', ' 4078].\\n(a) Compute an interval which you are 95% confident will contain the value of the yield in\\nthe next plot using fertilizer with phosphorous level x = 25.\\n(b) Compute an interval which you are 95% confident will contain the average yield for three\\nplots using fertilizer with phosphorous level x = 25.', 'Simple Linear Regression', ' confidence interval'], ['lr_final_1_p1', 'True/False. Please read each statement and put T(True)/F(False) in the beginning (2 pts\\nfor each statement). Suppose we have n observations (x1', ' y1)', ' . . . ', '(xn', ' yn) from the following model\\nyi = β0 + β1xi + β2x2\\ni + i', ' (1)\\nwhere 1', ' . . . ', ' n are i.i.d. standard normal random variables. Bill has fit the model (1) and obtained\\nthe least square estimates βˆ0', ' βˆ1', ' βˆ2. Jim has the same data', ' but he fits the following model without\\nthe quadratic term\\nyi = β0 + β1xi + i (2)\\nand obtains the least squares estimates β˜0', ' β˜1. In statements (a) – (e) you will compare Bill’s and\\nJim’s estimates.\\n(a) βˆ0 is unbiased.\\n(b) β˜1 must be biased.\\n(c) Var(βˆ1) ≥ Var(β˜1).\\n(d) Since Bill uses the true model (1) and Jim uses the wrong model (2)', ' the mean square\\nerror of β˜0 is at least as big as the mean square error of βˆ0.\\n(e) If model (1) and (2) have the same R2', ' we prefer model (2).\\n(f) Least absolute deviation regression is preferred over least square estimator when there\\nare possible outliers.\\n(g) A large VIF (Variance Inflation Factor) indicates that there exists multicollinearity\\nproblem.\\n(h) If by diagnostic we find the error variances are not equal across observations', ' we may\\nuse Ridge Regression to solve the problem.\\n(i) We need to fit the model n times when calculating the PRESS statistic for model (1).\\n(j) Even if we have multicollinearity problem', ' we still have good fit of the data and good\\nprediction.', ' Multiple Linear Regression', ' Diagnostic', ' Parameter'], ['lr_final_1_p2', 'Suppose we have the following two multiple linear regression models. Here i ∼i.i.d N(0', ' σ2).\\nYi = β0 + β1Xi1 + · · · + βp−1Xi', 'p−1 + i. (3)\\nYi = β0 + β1Xi1 + · · · + βp−1Xi', 'p−1 + βpXi', 'p + i. (4)\\n(a) (10 pts) Denote the R2 (the coefficient of multiple determination) for the two models as R2(1)\\nand R2(2). Is it true that R2(1) ≤ R2(2) always holds? If yes', ' prove it. If not', ' give a counter\\nexample. (If you are providing a counter example', ' please write down the design matrix and the\\nresponse explicitly. The reasoning of R2(1) > R2(2) is required.)\\n2\\n(b) (10 pts) Denote the R2\\na (the adjusted coefficient of multiple determination) for the two models\\nas R2\\na(1) and R2\\na(2). Is it true that R2\\na(1) ≤ R2\\na(2) always holds? If yes', ' prove it. If not', ' give a\\ncounter example. (If you are providing a counter example', ' please write down the design matrix\\nand the response explicitly. The reasoning of R2\\na(1) > R2\\na(2) is required.)', 'Multiple Linear Regression', ' r square'], ['lr_final_1_p3', 'Suppose we have the following multiple linear regression model. Here �i ∼i.i.d N(0', ' σ2).\\nYi = β0 + β1X1i + β2X2i + β3X3i + �i. (5)\\nWe perform the analysis in R:\\n> n=100\\n> X1 = rnorm(n)\\n> X2 = rnorm(n)\\n> X3 = X1*0.2 + X2*0.3 + 0.5* rnorm(n)\\n> Y = 2*X1 + 3*X2+ rnorm(n)\\n> fit = lm(Y~X1+X2+X3)\\n> anova(fit)\\nAnalysis of Variance Table\\nResponse: Y\\nDf Sum Sq Mean Sq F value Pr(>F)\\nX1 1 460.61 460.61 ? <2e-16 ***\\nX2 1 873.37 873.37 1036.4910 <2e-16 ***\\nX3 1 0.63 0.63 0.7529 0.3877\\nResiduals 96 80.89 0.84\\n---\\nSignif. codes: 0 *** 0.001 ** 0.01 * 0.05 . 0.1 1\\n(a) (5 pts) Calculate the missing F value for X1.\\n(b) (5 pts) Calculate the SSR (Regression Sum of Squares) from the output.\\n(c) (5 pts) Calculate the adjusted R-square value from the output.\\n(d) (5 pts) Perform the following hypothesis test\\nH0 : β2 = β3 = 0 v.s. H1 : not both β2 and β3 equal zero.\\nWrite down the test method with the rejection rule and calculate the test statistic.\\n(e) (10 pts) Calculate the Extra Sum of Squares SSR(X3|X1', ' X2) and the coefficient of partial\\ndetermination R2\\nY 3|12.', 'Multiple Linear Regression', ' r square', ' Hypothesis Testing'], ['lr_final_1_p4', 'Consider multiple linear regression model as follows:\\nYi = β0 + β1Xi', '1 + · · · + βp−1Xi', 'p−1 + i', ' (6)\\nwhere i ∼i.i.d N(0', ' σ2)', 'i = 1', ' · · · ', ' n. Assume that X1', ' X2', ' ...', ' Xp−1 are all centered at 0 (i.e.', ' X¯k =\\nn−1 n\\ni=1 Xi', 'k = 0', ' k = 1', ' ...', ' p − 1)', ' and are uncorrelated with each other (i.e.', '\\nn\\ni=1 Xi', 'jXi', 'k = 0', '\\n∀j ∕= k). Denote the Ordinary Least Squares (OLS) estimate as bk for βk', ' k = 0', ' · · · ', ' p − 1.\\n(a) (10 pts) Now for each k = 1', ' · · · ', ' p − 1', ' we perform the following simple linear regression model\\nwith the data points (X1', 'k', ' Y1)', ' · · · ', '(Xn', 'k', ' Yn).\\nYi = β0 + βkXi', 'k + i. (7)\\nSuppose the new OLS estimate for the simple linear regression model is denoted as βˆ\\nk. Prove\\nβˆ\\nk = bk', ' for k = 1', ' · · · ', ' p − 1.\\n(b) (10 pts) Suppose we are considering dropping the predictor X∗ ∈ {X1', ' X2', ' ...', ' Xp−1} from model\\n(6). Show that', ' for this purpose', ' the F-statistics in general linear test is equal to the square of\\nthe t-statistics in the corresponding t-test.', 'Multiple Linear Regression', ' Simple Linear Regression', ' Hypothesis Testing'], ['lr_final_1_p5', 'Suppose we have data (x1', ' y1)', ' · · · ', '(xn', ' yn)', ' where xi = (xi1', ' · · · ', ' xip)′\\n', 'i = 1', ' · · · ', ' n. If we\\nfit the following regression model.\\nyi = x′\\niβ + i', 'i = 1', ' · · · ', ' n', ' (8)\\nwhere β = (β1', ' β2', ' · · · ', ' βp)′\\n. Let βˆ be the least square estimate', ' then it is found out that the i-th\\nobservation lies exactly on the regression line', ' i.e.', ' yi = x′\\niβˆ.\\nNow', ' fit the regression model again using all observations except the i-th observation (xi', ' yi). Denote\\nthe least square estimate of the regression coefficients by βˆ(i). Show βˆ(i) = βˆ.', 'Multiple Linear Regression', ' least square estimation'], ['lr_gy_hw1_p1', 'Sixteen batches of the plastic were made', ' and from each batch one test item was molded.\\nEach test item was randomly assigned to one of the four predetermined time levels', ' and\\nthe hardness was measured after the assigned elapsed time. The results are shown below;\\nX is the elapsed time in hours', ' and Y is hardness in Brinell units. Assume the \\x0crst-order\\nregression model (1.1) is appropriate (model (2.1) in the notes).\\nData not displayed\\nPerform the following tasks:\\ni. Use R to obtain the estimated regression function.\\nii. Use R to create a scatter plot with the line of best \\x0ct. Make the line of best \\x0ct red.\\niii. Use R to calculate the best point estimate of \\x1b2.\\niv. Use R to calculate the sample correlation coe\\x0ecient and coe\\x0ecient of determination.', 'Simple Linear Regression', ' least squares estimation'], ['lr_gy_hw1_p2', 'Recall the sample residual is de\\x0cned by ei = yi\\U00100000^yi', ' where yi is the ith response value and ^yi\\nis its corresponding \\x0ctted value computed by least squares estimates ^yi = ^ \\x0c0 + ^ \\x0c1xi. Prove\\nthe following properties:\\ni.\\nXn\\ni=1\\nxiei = 0\\nii.\\nXn\\ni=1\\n^yiei = 0\\n1', 'Simple Linear Regression', ' least squares estimation', ' parameter'], ['lr_gy_hw1_p3', 'Recall that the ith \\x0ctted value ^ Yi can be expressed as a linear combination of the response\\nvalues', ' i.e.', '\\n^ Yi =\\nX\\nj=1\\nhijYj ;\\nwhere\\nhij =\\n1\\nn\\n+\\n(xi \\U00100000 \\x16x)(xj \\U00100000 \\x16x)\\nSxx\\n;\\nand\\nSxx =\\nX\\ni=1\\n(xi \\U00100000 \\x16x)2:\\nProve the following properties of the hat-values hij .\\ni. X\\nj=1\\nh2\\nij = hii\\nii. X\\nj=1\\nhijxj = xi', 'Simple Linear Regression', ' least squares estimation', ' parameter'], ['lr_gy_hw1_p4', 'Consider the regression through the origin model given by\\n(1) Yi = \\x0cxi + \\x0fi i = 1; 2; : : : ; n \\x0fi\\niid \\x18 N(0; \\x1b2):\\nThe estimated model at observed point (x; y) is\\n^y = ^ \\x0cx;\\nwhere\\n(2) ^ \\x0c =\\nPn\\nPi=1 xiyi n\\ni=1 x2i\\n:\\nComplete the following tasks\\ni. Show that\\n^ \\x0c =\\nPn\\ni=1 P xiYi n\\ni=1 x2i\\nis an unbiased estimator of \\x0c.\\nii. Compute the standard error of estimator ^ \\x0c.\\niii. Identify the probability distribution of estimator ^ \\x0c.', 'Simple Linear Regression', ' least squares estimation', ' parameter'], ['lr_gy_hw2_p1', 'Sixteen batches of plastic were made', ' and from each batch one test item was molded. Each\\ntest item was randomly assigned to one of the four predetermined time levels', ' and the\\nhardness was measured after the assigned elapsed time. The results are shown below; X\\nis the elapsed time in hours', ' and Y is hardness in Brinell units. Assume the \\x0crst-order\\nregression model (1.1) is appropriate ((2.1) in the notes).\\nData not displayed\\nUse R to perform the following tasks:\\ni. Estimate the change in the mean hardness when the elapsed time increases by one hour.\\nUse a 99 percent con\\x0cdence interval. Interpret your interval estimate.\\nii. The plastic manufacturer has stated that the mean hardness should increase by 2 Brinell\\nunits per hour. Conduct a two-sided test to decide whether this standard is being\\nsatis\\x0ced; use \\x0b = :01.\\niii. Set up the ANOVA table.\\niv. Test by means of an F-test whether or not there is a linear association between the\\nhardness of the plastic and the elapsed time. Use \\x0b = :01.\\nv. Does t2\\ncalc from part [ii] equal fcalc from part [iv]? Explain why this identity holds or\\ndoes not hold.\\nvi. Construct 95% Bonferroni joint con\\x0cdence intervals for estimating both the true inter-\\ncept \\x0c0 and the true slope \\x0c1.\\nvii. Construct 95% Bonferroni joint con\\x0cdence intervals for predicting the true average\\nhardness corresponding to elapsed times 20', ' 28 and 36 hours.\\n1', 'Simple Linear regression', ' parameter', ' Hypothesis Testing', ' t test', ' F test', ' ANOVA', ' simaltaneous inference'], ['lr_gy_hw2_p2', 'Consider the simple linear regression model\\nYi = \\x0c0 + \\x0c1x + \\x0fi i = 1; 2; : : : ; n \\x0fi\\niid \\x18 N(0; \\x1b2):\\ni. Assuming H0 : \\x0c1 = 0 is true', ' use R to simulate the sampling distribution of the\\nF-statistic\\nF =\\nMSR\\nMSE\\n=\\nSSR=1\\nSSE=(n \\U00100000 2)\\n:\\nAssume \\x0c0 = 10', ' \\x1b = 3', ' n = 30 and run the loop 10', '000 times to generate the sampling\\ndistribution. Run the following code preceding the loop so that everyone has the same\\nseed and X data vector. Fill in the missing code to receive full credit.\\n# Set seed\\nset.seed(0)\\n# Assign sample size and create x vector\\nn <- 30\\n# Empty list for f-statistics\\nf.list <- NULL\\nx <- runif(n', 'min=0', 'max=100)\\n# Run loop\\nfor (i in 1:10000) {\\n# Fill in the body of the loop here...\\n}\\nii. From the simulated sampling distribution', ' plot a histogram and overlay the correct F-\\ndensity on the histogram. Adjust the bin-size to breaks=50 in the histogram. Overlay\\nthe F-density in red.\\niii. Compute the 95th percentile of both the simulated sampling distribution and the correct\\nF-distribution. Compare these values.', 'Simple Linear Regression', ' Hypothesis Testing', ' F test'], ['lr_gy_hw2_p3', 'Consider splitting the response values y1; : : : ; yn into two groups with respective sample sizes\\nn1 and n2. De\\x0cne the dummy variable\\n(1) xi =\\n(\\n1 if group one\\n0 if group two\\nShow that the least squares estimators of \\x0c1 and \\x0c0 are respectively\\n^ \\x0c1 = \\x16y1 \\U00100000 \\x16y2 and ^ \\x0c0 = \\x16y2;\\nwhere \\x16y1 and \\x16y2 are the respective sample means of each group.', 'Simple Linear Regression', ' parameter', ' least square estimation', ' dummy variable'], ['lr_gy_hw2_p4', 'Fusible interlinings are being used with increasing frequency to support outer fabrics and\\nimprove the shape and drape of various pieces of clothing. The article Compatibility of\\nOuter and Feasible Interlining Fabrics in Tailored Garments gave the accompanying data\\non extensibility (%) at 100 gm/cm for both high-quality (H) fabric and poor-quality (P)\\nfabric specimens.\\nH 1.2 .9 .7 1.0 1.7 1.7 1.1 .9 1.7\\n1.9 1.3 2.1 1.6 1.8 1.4 1.3 1.9 1.6\\n.8 2.0 1.7 1.6 2.3 2.0\\nP 1.6 1.5 1.1 2.1 1.5 1.3 1.0 2.6\\nUse R to perform the following tasks.\\ni. Create an appropriate graphic to visualize the relationship between extensibility and\\nquality. Do you think there is a relationship between extensibility and quality? Make\\nsure to label the plot.\\nii. Using the indicator variable\\nx =\\n(\\n1 if high quality\\n0 if low quality\\nrun a regression analysis to test if the average fabric extensibility di\\x0bers per group.', 'Simple Linear regression', ' parameter', ' Diagnostic', ' linearity', ' dummy variable'], ['lr_gy_hw2_p5', 'i. Consider the regression through the origin model given by\\n(2) Yi = \\x0cxi + \\x0fi i = 1; 2; : : : ; n \\x0fi\\niid \\x18 N(0; \\x1b2):\\nDerive the maximum likelihood estimators of \\x0c and \\x1b2.\\nii. Consider the residuals ei related to the regression through the origin model (2). Prove\\nthat\\nXn\\ni=1\\neixi = 0:\\nAlso', ' in the regression through the origin model (2)', ' is the sum of residuals equal to\\nzero? I.e.', ' is the following relation true?\\nXn\\ni=1\\nei = 0:\\nExplain your answer in a few sentences or less.\\niii. Consider testing the null/alternative pair\\nH0 : \\x0c = \\x0c0 v.s. HA : \\x0c 6= \\x0c0:\\nNote that \\x0c0 is the hypothesized value. Show that the likelihood-ratio test can be based\\non the rejection region jTj > k with test statistic\\nT =\\n^ \\x0c \\U00100000 \\x0c0\\nrPn\\ni=1(YPi\\U00100000\\x0c^xi)2=(n\\U001000001) n\\ni=1 x2i\\n:\\nNote that k is some positive real number and ^ \\x0c is the maximum likelihood estimator\\nof \\x0c.\\niv. Under H0', ' what is the probability distribution of the above test statistic T?\\nHints: To solve 5 Part iii:\\n(a) Compute the likelihood-ratio test statistic (\\x15) from De\\x0cnition 2.4 on Page 45 of\\nthe class notes.\\n(b) When simplifying the expression', ' the following trick might be useful:\\nXn\\ni=1\\n(Yi \\U00100000 \\x0c0xi)2 =\\nXn\\ni=1\\n(Yi \\U00100000 ^ \\x0cxi + ^ \\x0cxi \\U00100000 \\x0c0xi)2:\\n(c) After simplifying \\x15 < c', ' \\x0cnd a suitable transformation of \\x15 that yields the desired\\ntest statistic and rejection rule.', 'Simple Linear Regression', ' maximum likelihood estimation', ' residual', ' Hypothesis Testing', ' t test'], ['lr_gy_hw3_p1', 'Consider the model\\nYi = \\x16 + \\x0fi i = 1; 2; : : : ; n \\x0fi\\niid \\x18 N(0; \\x1b2):\\nThe sample mean and sample variance are de\\x0cned respectively as\\n\\x16 Y =\\n1\\nn\\nXn\\ni=1\\nYi\\nand\\nS2\\nY =\\n1\\nn \\U00100000 1\\nXn\\ni=1\\n(Yi \\U00100000 \\x16 Y )2:\\nUse Theorem (3.6) on Page 91 to prove that the sample mean \\x16 Y and sample variance S2\\nY\\nare independent random variables.', 'Multiple Linear Regression', ' parameter'], ['lr_gy_hw3_p2', 'Consider the single factor anova model with three groups. The three groups are drug dose\\n1', ' drug dose 2 and control. Let n1 and \\x16y1 respectively denote the number of respondents\\nand sample mean response for drug dose 1 group. Let n2 and \\x16y2 respectively denote the\\nnumber of respondents and sample mean response for drug dose 2 group. Let n3 and \\x16y3\\nrespectively denote the number of respondents and sample mean response for the control\\ngroup. Note that n = n1 +n2 +n3. The one-way anova can be expressed using the multiple\\nlinear regression model\\n(1) Yi = \\x0c0 + \\x0c1xi1 + \\x0c2xi2 + \\x0fi; i = 1; 2; : : : ; n \\x0fi\\niid \\x18 N(0; \\x1b2);\\nxi1 =\\n(\\n1 if drug dose 1\\n0 otherwise\\nxi2 =\\n(\\n1 if drug dose 2\\n0 otherwise\\n1\\ni. Write down the design matrix and response vector describing model (1).\\nii. Compute (XTX)\\U001000001 and simplify the result. Note: This requires inverting a 3 \\x02 3\\nmatrix. Simply look up the formula online.\\niii. Estimate \\x0c =\\n\\U00100000\\n\\x0c0 \\x0c1 \\x0c2\\n\\x01T\\nusing the least squares equation.\\niv. Write down an expression for the estimated covariance matrix of ^\\x0c\\n.', 'Multiple Linear Regression', ' parameter', ' least square estimation', ' dummy variable'], ['lr_gy_hw3_p3', 'A commercial real estate company evaluates vacancy rates', ' square footage', ' rental rates', '\\nand operating expenses for commercial properties in a large metropolitan area in order to\\nprovide clients with quantitative information upon which to make rental decisions. The\\ndata below are taken from 81 suburban commercial properties that are the newest', ' best\\nlocated', ' most attractive', ' and expensive for \\x0cve speci\\x0cc geographic areas. The data consists\\nof variables age (X1)', ' operating expenses and taxes (X2)', ' vacancy rates (X3)', ' total square\\nfootage (X4)', ' and rental rates (Y ). For this data set', ' we skip residual diagnostics but in\\npractice', ' that should be included in the analysis. The data set HW3Problem3.txt is posted\\non Canvas. Use R to perform the following tasks:\\ni. Regress the rental rates (Y ) against all of the covariates; age (X1)', ' operating expenses\\nand taxes (X2)', ' vacancy rates (X3)', ' total square footage (X4). Write down the estimated\\nlinear model.\\nii. What percentage of variation in rental rates is explained by this model?\\niii. Are there any marginal relationships between the response variable and covariates?\\n(Run t-tests on all slope parameters.)\\niv. Run a F-test to see if there is an overall relationship between the rental rates and all\\nof the covariates.\\nv. Run a F-test to simultaneously test the slopes for age (X1) and vacancy rates (X3).\\nvi. Run a F-test to see if vacancy rates (X3) is a signi\\x0ccant predictor after holding all other\\nvariables constant. To perform this test', ' use the full and reduced models. How does\\nthis test relate to the summary output from part (ii)?\\nvii. The researcher wishes to obtain 95% interval estimates of the mean rental rates for\\nfour typical properties speci\\x0ced as follows. Find the four con\\x0cdence intervals using the\\nBonferroni procedure.\\n2\\n1 2 3 4\\nx1 5.0 6.0 14.0 12.0\\nx2 8.25 8.50 11.50 10.25\\nx3 0 0.23 0.11 0\\nx4 250', '000 270', '000 300', '000 310', '000', 'Multiple Linear Regression', ' least square estimation', ' F test'], ['lr_gy_hw3_p4', 'i. Recall the multiple linear regression model:\\n(2) Yi = \\x0c0 + \\x0c1xi1 + \\x0c2xi2 + \\x01 \\x01 \\x01 + \\x0cp\\U001000001xi;p\\U001000001 + \\x0fi:\\nFor each of the following regression models', ' indicate whether it can be expressed in the\\nform of (2) by a suitable transformation. To receive full credit', ' describe the transfor-\\nmation if it exists.\\na. Yi = \\x0c0 + \\x0c1xi1 + \\x0c2 log(xi2) + \\x0c3x2i\\n1 + \\x0fi\\nb. Yi = \\x0fi expf\\x0c0 + \\x0c1xi1 + \\x0c2x2i\\n1g\\nc. Yi = log(\\x0c1xi1) + \\x0c2xi2 + \\x0fi\\nd. Yi = \\x0c0 expf\\x0c1xi1g + \\x0fi\\ne. Yi = [1 + expf\\x0c0 + \\x0c1xi1 + \\x0fig]\\U001000001\\nii. Consider the toy data set:\\ny 2.44 8.36 98.33 115.06 128.91 123.46 148.30 138.10 153.10 119.08\\n87.66 134.88 91.71 126.81 40.41 54.94 33.03 35.74 14.99 -1.18\\n2.44 8.36 28.33 45.06 48.91 43.46 118.30 108.10 233.10 199.08\\n337.66 384.88\\nx 0.00 0.00 1.00 1.00 2.00 2.00 3.00 3.00 4.00 4.00\\n5.00 5.00 6.00 6.00 7.00 7.00 8.00 8.00 9.00 9.00\\n10.00 10.00 11.00 11.00 12.00 12.00 13.00 13.00 14.00 14.00\\n15.00 15.00\\nThe data set is provided in the \\x0cle HW3Problem4.txt on canvas. Use multiple linear\\nregression techniques to \\x0ct a polynomial to the above data set. To receive full credit', '\\nwrite down the estimated model and create a scatter plot with the estimated curve\\noverlaid on the plot.', 'Generalized Linear Regression'], ['lr_gy_hw4_p2', 'Sixteen of the plastic were made', ' and from each batch one test item was molded. Each test\\nitem was randomly assigned to one of the four predetermined time levels', ' and the hardness\\nwas measured after the assigned elapsed time. For this data set; x is the elapsed time\\nin hours and Y is hardness in Brinell units. Use R to run a F- lack-of-\\x0ct test to see if a\\nlinear relationship is appropriate for this data set. The data set 1 22.txt is posted on\\nCanvas.', 'Simple Linear Regression', ' Hypothesis Testing', ' F lack-of-fit test'], ['lr_gy_hw4_p3', 'Consider the non-constant variance linear model\\n(1) Yi = \\x0c0 + \\x0c1xi;1 + \\x0c2xi;2 + \\x01 \\x01 \\x01 + \\x0cp\\U001000001xi;p\\U001000001 + \\x0fi;\\nwith\\n\\x0fi\\niid \\x18 N(0; \\x1b2\\ni ); i = 1; : : : ; n:\\nDe\\x0cne the reciprocal of the variance \\x1b2\\ni as the weight wi:\\nwi =\\n1\\n\\x1b2\\ni\\nand let\\nW =\\n0\\nBBBB@\\nw1 0 \\x01 \\x01 \\x01 0\\n0 w2 \\x01 \\x01 \\x01\\n...\\n...\\n...\\n. . . 0\\n0 \\x01 \\x01 \\x01 0 wn\\n1\\nCCCCA\\n:\\nWe can estimate the non-constant variance model by minimizing the objective function\\n(2) Qw(\\x0c) =\\nXn\\ni=1\\nwi(yi \\U00100000 \\x0c0 \\U00100000 \\x0c1xi1 \\U00100000 \\x01 \\x01 \\x01 \\x0cp\\U001000001xi;p\\U001000001)2\\nTask: Derive the weighted least squares equation\\n(3) ^\\x0c\\nw = (XTWX)\\U001000001XTWY', 'Multiple Linear Regression', ' least square estimation'], ['lr_gy_hw4_p4', 'Observations on Y are to be taken when x = 10; 20; 30; 40; and 50', ' respectively. The true\\nregression function is E[Y ] = 20 + 10x. The error terms are independent and normally\\ndistributed with E[\\x0fi] = 0 and V ar[\\x0fi] = :8x.\\ni. Generate a random Y observation for each x level and calculate both the ordinary and\\nweighted least squares estimates of the regression coe\\x0ecient \\x0c1 in the simple linear\\nregression function.\\nii. Repeat part (a) 10', '000 times', ' generating new random numbers each time.\\niii. Calculate the mean and variance of the 10', '000 ordinary least squares estimates of \\x0c1\\nand do the same for the 10', '000 weighted least squares estimates.\\niv. Do both the ordinary least squares and weighted least squares estimators appear to be\\nunbiased? Explain. Which estimator appears to be more precise here? Comment.', 'Multiple Linear Regression', ' weighted least square estimation', ' prediction'], ['lr_gy_hw5_p1', 'A commercial real estate company evaluates vacancy rates', ' square footage', ' rental rates', '\\nand operating expenses for commercial properties in a large metropolitan area in order to\\nprovide clients with quantitative information upon which to make rental decisions. The\\ndata below are taken from 81 suburban commercial properties that are the newest', ' best\\nlocated', ' most attractive', ' and expensive for \\x0cve speci\\x0cc geographic areas. The data consists\\nof variables age (X1)', ' operating expenses and taxes (X2)', ' vacancy rates (X3)', ' total square\\nfootage (X4)', ' and rental rates (Y ). The data set HW3Problem3.txt is posted on canvas.\\nUse R to perform the following exercises. Using type I sums of squares (F-stat)', ' test if age\\n(X1) is a signi\\x0ccant predictor', ' after holding operating expenses & taxes (X2)', ' vacancy rates\\n(X3) and total square footage (X4) constant.', 'Simple Linear Regression', ' F test'], ['lr_gy_hw5_p2', 'Show that: SSR(x1; x2; x3; x4) = SSR(x1) + SSR(x2; x3jx1) + SSR(x4jx1; x2; x3).', 'sum of squares'], ['lr_gy_hw5_p3', 'Consider the model\\nYi = \\x0c0 + \\x0c1xi1 + \\x0c2xi2 + \\x0fi:\\nAssuming that the sample correlation between x1 and x2 is zero', ' i.e.', '\\n1\\n(n \\U00100000 1)s1s2\\nXn\\ni=1\\n(xi1 \\U00100000 \\x16x1)(xi2 \\U00100000 \\x16x2) = 0;\\nshow that\\nSSR(x2jx1) = SSR(x2):\\n1', 'sum of squares'], ['lr_gy_hw5_p4', \"An assistant in the district sales o\\x0ece of a national cosmetics \\x0crm obtained data on advertis-\\ning expenditures and sales last year in the district's 44 territories. X1 denotes expenditures\\nfor point-of-sale displays in beauty salons and department stores (in thousand dollars)\", ' and\\nX2 and X3 represent the corresponding expenditures for local media advertising and pro-\\nrated share of national media advertising', ' respectively. Let Y denote sales (in thousand\\ncases). The assistant was instructed to study the in\\nuence variables X1 and X2 have on\\nsales Y . The data set CosmeticsSales.txt is posted on Canvas.\\nUse R to perform the following tasks:\\ni. Run the simple linear regression Y \\x18 X1. Test if expenditures for point-of-sale displays\\nin beauty salons and department stores (X1) statistically in\\nuences sales (Y ).\\nii. Run the simple linear regression Y \\x18 X2. Test if expenditures for local media advertis-\\ning (X2) statistically in\\nuences sales (Y ).\\niii. Now run the the full regression Y \\x18 X1 + X2 + X3. Perform marginal t-tests to see\\nif X1 statistically in\\nuences sales (Y ) and if X2 statistically in\\nuences sales (Y )', ' after\\ncontrolling for the variance of X3. Brie\\ny compare the results to Parts i. and ii. and\\ncomment on any discrepancies. Note: No need for Bonferroni.\\niv. You should have noticed some discrepancies in Part iii. Explain why these discrepan-\\ncies are occurring and provide graphical or exploratory evidence to complement your\\nargument.\\n2', 'Simple Linear Regression', ' Hypothesis Testing', ' t test'], ['lr_gy_hw5_p5', 'Consider the standardized regression model\\nY \\x03\\ni = \\x0c\\x03\\n1x\\x03\\ni1 + \\x0c\\x03\\n2x\\x03\\ni2 + \\x0f\\x03\\ni :\\nThe variables Y \\x03\\ni ; x\\x03i\\n1 and x\\x03\\n21\\x03 are standardized versions of Yi; xi1 and xi2', ' i.e.', '\\nY \\x03\\ni =\\n1\\np\\nn \\U00100000 1\\n\\x12\\nYi \\U00100000 \\x16 Y\\nsY\\n\\x13\\n; x\\x03\\ni1 =\\n1\\np\\nn \\U00100000 1\\n\\x12\\nxi1 \\U00100000 \\x16x1\\ns1\\n\\x13\\n; x\\x03\\ni2 =\\n1\\np\\nn \\U00100000 1\\n\\x12\\nxi2 \\U00100000 \\x16x1\\ns2\\n\\x13\\nTask: Using the least squares equation', ' derive the estimators\\n^ \\x0c\\x03\\n1 =\\nrY 1 \\U00100000 r12rY 2\\n1 \\U00100000 r2\\n12\\nand ^ \\x0c\\x03\\n2 =\\nrY 2 \\U00100000 r12rY 1\\n1 \\U00100000 r2\\n12\\n:\\nConsider the linear regression model\\nYi = \\x0c0 + \\x0c1xi1 + \\x0c2xi2 + \\x0fi; i = 1; 2; : : : ; n; \\x0fi\\niid \\x18 N(0; \\x1b2):\\nIt is easy to show that the parameters \\x0c\\x03\\n1 ; \\x0c\\x03\\n2 in the standardized regression model and the\\noriginal parameters \\x0c0; \\x0c1; \\x0c2 are related as follows:\\n\\x0c1 =\\nsY\\ns1\\n\\x0c\\x03\\n1 ; \\x0c2 =\\nsY\\ns2\\n\\x0c\\x03\\n2 ; and \\x0c0 = \\x16 Y \\U00100000 \\x0c1\\x16x1 \\U00100000 \\x0c2\\x16x2:\\nReference pages 273 to 278 of the textbook for further details.\\n3', 'Generalized Linear Regression', ' standardization'], ['lr_hw_hw7_2016_p1', '(Problem 9.10 in KNN) A personnel o\\x0ecer in a governmental agency administered four newly\\ndeveloped aptitude tests to each of 25 applicants for entry-level clerical positions in the agency.\\nFor purpose of the study', ' all 25 applicants were accepted for positions irrespective of their\\ntest scores. After a probationary period', ' each applicant was rated for pro\\x0cciency on the job.\\nThe scores on the four tests (X1;X2;X3;X4) and the job pro\\x0cciency score (Y ) for the 25\\napplicants were as given in the data \\x0cle job_proficiency.txt', ' available in the CourseWorks\\nData folder.\\n(a) Prepare a set of adjacent box plots for the test scores of the four newly developed aptitude\\ntests (that is', ' all four box plots on one set of axes). Are there any noteworthy features\\nin these plots? Comment.\\n(b) Obtain the scatterplot matrix for response and predictor variables combined. What\\ndo the scatterplots suggest abut the nature of the functional relationship between the\\nresponse variable Y and each of the predictor variables? Are any serious multicollinearity\\nproblems evident? Explain.\\n(c) Fit the multiple regression function containing all four predictor variables as \\x0crst-order\\nterms. Does it appear that all predictor variables should be retained? Explain.', 'Simple Linear Regression', ' Diagnostic', ' residual plots', ' Multiple Linear Regression'], ['lr_hw_hw7_2016_p2', '(Problem 9.18 in KNN) Continue with the Job pro\\x0cciency data from the previous exercise.\\n(a) Run the backward elimination algorithm using AIC as the model selection criterion.\\nWhat mean function is selected by the algorithm?\\n(b) Run a forward selection routine', ' again choosing a \\\\best\" model based on AIC. What\\nmean function does this algorithm suggest?\\n(c) Repeat parts (a) and (b) using the Bayesian criterion SBC instead of AIC. Does your\\nconclusion about the \\\\best\" mean function change? Explain.', 'Remedy', ' variable selection'], ['lr_hw_hw7_2016_p3', '(Project 9.25 in KNN) Consider again the SENIC Project data', ' this time regressing Y =\\n(Length of stay)\\U001000001 on the predictor variables\\nVariable Description\\nX1 Average age of patients\\nX2 Infection risk\\nX3 log(Routine culturing ratio)\\nX4 Routine chest X-ray ratio\\nX5 log(Number of beds)\\nX6 log(Average daily census)\\nX7 log(Number of nurses)\\nX8 Available facilities and services\\n(a) Use the Box-Cox methodology to justify the choice of response and predictor transfor-\\nmations suggested in the variable de\\x0cnitions above.\\n(b) Examine (but do not submit) a scatterplot matrix of response and all predictors. Which\\nbatch of predictor variables are most highly correlated?\\n(c) Run a forward selection algorithm', ' starting with the intercept-only model', ' and taking all\\neight predictor variables as the scope. Which variables are included in the mean function\\nsuggested by the forward selection algorithm?\\n(d) Run the backward elimination routine on the mean function that includes all eight predic-\\ntor variables. Does this algorithm choose the same mean function as forward selection?\\n(e) Present and interpret a \\x0cnal \\x0ctted model of your choosing', ' that you feel provides the best\\navailable aid to our understanding of how the average length of hospital stay is related\\nto the hospital characteristics encompassed by the variables in the SENIC dataset.', 'Diagnostic', ' Box-Cox procedure', ' variable selection'], ['lr_hw_hw8_2016_p1', '(Problem 6.15 in KNN) Continue with the Patient satisfaction data from an earlier homework', '\\nwhere the response variable Y is patient satisfaction', \" and the predictors are patient's age in\\nyears (X1) and indexes of illness severity and anxiety level (X2 and X3\", ' respectively).\\n(a) Consider the regression model where Y jX = x is normally distributed with with mean\\nE(Y jX = x) = \\x0c0 + \\x0c1x1 + \\x0c2x2 + \\x0c3x3\\nand variance\\nVar(Y jX = x) = \\x1b2 :\\nTest for lack of \\x0ct in the linear mean function by testing the need for a quadratic term in\\neach of the three variables individually', \" as well as the \\x0ctted values (the latter is Tukey's\\ntest of additivity). What do you conclude?\\n(b) Conduct the Breusch-Pagan test for nonconstancy of the variance function. Clearly state\\nyour null and alternative hypotheses\", ' report a p-value', ' and clearly state your conclusion.', 'Simple Linear Regression', ' Hypothesis Testing', ' F lack-of-fit test'], ['lr_hw_hw8_2016_p2', '(Problem 10.11 in KNN) Continue with the Patient satisfaction data from the previous exer-\\ncise.\\n(a) Obtain the studentized deleted residuals and identify any cases whose outlier t-test gives\\na Bonferroni-adjusted p-value less than 1. What do you conclude?\\n(b) Obtain the diagonal elements of the hat matrix', ' i.e.', ' the \\\\hat-values\" hii. Do there seem\\nto be any high-leverage cases in this data set?\\n1\\n(c) Hospital management wishes to estimate the mean patient satisfaction for patients who\\nare X1 = 30 years old', ' whose index of illness severity is X2 = 58', ' and whose index of\\nanxiety level is X3 = 2:0. Might this estimate involve a \\\\hidden extrapolation\"? Answer\\nwith reference to the new case leverage value\\nhnew', \"new = x0\\nnew(X0X)\\U001000001xnew\\n(d) Calculate Cook's distance Di for each case and prepare an index plot. Which is the most\\nin\\nuential case in the dataset according to this measure? Is this case in\\nuential because\\nof its \\\\outlierness\", '\" high leverage', ' or both?', 'Hypothesis Testing', ' simultaneous inference', ' Diagnostic', ' high leverage', ' outlier'], ['lr_hw_hw8_2016_p3', '(Project 10.27 in KNN) Continue with our analysis of the SENIC Project data. Here we take\\naverage length of stay as the response variable Y ', ' and consider three predictors: X1 = age', '\\nX2 = routine chest X-ray ratio', ' and X3 = average daily census.\\n(a) Consider the regression model where Y jX = x is normally distributed with with mean\\nE(Y jX = x) = \\x0c0 + \\x0c1x1 + \\x0c2x2 + \\x0c3x3\\nand variance\\nVar(Y jX = x) = \\x1b2 :\\nTest for lack of \\x0ct in the linear mean function by testing the need for a quadratic term in\\neach of the three variables individually', \" as well as the \\x0ctted values (the latter is Tukey's\\ntest of additivity). What do you conclude?\\n(b) Conduct the Breusch-Pagan test for nonconstancy of the variance function. Clearly state\\nyour null and alternative hypotheses\", ' report a p-value', ' and clearly state your conclusion.\\n(c) Obtain the studentized deleted residuals and identify any cases whose outlier t-test gives\\na Bonferroni-adjusted p-value less than 1. What do you conclude?\\n(d) Obtain the diagonal elements of the hat matrix', ' i.e.', ' the \\\\hat-values\" hii. Do there seem\\nto be any high-leverage cases in this data set?\\n(e) Calculate Cook\\'s distance Di for each case and prepare an index plot. Identify the two\\nmost in\\nuential cases in the dataset according to this measure. Are these cases in\\nuential\\nbecause of their \\\\outlierness', '\" high leverage', ' or both?', 'Simple Linear Regression', ' Hypothesis Test', ' F lack-of-fit test', ' simultaneous inference', ' high-leverage', ' outlier'], ['lr_hw_hw8_2016_p4', 'Repeat the previous exercise', ' but this time we take as our response variable\\nY = (Length of stay)\\U001000001\\nand consider the predictors X1 = Age', ' X2 = X-ray ratio', ' and X3 = log(Census). How do\\nyour conclusions change under these transformations?', 'Generalized Linear Regression', ' power transformation'], ['lr_hw_hw8_p1', 'Suppose that Yi = \\x0c0 + \\x0c1Xi1 + : : : + \\x0cpXip + \\x0fi', ' where \\x0fi\\niid \\x18 N(0; \\x1b2) and de\\x0cne\\nP = X(XTX)\\U001000001XT .\\n(a) Prove that P2 = P.\\n(b) Prove that Tr(P) = p + 1.\\n(c) Use part (a) to prove that 0 \\x14 Pii \\x14 1.\\n(d) Suppose that ^ Yi = ^ \\x0c0 + ^ \\x0c1Xi1 + : : : + ^ \\x0cpXip', ' where ^\\x0c\\ndenotes the estimate of the\\nOLS. Prove that\\nYi \\U00100000 ^ Yi \\x18 N(0; \\x1b2(1 \\U00100000 Pii))\\nPii is known as the leverage score for observation i.', 'Multiple Linear Regression', ' parameter', ' least square estimation'], ['lr_hw_hw8_p3', 'Consider the prostate cancer data that was introduced in HW5 and answer the following\\nquestions about that dataset. If a datapoint has F in its last column you should put\\nit in a test set. Otherwise', ' put it in your training set.\\n(a) Using visual inspection of the scatter and box-plots only propose a model for your\\ndata. Feel free to introduce new predictors if you think they are useful.\\n(b) Fit the model you proposed in the last part', ' and use residual plots to see if there\\nare still patterns you want to capture.\\n(c) Try to improve your model.\\n(d) Use forward stepwise to order the predictors and then \\x0cnd the optimal number\\nof predictors you would like to include using the estimate of the prediction error\\nbased on the test set.\\n(e) Use both ridge and LASSO to predict lpsa. Use cross validation (based on the\\nestimate of the prediction error on the test set) to optimize the parameter (\\x15) of\\nLASSO and Ridge.', 'Diagnostic', ' residual plots', ' Remedy', ' variable selection', ' Ridge Regression', ' LASSO Regression'], ['lr_hw1_2016_p1', 'What is the rank of the n × n identity matrix?', 'Multiple Linear Regrssion', 'linear algebra'], ['lr_hw1_2016_p2', 'Consider a matrix A ∈ Rn×n with Ai', 'j = 1 for every i and j. What is the rank of this matrix?', 'Linear Algebra'], ['lr_hw1_2016_p3', 'Consider a matrix A ∈ Rn×n whose rank is c < n. Let Ai denote the ith column of A', ' and suppose that the first c columns of A are linearly independent.', 'Linear Algebra'], ['lr_hw1_2016_p5', 'Let A', ' B ∈ Rn×n denote two invertible matrices. Prove that AB is an invertible matrix and its inverse is B−1A−1', 'Linear Algebra'], ['lr_hw1_2016_p6', 'Let A ∈ Rm×n and B ∈ Rn×p. Prove that rank(AB) ≤ min(rank(A)', ' rank(B))', 'Linear Algebra'], ['lr_hw1_2016_p7', 'Trace of a matrix A ∈ Rn×n is defined as the sum of its diagonal elements', 'Linear Algebra'], ['lr_hw2_2016_p1', 'Calculate the gradient of the function f(x1', 'x2', 'x3) = 2x51x32x43 +x1x2 +x2x3 at (1', '1', '1).', 'Linear Algebra'], ['lr_hw2_2016_p2', 'Let f(x1', 'x2) = (x1 +x2)4 −8(x1 +x2)2. Find all the local minimas and local maximas\\nof this function. Guess what the graph of this function looks like.', 'Linear Algebra'], ['lr_hw2_2016_p3', 'Let x ∈ R2. Consider the function f(x) = xTAx. If A is an invertible matrix', ' then prove that this function has only one stationary point at 0. ', 'Linear Algebra'], ['lr_hw2_2016_p5', 'Let x ∈ Rp and find the gradient of the following functions', 'Linear Algebra'], ['lr_hw2_2016_p6', 'Let A ∈ Rn×p denote a fat matrix', ' i.e.', ' n < p. Explain why we should expect the equation y = Ax to have infinitely many solutions', 'Linear Algebra'], ['lr_hw4_2016_p2', 'Suppose that the joint pdf of T1', 'T2', 'T3 is equal to one on the cube [0', '1]×[0', '1]×[0', '1] (and clearly zero elsewhere). Calculate the probability of', 'probability distribution'], ['lr_hw4_2016_p3', 'Suppose that T1', 'T2', '...', 'Tn ∼ N(μ', 'σ2) are iid Gaussian random variables. DefineT ̄ = (T1 + T2 + . . . + Tn)/n. Prove that \\U0010fc13ni=1(Ti − T ̄)2 is independent of T ̄', 'probability distribution'], ['lr_hw4_2016_p4', 'Suppose that T1', ' T2', ' . . . ', ' Tl are jointly Gaussian random variables. Prove that they are indepednent if and only if the covariance matrix is diagonal', 'Linear Algebra', 'probability distribution'], ['lr_hw4_2016_p5', 'Consider the linear regression model Yi = β0 + β1Xi1 + β2Xi2 + . . . + βpXip + εi for iid 2\\ni = 1', '2', '...', 'n', 'Multiple Linear Regrssion', ' probability distribution', ' linear algebra'], ['lr_hw4_2016_p6', 'Suppose that we have X1', 'X2', '...', 'Xn ∼ N(μ', 'Σ)', ' where Σ is a known matrix. Find\\nthe maximum likelihood estimate of μ.', 'maximum likelihood estimator'], ['lr_hw4_2016_p8', 'In this problem we want to play with hedonic pricing model we discussed in the class. Download the dataset from the website. Explain all the predictors you have in this dataset. Answer the following questions regarding this dataset.\\n', 'simple linear regression', 'hypothesis testing', ' parameter estimation', 'prediction'], ['lr_hw5_2018_p1', 'Consider a n-dimensional random variance X with mean covariance matrix Σ. Let A be a k×n matrix. Show that the covariance matrix of Y = AX is AΣA⊤.', 'Linear Algebra'], ['lr_hw5_2018_p2', 'Considerlinearmodely=β0+β1x1+...+βpxp+εandε∼N(0', 'σ)andhypotheses H0 :β1 =0 H1 :β1 ̸=0.\\nShow that t-test and F -test are equivalent in the sense that the T 2 = F where T is the t-statistic and F is the F-statistic.', 'Multiple Linear Regrssion', 'hypothesis testing'], ['lr_hw5_2016_p1', 'Use the definition of the conditional pdf which is', 'probability distribution'], ['lr_hw5_2016_p2', 'Consider a linear regression model Yi = β0 +β1X1i +. . .+βpXip +εi under the following assumptions: Find the maximum likelihood estimate of β and σ2. Now I slightly change the assump-\\ntions of the model to:', 'maximum likelihood estimation', 'multiple linear regression'], ['lr_hw5_2016_p3', 'Consider a linear regression model Yi = β0 +β1X1i +. . .+βpXip +εi under the following assumptions: \\nFind the MLE of β. Your answer should have an explicit form.', 'Multiple Linear Regrssion', 'maximum likelihood estimation'], ['lr_hw5_2016_p4', 'In one of the notes we will show that \\U0010fc13i=1(Ti −T )2/σ2 has a χ2 distribution with n−1 degrees of freedom. Characterize the distribution of', 'probability distribution'], ['lr_hw5_2016_p5', '(Performance of OLS for non-identically distributed noises) (a) βˆMLE under the fixed X assumption.\\n(b) βˆOLS under the fixed X assumption.', 'Multiple Linear Regrssion', 'least square estimation', ' maximum likelihood estimation'], ['lr_hw5_2016_p6', 'Consider a linear regression model Yi = β0 +β1X1i +. . .+βpXip +εi under the following assumptions: εi∼N(0', 'σ)', 'whereσ isnotknown.\\nT iid\\n(b) If xi = [Xi1', 'Xi2', '...', 'Xip] ', 'then xi ∼ N(0', 'Σ)', ' where Σ is a known covariance\\nmatrix.Calculate E(y − yˆ)T (y − yˆ). After you do this problem', ' compare it with Problem 2. Again we will later prove that if we divide (y − yˆ)T (y − yˆ) by σ2 it will have a χ2 distribution. Use your calculations to figure out the degrees of freedom of this χ2 random variable.', 'Multiple Linear Regrssion，probability distribution'], ['lr_hw5_2016_p7', 'In this problem we would like to compare different linear regression schemes for a prostate cancer dataset. You can download the dataset and its explanation from the course webpage. Our comparison strategy among different approaches is the following. We will explain it more formally later in the course. Check the last column of the dataset. Based on this column break your dataset into two pieces. The first dataset has all the patients whose last columns are equal to T . The second dataset includes the rest of the patients. lpsa denotes the response in this dataset and we want to predict it based on the features (predictors we have).', 'simple linear regression', ' multiple linear regression', ' parameter estimation', ' residual'], ['lr_hw6_1', 'An assistant in the district sales office of a national cosmetics firm obtained data on advertis- ing expenditures and sales last year in the district’s 44 territories. X1 denotes expenditures for point-of-sale displays in beauty salons and department stores (in thousand dollars)', ' and X2 and X3 represent the corresponding expenditures for local media advertising and prorated share of national media advertising', ' respectively. Let Y denote sales (in thou- sand cases). The assistant was instructed to estimate the increase in expected sales when X1 is increased by 1 thousand dollars and X2 and X3 are held constant', ' and was told to use the most appropriate model based on the covariance structure of explanatory variables X1', 'X2', 'X3. The increase in expected sales should be estimated with its uncertainty', ' i.e.', ' construct the estimate with its 95% confidence interval. The data set CosmeticsSales.txt is posted on Canvas.', 'diagnostic/remedy', ' collinearity', 'variance', 'ridge regression', ' bootstrap', 'hypothesis testing'], ['lr_hw6_2', 'Consider the data set y1', 'y2', '...', 'yn. Using the absolute value loss function (ψ(u) = |u|)', ' minimize the objective function\\n\\n', 'objective function'], ['lr_hw6_4', 'A local health clinic sent fliers to its clients to encourage everyone', ' but especially older persons at high risk of complications', ' to get a flu shot in time for protection against an expected flu epidemic. In a pilot follow up study', ' 159 clients were randomly selected and asked whether they actually received a flu shot. A client who received a flu shot was coded Y = 1 and a client who did not receive a flu shot was coded Y = 0. In addition. data were collected on their age (X1) and their health awareness. The latter data were combined into a health awareness index (X2)', ' for which higher values indicate greater awareness. Also included in the data was client gender', ' where males were coded X3 = 1 and females were coded X3 = 0. The data set FluShots.txt is Posted on Canvas.\\nUse R to perform the following tasks:\\ni. Find the maximum likelihood estimators of β0', ' β1', ' β2', ' β3. State the fitted response\\nfunction. Obtain exp (βˆ1)', ' exp (βˆ2)', ' exp (βˆ3). Interpret these numbers What is the estimated probability that male clients aged 55 with a health awareness index of 60 will receive a flu shot? Estimate this probability with 95% confidence.\\nR code and output Construct a ROC plot for the above model. Interpret the ROC plot.\\n', 'logstic regression', ' maximum likelihood estimation', ' parameter estimation', ' confidence interval', ' diagnostic', ' ROC']]\n"
     ]
    }
   ],
   "source": [
    "train=pd.DataFrame({'content':train_df})\n",
    "train.head()\n",
    "train_content=[row.split(',') for row in train['content']]\n",
    "print(train_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_df=pd.read_excel(\"Linear regression concept.xlsx\",header=None)\n",
    "concept_df.shape\n",
    "concept_df[0].iloc[1:8]=\"Model\"\n",
    "concept_df[0].iloc[9:16]=\"Hypothesis Testing\"\n",
    "concept_df[0].iloc[17:19]=\"Prediction\"\n",
    "concept_df[0].iloc[20]=\"Inference\"\n",
    "concept_df[0].iloc[21:35]=\"Diagnostic / remedy\"\n",
    "concept_df[0]\n",
    "concept_df[2].iloc[1:3]=\"parameter\"\n",
    "concept_df[2].iloc[4]=\"prediction of new y value\"\n",
    "concept_df[2].iloc[6]=\"residual\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0                           1\n",
      "0  Model    Simple Linear Regression\n",
      "1  Model  Multiple Linear Regression\n",
      "2  Model            Ridge Regression\n",
      "3  Model            Lasso Regression\n",
      "4  Model         Logistic Regression\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(38, 2)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concept_df2=concept_df[[2,3]].dropna()\n",
    "concept_df2.rename(columns={2:0,3:1},inplace=True)\n",
    "concept_df1=concept_df[[0,1]].dropna()\n",
    "concept=pd.concat([concept_df1,concept_df2]).dropna()\n",
    "print(concept.head())\n",
    "concept.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 1232.46it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "List=[]\n",
    "topic=concept_df[0].unique()\n",
    "for i in tqdm(topic):\n",
    "    temp=concept[concept[0]==i][1].tolist()\n",
    "    List.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43, 170)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concept_model=Word2Vec(window=5,sg=1,min_count=1)\n",
    "concept_model.build_vocab(List)\n",
    "concept_model.train(List,total_examples=concept_model.corpus_count,epochs=concept_model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.4025048e-03 -2.2540598e-03  2.3557481e-03 ... -1.2726001e-03\n",
      "   1.5688052e-05  3.4361992e-03]\n",
      " [ 1.5865153e-03 -3.4530819e-03 -4.3401797e-03 ... -1.6445327e-03\n",
      "   2.4168617e-03  4.9953000e-03]\n",
      " [-4.0297955e-03  1.5541953e-03  2.6424706e-03 ...  1.3959374e-03\n",
      "   3.5221723e-03  4.0638605e-03]\n",
      " ...\n",
      " [-1.7758177e-03 -3.4120893e-03  1.7477464e-03 ... -3.8212170e-03\n",
      "   2.6864139e-03 -4.6649599e-03]\n",
      " [ 2.5979441e-03  8.2017406e-04  3.3394902e-03 ...  5.1992986e-04\n",
      "  -1.2254182e-04 -3.8881542e-04]\n",
      " [-4.5262487e-03 -1.6414494e-03  6.9838844e-04 ...  3.5164380e-03\n",
      "   9.1779261e-04 -6.6567305e-04]]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"word 'walk' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-172-55052d309050>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mConcept_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConcept_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mconcept_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilar_by_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"walk\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1459\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m                 )\n\u001b[0;32m-> 1461\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1463\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36msimilar_by_vector\u001b[0;34m(self, vector, topn, restrict_vocab)\u001b[0m\n\u001b[1;32m   1418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1419\u001b[0m         \"\"\"\n\u001b[0;32m-> 1420\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilar_by_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method will be removed in 4.0.0, use self.wv.doesnt_match() instead\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36msimilar_by_vector\u001b[0;34m(self, vector, topn, restrict_vocab)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m         \"\"\"\n\u001b[0;32m--> 622\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m     @deprecated(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'walk' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "Concept_vec=concept_model[concept_model.wv.vocab]\n",
    "Concept_vec.shape\n",
    "print(Concept_vec)\n",
    "concept_model.similar_by_vector(\"walk\",topn=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=concept_model.wv.vocab\n",
    "vocab_list=list((vocab.keys()))\n",
    "vocab_list\n",
    "\n",
    "def similar_concept(v,n=6):\n",
    "    \n",
    "    ms=concept_model.similar_by_vector(v,topn=n+1)\n",
    "    \n",
    "    return ms\n",
    "    \n",
    "\n",
    "\n",
    "Similar_list=[]\n",
    "\n",
    "\n",
    "for i in vocab_list:\n",
    "    \n",
    "    temp=similar_concept(i)\n",
    "    \n",
    "    Similar_list.append(temp)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>concept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>(ANOVA, 0.1929926574230194)</td>\n",
       "      <td>(linearity, 0.15306922793388367)</td>\n",
       "      <td>(collinearity, 0.12341587990522385)</td>\n",
       "      <td>(residual plots, 0.10542687773704529)</td>\n",
       "      <td>(Higher Order Regression , 0.08797528594732285)</td>\n",
       "      <td>(F test, 0.07801644504070282)</td>\n",
       "      <td>(variable selection, 0.06102385371923447)</td>\n",
       "      <td>Simple Linear Regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>(collinearity, 0.2468145340681076)</td>\n",
       "      <td>(residual plots, 0.13267181813716888)</td>\n",
       "      <td>(Z test, 0.10875286161899567)</td>\n",
       "      <td>(confidence interval, 0.09699143469333649)</td>\n",
       "      <td>(Ridge Regression, 0.0905735045671463)</td>\n",
       "      <td>(likelihood ratio test, 0.09032928198575974)</td>\n",
       "      <td>(receiver operation characteristic curve/ROC, ...</td>\n",
       "      <td>Multiple Linear Regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>(general linear test, 0.19466166198253632)</td>\n",
       "      <td>(r square, 0.18467141687870026)</td>\n",
       "      <td>(true average, 0.1726243942975998)</td>\n",
       "      <td>(F test, 0.14374686777591705)</td>\n",
       "      <td>(Logistic Regression, 0.11132673919200897)</td>\n",
       "      <td>(high-leverage, 0.10124050080776215)</td>\n",
       "      <td>(model validation, 0.0952289029955864)</td>\n",
       "      <td>Ridge Regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>(constant variance, 0.33102941513061523)</td>\n",
       "      <td>(prediction interval, 0.2437368631362915)</td>\n",
       "      <td>(Box-Cox procedure, 0.15155375003814697)</td>\n",
       "      <td>(prediction error, 0.11475978046655655)</td>\n",
       "      <td>(student t test, 0.09872320294380188)</td>\n",
       "      <td>(ANOVA, 0.09861032664775848)</td>\n",
       "      <td>(normality, 0.09558409452438354)</td>\n",
       "      <td>Lasso Regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>(receiver operation characteristic curve/ROC, ...</td>\n",
       "      <td>(simultaneous inference, 0.1678580641746521)</td>\n",
       "      <td>(general linear test, 0.14328771829605103)</td>\n",
       "      <td>(true average, 0.13062606751918793)</td>\n",
       "      <td>(Ridge Regression, 0.11132672429084778)</td>\n",
       "      <td>(high-leverage, 0.0985400527715683)</td>\n",
       "      <td>(F test, 0.09702174365520477)</td>\n",
       "      <td>Logistic Regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>(general linear test, 0.2670595049858093)</td>\n",
       "      <td>(likelihood ratio test, 0.23341086506843567)</td>\n",
       "      <td>(independence, 0.20995411276817322)</td>\n",
       "      <td>(Box-Cox procedure, 0.19310052692890167)</td>\n",
       "      <td>(constant variance, 0.16567379236221313)</td>\n",
       "      <td>(confidence interval, 0.16311213374137878)</td>\n",
       "      <td>(Z test, 0.1357373297214508)</td>\n",
       "      <td>Generalized Linear Regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>(ANOVA, 0.14778073132038116)</td>\n",
       "      <td>(standard error, 0.1460542380809784)</td>\n",
       "      <td>(linearity, 0.14512476325035095)</td>\n",
       "      <td>(Generalized Linear Regression, 0.110866203904...</td>\n",
       "      <td>(confidence interval, 0.10585040599107742)</td>\n",
       "      <td>(Z test, 0.1052543893456459)</td>\n",
       "      <td>(receiver operation characteristic curve/ROC, ...</td>\n",
       "      <td>Higher Order Regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>(Box-Cox procedure, 0.2392181158065796)</td>\n",
       "      <td>(Generalized Linear Regression, 0.111858606338...</td>\n",
       "      <td>(general linear test, 0.1044408455491066)</td>\n",
       "      <td>(Lasso Regression, 0.09872320294380188)</td>\n",
       "      <td>(Higher Order Regression , 0.08679993450641632)</td>\n",
       "      <td>(ANOVA, 0.08259317278862)</td>\n",
       "      <td>(constant variance, 0.05541742593050003)</td>\n",
       "      <td>student t test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>(model validation, 0.29176560044288635)</td>\n",
       "      <td>(independence, 0.21599829196929932)</td>\n",
       "      <td>(ANOVA, 0.15171125531196594)</td>\n",
       "      <td>(Ridge Regression, 0.14374685287475586)</td>\n",
       "      <td>(Generalized Linear Regression, 0.124476894736...</td>\n",
       "      <td>(r square, 0.10805027931928635)</td>\n",
       "      <td>(Logistic Regression, 0.09702174365520477)</td>\n",
       "      <td>F test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>(standard error, 0.28676021099090576)</td>\n",
       "      <td>(linearity, 0.14630046486854553)</td>\n",
       "      <td>(Generalized Linear Regression, 0.135737329721...</td>\n",
       "      <td>(Multiple Linear Regression, 0.10875286161899567)</td>\n",
       "      <td>(Higher Order Regression , 0.1052543893456459)</td>\n",
       "      <td>(Box-Cox procedure, 0.08991564810276031)</td>\n",
       "      <td>(model validation, 0.0524858757853508)</td>\n",
       "      <td>Z test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>(normality, 0.2877519130706787)</td>\n",
       "      <td>(Generalized Linear Regression, 0.233410835266...</td>\n",
       "      <td>(collinearity, 0.09174738079309464)</td>\n",
       "      <td>(Multiple Linear Regression, 0.09032925963401794)</td>\n",
       "      <td>(constant variance, 0.08030059933662415)</td>\n",
       "      <td>(ANOVA, 0.07910704612731934)</td>\n",
       "      <td>(prediction error, 0.07661528140306473)</td>\n",
       "      <td>likelihood ratio test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>(Generalized Linear Regression, 0.267059504985...</td>\n",
       "      <td>(Ridge Regression, 0.19466164708137512)</td>\n",
       "      <td>(Logistic Regression, 0.14328770339488983)</td>\n",
       "      <td>(confidence interval, 0.13587045669555664)</td>\n",
       "      <td>(receiver operation characteristic curve/ROC, ...</td>\n",
       "      <td>(student t test, 0.1044408306479454)</td>\n",
       "      <td>(independence, 0.09386114031076431)</td>\n",
       "      <td>general linear test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>(Simple Linear Regression, 0.1929926574230194)</td>\n",
       "      <td>(F test, 0.15171125531196594)</td>\n",
       "      <td>(Higher Order Regression , 0.14778073132038116)</td>\n",
       "      <td>(high-leverage, 0.12075365334749222)</td>\n",
       "      <td>(Lasso Regression, 0.09861032664775848)</td>\n",
       "      <td>(constant variance, 0.09756597876548767)</td>\n",
       "      <td>(student t test, 0.08259317278862)</td>\n",
       "      <td>ANOVA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>(independence, 0.23470014333724976)</td>\n",
       "      <td>(r square, 0.11927082389593124)</td>\n",
       "      <td>(constant variance, 0.059415511786937714)</td>\n",
       "      <td>(prediction error, 0.04834107682108879)</td>\n",
       "      <td>(Generalized Linear Regression, 0.044774331152...</td>\n",
       "      <td>(Ridge Regression, 0.04376726970076561)</td>\n",
       "      <td>(confidence interval, 0.03716166689991951)</td>\n",
       "      <td>F lack-of-fit test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>(model validation, 0.2316126823425293)</td>\n",
       "      <td>(receiver operation characteristic curve/ROC, ...</td>\n",
       "      <td>(Logistic Regression, 0.1678580790758133)</td>\n",
       "      <td>(Lasso Regression, 0.07758918404579163)</td>\n",
       "      <td>(general linear test, 0.07183746993541718)</td>\n",
       "      <td>(Ridge Regression, 0.07120214402675629)</td>\n",
       "      <td>(r square, 0.06716988980770111)</td>\n",
       "      <td>simultaneous inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>(outlier, 0.20237258076667786)</td>\n",
       "      <td>(receiver operation characteristic curve/ROC, ...</td>\n",
       "      <td>(high-leverage, 0.2007388174533844)</td>\n",
       "      <td>(true average, 0.15606136620044708)</td>\n",
       "      <td>(Box-Cox procedure, 0.1492161750793457)</td>\n",
       "      <td>(model validation, 0.13980358839035034)</td>\n",
       "      <td>(Generalized Linear Regression, 0.126735344529...</td>\n",
       "      <td>prediction error</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>(Lasso Regression, 0.2437368482351303)</td>\n",
       "      <td>(normality, 0.13545221090316772)</td>\n",
       "      <td>(confidence interval, 0.12746399641036987)</td>\n",
       "      <td>(true average, 0.06308300793170929)</td>\n",
       "      <td>(residual plots, 0.056969933211803436)</td>\n",
       "      <td>(likelihood ratio test, 0.04898994415998459)</td>\n",
       "      <td>(Z test, 0.04443206638097763)</td>\n",
       "      <td>prediction interval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>(high-leverage, 0.25019049644470215)</td>\n",
       "      <td>(residual plots, 0.17820537090301514)</td>\n",
       "      <td>(Ridge Regression, 0.1726243942975998)</td>\n",
       "      <td>(prediction error, 0.1560613512992859)</td>\n",
       "      <td>(independence, 0.14195162057876587)</td>\n",
       "      <td>(Logistic Regression, 0.13062606751918793)</td>\n",
       "      <td>(Generalized Linear Regression, 0.082611486315...</td>\n",
       "      <td>true average</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>(Z test, 0.28676021099090576)</td>\n",
       "      <td>(model validation, 0.15441271662712097)</td>\n",
       "      <td>(Higher Order Regression , 0.1460542380809784)</td>\n",
       "      <td>(variable selection, 0.1404261440038681)</td>\n",
       "      <td>(Generalized Linear Regression, 0.079941794276...</td>\n",
       "      <td>(Ridge Regression, 0.06272849440574646)</td>\n",
       "      <td>(Box-Cox procedure, 0.059215247631073)</td>\n",
       "      <td>standard error</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>(Generalized Linear Regression, 0.163112133741...</td>\n",
       "      <td>(general linear test, 0.13587044179439545)</td>\n",
       "      <td>(prediction interval, 0.12746399641036987)</td>\n",
       "      <td>(Higher Order Regression , 0.10585040599107742)</td>\n",
       "      <td>(Multiple Linear Regression, 0.09699143469333649)</td>\n",
       "      <td>(Ridge Regression, 0.07629281282424927)</td>\n",
       "      <td>(true average, 0.07297926396131516)</td>\n",
       "      <td>confidence interval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>(Simple Linear Regression, 0.15306922793388367)</td>\n",
       "      <td>(Z test, 0.14630046486854553)</td>\n",
       "      <td>(Higher Order Regression , 0.14512474834918976)</td>\n",
       "      <td>(residual plots, 0.10823766142129898)</td>\n",
       "      <td>(high-leverage, 0.07973775267601013)</td>\n",
       "      <td>(outlier, 0.07475534826517105)</td>\n",
       "      <td>(simultaneous inference, 0.06421341001987457)</td>\n",
       "      <td>linearity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>(Lasso Regression, 0.33102941513061523)</td>\n",
       "      <td>(variable selection, 0.22594746947288513)</td>\n",
       "      <td>(r square, 0.21703097224235535)</td>\n",
       "      <td>(Generalized Linear Regression, 0.165673792362...</td>\n",
       "      <td>(outlier, 0.14924119412899017)</td>\n",
       "      <td>(ANOVA, 0.09756597876548767)</td>\n",
       "      <td>(Higher Order Regression , 0.08847250044345856)</td>\n",
       "      <td>constant variance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>(likelihood ratio test, 0.2877519130706787)</td>\n",
       "      <td>(model validation, 0.13944272696971893)</td>\n",
       "      <td>(prediction interval, 0.13545222580432892)</td>\n",
       "      <td>(Lasso Regression, 0.09558410197496414)</td>\n",
       "      <td>(Logistic Regression, 0.07908844947814941)</td>\n",
       "      <td>(Ridge Regression, 0.06336729228496552)</td>\n",
       "      <td>(ANOVA, 0.037731386721134186)</td>\n",
       "      <td>normality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>(F lack-of-fit test, 0.23470014333724976)</td>\n",
       "      <td>(F test, 0.21599829196929932)</td>\n",
       "      <td>(Generalized Linear Regression, 0.209954112768...</td>\n",
       "      <td>(true average, 0.14195162057876587)</td>\n",
       "      <td>(general linear test, 0.09386114776134491)</td>\n",
       "      <td>(ANOVA, 0.07463632524013519)</td>\n",
       "      <td>(variable selection, 0.05348234996199608)</td>\n",
       "      <td>independence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>(F test, 0.29176560044288635)</td>\n",
       "      <td>(simultaneous inference, 0.2316126823425293)</td>\n",
       "      <td>(standard error, 0.15441271662712097)</td>\n",
       "      <td>(prediction error, 0.13980358839035034)</td>\n",
       "      <td>(normality, 0.13944272696971893)</td>\n",
       "      <td>(Ridge Regression, 0.09522891044616699)</td>\n",
       "      <td>(r square, 0.07704226672649384)</td>\n",
       "      <td>model validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>(outlier, 0.2671136260032654)</td>\n",
       "      <td>(true average, 0.25019049644470215)</td>\n",
       "      <td>(prediction error, 0.2007388174533844)</td>\n",
       "      <td>(ANOVA, 0.12075365334749222)</td>\n",
       "      <td>(Ridge Regression, 0.10124050825834274)</td>\n",
       "      <td>(Logistic Regression, 0.0985400527715683)</td>\n",
       "      <td>(linearity, 0.07973775267601013)</td>\n",
       "      <td>high-leverage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>(high-leverage, 0.2671136260032654)</td>\n",
       "      <td>(prediction error, 0.20237258076667786)</td>\n",
       "      <td>(constant variance, 0.14924119412899017)</td>\n",
       "      <td>(Higher Order Regression , 0.07528427243232727)</td>\n",
       "      <td>(linearity, 0.07475534826517105)</td>\n",
       "      <td>(Generalized Linear Regression, 0.063147053122...</td>\n",
       "      <td>(F test, 0.05369115620851517)</td>\n",
       "      <td>outlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>(Multiple Linear Regression, 0.2468145340681076)</td>\n",
       "      <td>(Simple Linear Regression, 0.12341585010290146)</td>\n",
       "      <td>(likelihood ratio test, 0.09174737334251404)</td>\n",
       "      <td>(Higher Order Regression , 0.07025565952062607)</td>\n",
       "      <td>(Ridge Regression, 0.051177166402339935)</td>\n",
       "      <td>(independence, 0.021382804960012436)</td>\n",
       "      <td>(student t test, 0.008807018399238586)</td>\n",
       "      <td>collinearity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>(constant variance, 0.22594746947288513)</td>\n",
       "      <td>(standard error, 0.1404261440038681)</td>\n",
       "      <td>(Higher Order Regression , 0.086030513048172)</td>\n",
       "      <td>(likelihood ratio test, 0.06222289428114891)</td>\n",
       "      <td>(Simple Linear Regression, 0.06102385371923447)</td>\n",
       "      <td>(Generalized Linear Regression, 0.059919781982...</td>\n",
       "      <td>(model validation, 0.05837271362543106)</td>\n",
       "      <td>variable selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>(Logistic Regression, 0.24240829050540924)</td>\n",
       "      <td>(prediction error, 0.2017320990562439)</td>\n",
       "      <td>(r square, 0.17102490365505219)</td>\n",
       "      <td>(simultaneous inference, 0.17048360407352448)</td>\n",
       "      <td>(general linear test, 0.12176483124494553)</td>\n",
       "      <td>(Higher Order Regression , 0.09857264161109924)</td>\n",
       "      <td>(Box-Cox procedure, 0.0953662097454071)</td>\n",
       "      <td>receiver operation characteristic curve/ROC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>(true average, 0.17820537090301514)</td>\n",
       "      <td>(Multiple Linear Regression, 0.13267181813716888)</td>\n",
       "      <td>(linearity, 0.10823766887187958)</td>\n",
       "      <td>(Simple Linear Regression, 0.10542687773704529)</td>\n",
       "      <td>(r square, 0.08450035750865936)</td>\n",
       "      <td>(Logistic Regression, 0.07368634641170502)</td>\n",
       "      <td>(prediction interval, 0.05696992576122284)</td>\n",
       "      <td>residual plots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>(student t test, 0.2392181158065796)</td>\n",
       "      <td>(Generalized Linear Regression, 0.193100526928...</td>\n",
       "      <td>(Lasso Regression, 0.15155375003814697)</td>\n",
       "      <td>(prediction error, 0.1492161750793457)</td>\n",
       "      <td>(receiver operation characteristic curve/ROC, ...</td>\n",
       "      <td>(Z test, 0.08991565555334091)</td>\n",
       "      <td>(r square, 0.08827303349971771)</td>\n",
       "      <td>Box-Cox procedure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>(constant variance, 0.21703097224235535)</td>\n",
       "      <td>(Ridge Regression, 0.18467141687870026)</td>\n",
       "      <td>(receiver operation characteristic curve/ROC, ...</td>\n",
       "      <td>(F lack-of-fit test, 0.11927082389593124)</td>\n",
       "      <td>(F test, 0.10805027931928635)</td>\n",
       "      <td>(Lasso Regression, 0.09194591641426086)</td>\n",
       "      <td>(Box-Cox procedure, 0.08827303349971771)</td>\n",
       "      <td>r square</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0  \\\n",
       "0                         (ANOVA, 0.1929926574230194)   \n",
       "1                  (collinearity, 0.2468145340681076)   \n",
       "2          (general linear test, 0.19466166198253632)   \n",
       "3            (constant variance, 0.33102941513061523)   \n",
       "4   (receiver operation characteristic curve/ROC, ...   \n",
       "5           (general linear test, 0.2670595049858093)   \n",
       "6                        (ANOVA, 0.14778073132038116)   \n",
       "7             (Box-Cox procedure, 0.2392181158065796)   \n",
       "8             (model validation, 0.29176560044288635)   \n",
       "9               (standard error, 0.28676021099090576)   \n",
       "10                    (normality, 0.2877519130706787)   \n",
       "11  (Generalized Linear Regression, 0.267059504985...   \n",
       "12     (Simple Linear Regression, 0.1929926574230194)   \n",
       "13                (independence, 0.23470014333724976)   \n",
       "14             (model validation, 0.2316126823425293)   \n",
       "15                     (outlier, 0.20237258076667786)   \n",
       "16             (Lasso Regression, 0.2437368482351303)   \n",
       "17               (high-leverage, 0.25019049644470215)   \n",
       "18                      (Z test, 0.28676021099090576)   \n",
       "19  (Generalized Linear Regression, 0.163112133741...   \n",
       "20    (Simple Linear Regression, 0.15306922793388367)   \n",
       "21            (Lasso Regression, 0.33102941513061523)   \n",
       "22        (likelihood ratio test, 0.2877519130706787)   \n",
       "23          (F lack-of-fit test, 0.23470014333724976)   \n",
       "24                      (F test, 0.29176560044288635)   \n",
       "25                      (outlier, 0.2671136260032654)   \n",
       "26                (high-leverage, 0.2671136260032654)   \n",
       "27   (Multiple Linear Regression, 0.2468145340681076)   \n",
       "28           (constant variance, 0.22594746947288513)   \n",
       "29         (Logistic Regression, 0.24240829050540924)   \n",
       "30                (true average, 0.17820537090301514)   \n",
       "31               (student t test, 0.2392181158065796)   \n",
       "32           (constant variance, 0.21703097224235535)   \n",
       "\n",
       "                                                    1  \\\n",
       "0                    (linearity, 0.15306922793388367)   \n",
       "1               (residual plots, 0.13267181813716888)   \n",
       "2                     (r square, 0.18467141687870026)   \n",
       "3           (prediction interval, 0.2437368631362915)   \n",
       "4        (simultaneous inference, 0.1678580641746521)   \n",
       "5        (likelihood ratio test, 0.23341086506843567)   \n",
       "6                (standard error, 0.1460542380809784)   \n",
       "7   (Generalized Linear Regression, 0.111858606338...   \n",
       "8                 (independence, 0.21599829196929932)   \n",
       "9                    (linearity, 0.14630046486854553)   \n",
       "10  (Generalized Linear Regression, 0.233410835266...   \n",
       "11            (Ridge Regression, 0.19466164708137512)   \n",
       "12                      (F test, 0.15171125531196594)   \n",
       "13                    (r square, 0.11927082389593124)   \n",
       "14  (receiver operation characteristic curve/ROC, ...   \n",
       "15  (receiver operation characteristic curve/ROC, ...   \n",
       "16                   (normality, 0.13545221090316772)   \n",
       "17              (residual plots, 0.17820537090301514)   \n",
       "18            (model validation, 0.15441271662712097)   \n",
       "19         (general linear test, 0.13587044179439545)   \n",
       "20                      (Z test, 0.14630046486854553)   \n",
       "21          (variable selection, 0.22594746947288513)   \n",
       "22            (model validation, 0.13944272696971893)   \n",
       "23                      (F test, 0.21599829196929932)   \n",
       "24       (simultaneous inference, 0.2316126823425293)   \n",
       "25                (true average, 0.25019049644470215)   \n",
       "26            (prediction error, 0.20237258076667786)   \n",
       "27    (Simple Linear Regression, 0.12341585010290146)   \n",
       "28               (standard error, 0.1404261440038681)   \n",
       "29             (prediction error, 0.2017320990562439)   \n",
       "30  (Multiple Linear Regression, 0.13267181813716888)   \n",
       "31  (Generalized Linear Regression, 0.193100526928...   \n",
       "32            (Ridge Regression, 0.18467141687870026)   \n",
       "\n",
       "                                                    2  \\\n",
       "0                 (collinearity, 0.12341587990522385)   \n",
       "1                       (Z test, 0.10875286161899567)   \n",
       "2                  (true average, 0.1726243942975998)   \n",
       "3            (Box-Cox procedure, 0.15155375003814697)   \n",
       "4          (general linear test, 0.14328771829605103)   \n",
       "5                 (independence, 0.20995411276817322)   \n",
       "6                    (linearity, 0.14512476325035095)   \n",
       "7           (general linear test, 0.1044408455491066)   \n",
       "8                        (ANOVA, 0.15171125531196594)   \n",
       "9   (Generalized Linear Regression, 0.135737329721...   \n",
       "10                (collinearity, 0.09174738079309464)   \n",
       "11         (Logistic Regression, 0.14328770339488983)   \n",
       "12    (Higher Order Regression , 0.14778073132038116)   \n",
       "13          (constant variance, 0.059415511786937714)   \n",
       "14          (Logistic Regression, 0.1678580790758133)   \n",
       "15                (high-leverage, 0.2007388174533844)   \n",
       "16         (confidence interval, 0.12746399641036987)   \n",
       "17             (Ridge Regression, 0.1726243942975998)   \n",
       "18     (Higher Order Regression , 0.1460542380809784)   \n",
       "19         (prediction interval, 0.12746399641036987)   \n",
       "20    (Higher Order Regression , 0.14512474834918976)   \n",
       "21                    (r square, 0.21703097224235535)   \n",
       "22         (prediction interval, 0.13545222580432892)   \n",
       "23  (Generalized Linear Regression, 0.209954112768...   \n",
       "24              (standard error, 0.15441271662712097)   \n",
       "25             (prediction error, 0.2007388174533844)   \n",
       "26           (constant variance, 0.14924119412899017)   \n",
       "27       (likelihood ratio test, 0.09174737334251404)   \n",
       "28      (Higher Order Regression , 0.086030513048172)   \n",
       "29                    (r square, 0.17102490365505219)   \n",
       "30                   (linearity, 0.10823766887187958)   \n",
       "31            (Lasso Regression, 0.15155375003814697)   \n",
       "32  (receiver operation characteristic curve/ROC, ...   \n",
       "\n",
       "                                                    3  \\\n",
       "0               (residual plots, 0.10542687773704529)   \n",
       "1          (confidence interval, 0.09699143469333649)   \n",
       "2                       (F test, 0.14374686777591705)   \n",
       "3             (prediction error, 0.11475978046655655)   \n",
       "4                 (true average, 0.13062606751918793)   \n",
       "5            (Box-Cox procedure, 0.19310052692890167)   \n",
       "6   (Generalized Linear Regression, 0.110866203904...   \n",
       "7             (Lasso Regression, 0.09872320294380188)   \n",
       "8             (Ridge Regression, 0.14374685287475586)   \n",
       "9   (Multiple Linear Regression, 0.10875286161899567)   \n",
       "10  (Multiple Linear Regression, 0.09032925963401794)   \n",
       "11         (confidence interval, 0.13587045669555664)   \n",
       "12               (high-leverage, 0.12075365334749222)   \n",
       "13            (prediction error, 0.04834107682108879)   \n",
       "14            (Lasso Regression, 0.07758918404579163)   \n",
       "15                (true average, 0.15606136620044708)   \n",
       "16                (true average, 0.06308300793170929)   \n",
       "17             (prediction error, 0.1560613512992859)   \n",
       "18           (variable selection, 0.1404261440038681)   \n",
       "19    (Higher Order Regression , 0.10585040599107742)   \n",
       "20              (residual plots, 0.10823766142129898)   \n",
       "21  (Generalized Linear Regression, 0.165673792362...   \n",
       "22            (Lasso Regression, 0.09558410197496414)   \n",
       "23                (true average, 0.14195162057876587)   \n",
       "24            (prediction error, 0.13980358839035034)   \n",
       "25                       (ANOVA, 0.12075365334749222)   \n",
       "26    (Higher Order Regression , 0.07528427243232727)   \n",
       "27    (Higher Order Regression , 0.07025565952062607)   \n",
       "28       (likelihood ratio test, 0.06222289428114891)   \n",
       "29      (simultaneous inference, 0.17048360407352448)   \n",
       "30    (Simple Linear Regression, 0.10542687773704529)   \n",
       "31             (prediction error, 0.1492161750793457)   \n",
       "32          (F lack-of-fit test, 0.11927082389593124)   \n",
       "\n",
       "                                                    4  \\\n",
       "0     (Higher Order Regression , 0.08797528594732285)   \n",
       "1              (Ridge Regression, 0.0905735045671463)   \n",
       "2          (Logistic Regression, 0.11132673919200897)   \n",
       "3               (student t test, 0.09872320294380188)   \n",
       "4             (Ridge Regression, 0.11132672429084778)   \n",
       "5            (constant variance, 0.16567379236221313)   \n",
       "6          (confidence interval, 0.10585040599107742)   \n",
       "7     (Higher Order Regression , 0.08679993450641632)   \n",
       "8   (Generalized Linear Regression, 0.124476894736...   \n",
       "9      (Higher Order Regression , 0.1052543893456459)   \n",
       "10           (constant variance, 0.08030059933662415)   \n",
       "11  (receiver operation characteristic curve/ROC, ...   \n",
       "12            (Lasso Regression, 0.09861032664775848)   \n",
       "13  (Generalized Linear Regression, 0.044774331152...   \n",
       "14         (general linear test, 0.07183746993541718)   \n",
       "15            (Box-Cox procedure, 0.1492161750793457)   \n",
       "16             (residual plots, 0.056969933211803436)   \n",
       "17                (independence, 0.14195162057876587)   \n",
       "18  (Generalized Linear Regression, 0.079941794276...   \n",
       "19  (Multiple Linear Regression, 0.09699143469333649)   \n",
       "20               (high-leverage, 0.07973775267601013)   \n",
       "21                     (outlier, 0.14924119412899017)   \n",
       "22         (Logistic Regression, 0.07908844947814941)   \n",
       "23         (general linear test, 0.09386114776134491)   \n",
       "24                   (normality, 0.13944272696971893)   \n",
       "25            (Ridge Regression, 0.10124050825834274)   \n",
       "26                   (linearity, 0.07475534826517105)   \n",
       "27           (Ridge Regression, 0.051177166402339935)   \n",
       "28    (Simple Linear Regression, 0.06102385371923447)   \n",
       "29         (general linear test, 0.12176483124494553)   \n",
       "30                    (r square, 0.08450035750865936)   \n",
       "31  (receiver operation characteristic curve/ROC, ...   \n",
       "32                      (F test, 0.10805027931928635)   \n",
       "\n",
       "                                                    5  \\\n",
       "0                       (F test, 0.07801644504070282)   \n",
       "1        (likelihood ratio test, 0.09032928198575974)   \n",
       "2                (high-leverage, 0.10124050080776215)   \n",
       "3                        (ANOVA, 0.09861032664775848)   \n",
       "4                 (high-leverage, 0.0985400527715683)   \n",
       "5          (confidence interval, 0.16311213374137878)   \n",
       "6                        (Z test, 0.1052543893456459)   \n",
       "7                           (ANOVA, 0.08259317278862)   \n",
       "8                     (r square, 0.10805027931928635)   \n",
       "9            (Box-Cox procedure, 0.08991564810276031)   \n",
       "10                       (ANOVA, 0.07910704612731934)   \n",
       "11               (student t test, 0.1044408306479454)   \n",
       "12           (constant variance, 0.09756597876548767)   \n",
       "13            (Ridge Regression, 0.04376726970076561)   \n",
       "14            (Ridge Regression, 0.07120214402675629)   \n",
       "15            (model validation, 0.13980358839035034)   \n",
       "16       (likelihood ratio test, 0.04898994415998459)   \n",
       "17         (Logistic Regression, 0.13062606751918793)   \n",
       "18            (Ridge Regression, 0.06272849440574646)   \n",
       "19            (Ridge Regression, 0.07629281282424927)   \n",
       "20                     (outlier, 0.07475534826517105)   \n",
       "21                       (ANOVA, 0.09756597876548767)   \n",
       "22            (Ridge Regression, 0.06336729228496552)   \n",
       "23                       (ANOVA, 0.07463632524013519)   \n",
       "24            (Ridge Regression, 0.09522891044616699)   \n",
       "25          (Logistic Regression, 0.0985400527715683)   \n",
       "26  (Generalized Linear Regression, 0.063147053122...   \n",
       "27               (independence, 0.021382804960012436)   \n",
       "28  (Generalized Linear Regression, 0.059919781982...   \n",
       "29    (Higher Order Regression , 0.09857264161109924)   \n",
       "30         (Logistic Regression, 0.07368634641170502)   \n",
       "31                      (Z test, 0.08991565555334091)   \n",
       "32            (Lasso Regression, 0.09194591641426086)   \n",
       "\n",
       "                                                    6  \\\n",
       "0           (variable selection, 0.06102385371923447)   \n",
       "1   (receiver operation characteristic curve/ROC, ...   \n",
       "2              (model validation, 0.0952289029955864)   \n",
       "3                    (normality, 0.09558409452438354)   \n",
       "4                       (F test, 0.09702174365520477)   \n",
       "5                        (Z test, 0.1357373297214508)   \n",
       "6   (receiver operation characteristic curve/ROC, ...   \n",
       "7            (constant variance, 0.05541742593050003)   \n",
       "8          (Logistic Regression, 0.09702174365520477)   \n",
       "9              (model validation, 0.0524858757853508)   \n",
       "10            (prediction error, 0.07661528140306473)   \n",
       "11                (independence, 0.09386114031076431)   \n",
       "12                 (student t test, 0.08259317278862)   \n",
       "13         (confidence interval, 0.03716166689991951)   \n",
       "14                    (r square, 0.06716988980770111)   \n",
       "15  (Generalized Linear Regression, 0.126735344529...   \n",
       "16                      (Z test, 0.04443206638097763)   \n",
       "17  (Generalized Linear Regression, 0.082611486315...   \n",
       "18             (Box-Cox procedure, 0.059215247631073)   \n",
       "19                (true average, 0.07297926396131516)   \n",
       "20      (simultaneous inference, 0.06421341001987457)   \n",
       "21    (Higher Order Regression , 0.08847250044345856)   \n",
       "22                      (ANOVA, 0.037731386721134186)   \n",
       "23          (variable selection, 0.05348234996199608)   \n",
       "24                    (r square, 0.07704226672649384)   \n",
       "25                   (linearity, 0.07973775267601013)   \n",
       "26                      (F test, 0.05369115620851517)   \n",
       "27             (student t test, 0.008807018399238586)   \n",
       "28            (model validation, 0.05837271362543106)   \n",
       "29            (Box-Cox procedure, 0.0953662097454071)   \n",
       "30         (prediction interval, 0.05696992576122284)   \n",
       "31                    (r square, 0.08827303349971771)   \n",
       "32           (Box-Cox procedure, 0.08827303349971771)   \n",
       "\n",
       "                                        concept  \n",
       "0                      Simple Linear Regression  \n",
       "1                    Multiple Linear Regression  \n",
       "2                              Ridge Regression  \n",
       "3                              Lasso Regression  \n",
       "4                           Logistic Regression  \n",
       "5                 Generalized Linear Regression  \n",
       "6                      Higher Order Regression   \n",
       "7                                student t test  \n",
       "8                                        F test  \n",
       "9                                        Z test  \n",
       "10                        likelihood ratio test  \n",
       "11                          general linear test  \n",
       "12                                        ANOVA  \n",
       "13                           F lack-of-fit test  \n",
       "14                       simultaneous inference  \n",
       "15                             prediction error  \n",
       "16                          prediction interval  \n",
       "17                                 true average  \n",
       "18                               standard error  \n",
       "19                          confidence interval  \n",
       "20                                    linearity  \n",
       "21                            constant variance  \n",
       "22                                    normality  \n",
       "23                                 independence  \n",
       "24                             model validation  \n",
       "25                                high-leverage  \n",
       "26                                      outlier  \n",
       "27                                 collinearity  \n",
       "28                           variable selection  \n",
       "29  receiver operation characteristic curve/ROC  \n",
       "30                               residual plots  \n",
       "31                            Box-Cox procedure  \n",
       "32                                     r square  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_df=pd.DataFrame(Similar_list)\n",
    "similar_df[\"concept\"]=vocab_list\n",
    "similar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "import os.path\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "agreement = 'https://'\n",
    "language = 'en'\n",
    "organization = '.wikipedia.org/w/api.php'\n",
    "\n",
    "API_URL = agreement + language + organization\n",
    "\n",
    "\n",
    "program = os.path.basename(sys.argv[0])\n",
    "logger = logging.getLogger(program)\n",
    "logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
    "\n",
    "\n",
    "def pageid(title = None, np = 0):\n",
    "    global API_URL\n",
    "    URL = API_URL\n",
    "    query_params = {\n",
    "        'action': 'query',\n",
    "        'prop': 'info',\n",
    "        'format': 'json',\n",
    "        'titles': title\n",
    "    }\n",
    "    if np != 0:\n",
    "        query_params['titles'] = 'Category:' + title\n",
    "    try:\n",
    "        r = requests.get(URL, params=query_params)\n",
    "        r.raise_for_status()\n",
    "        html, r.encoding = r.text, 'gb2312'\n",
    "    except:\n",
    "        html = \"\"\n",
    "    if html == \"\":\n",
    "        return -1\n",
    "    else:\n",
    "        try:\n",
    "            text = json.loads(html, encoding='gb2312')\n",
    "        except json.JSONDecodeError:\n",
    "            return -1\n",
    "        try:\n",
    "            for i in text[\"query\"]['pages']:\n",
    "                return int(i)\n",
    "        except:\n",
    "            return -1\n",
    "\n",
    "def summary(title = None):\n",
    "    global API_URL\n",
    "    URL = API_URL\n",
    "    query_params = {\n",
    "        'action': 'query',\n",
    "        'prop': 'extracts',\n",
    "        'explaintext': '',\n",
    "        'exintro': '',\n",
    "        'format': 'json',\n",
    "        'titles': title\n",
    "    }\n",
    "    try:\n",
    "        r = requests.get(URL, params=query_params)\n",
    "        r.raise_for_status()\n",
    "        html, r.encoding = r.text, 'gb2312'\n",
    "    except:\n",
    "        logger.error('error summary about ' + title)\n",
    "        return \"\"\n",
    "    text = json.loads(html, encoding='gb2312')\n",
    "    id = list(text[\"query\"][\"pages\"].keys())[0]\n",
    "    try:\n",
    "        return text[\"query\"][\"pages\"][id][\"extract\"]\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def body(title = None):\n",
    "    global API_URL\n",
    "    URL = API_URL\n",
    "    query_params = {\n",
    "        'action': 'query',\n",
    "        'prop': 'extracts',\n",
    "        'exlimit' : 'max',\n",
    "        'format': 'json',\n",
    "        'titles': title\n",
    "    }\n",
    "    try:\n",
    "        r = requests.get(URL, params=query_params)\n",
    "        r.raise_for_status()\n",
    "        html, r.encoding = r.text, 'gb2312'\n",
    "    except:\n",
    "        logger.error('error body about ' + title)\n",
    "        return \"\"\n",
    "    text = json.loads(html, encoding='gb2312')\n",
    "    id = list(text[\"query\"][\"pages\"].keys())[0]\n",
    "    try:\n",
    "        html_text = text[\"query\"][\"pages\"][id][\"extract\"]\n",
    "        def stripTagSimple(htmlStr):\n",
    "            dr = re.compile(r'</?\\w+[^>]*>', re.S)\n",
    "            htmlStr = re.sub(dr, '', htmlStr)\n",
    "            return htmlStr\n",
    "        html_text = stripTagSimple(html_text)\n",
    "        html_text = str(html_text).replace(\"\\n\", \"\")\n",
    "        return html_text\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def links(title = None):\n",
    "    global API_URL\n",
    "    URL = API_URL\n",
    "    query_params = {\n",
    "        'action': 'query',\n",
    "        'prop': 'links',\n",
    "        'pllimit': 'max',\n",
    "        'plnamespace': '0',\n",
    "        'format': 'json',\n",
    "        'titles': title\n",
    "    }\n",
    "    try:\n",
    "        r = requests.get(URL, params=query_params)\n",
    "        r.raise_for_status()\n",
    "        html, r.encoding = r.text, 'gb2312'\n",
    "    except:\n",
    "        logger.error('error links about ' + title)\n",
    "        return list()\n",
    "    text = json.loads(html, encoding='gb2312')\n",
    "    id = list(text[\"query\"][\"pages\"].keys())[0]\n",
    "    link = list()\n",
    "    summ = summary(title)\n",
    "    try:\n",
    "        for obj in text[\"query\"]['pages'][id][\"links\"]:\n",
    "            if obj['title'] in summ or obj['title'].lower() in summ:\n",
    "                link.append(obj['title'])\n",
    "    except:\n",
    "        return link\n",
    "    return link\n",
    "\n",
    "def linkss(title = None):\n",
    "    global API_URL\n",
    "    URL = API_URL\n",
    "    query_params = {\n",
    "        'action': 'query',\n",
    "        'prop': 'links',\n",
    "        'pllimit': 'max',\n",
    "        'plnamespace': '0',\n",
    "        'format': 'json',\n",
    "        'titles': title\n",
    "    }\n",
    "    try:\n",
    "        r = requests.get(URL, params=query_params)\n",
    "        r.raise_for_status()\n",
    "        html, r.encoding = r.text, 'gb2312'\n",
    "    except:\n",
    "        logger.error('error linkss about ' + title)\n",
    "        return list()\n",
    "    text = json.loads(html, encoding='gb2312')\n",
    "    id = list(text[\"query\"][\"pages\"].keys())[0]\n",
    "    link = list()\n",
    "    try:\n",
    "        for obj in text[\"query\"]['pages'][id][\"links\"]:\n",
    "            link.append(obj['title'])\n",
    "    except:\n",
    "        return link\n",
    "    return link\n",
    "\n",
    "def backlinks(title = None):\n",
    "    global API_URL\n",
    "    URL = API_URL\n",
    "    query_params = {\n",
    "        'action': 'query',\n",
    "        'list': 'backlinks',\n",
    "        'bllimit': 'max',\n",
    "        'blnamespace': '0',\n",
    "        'format': 'json',\n",
    "        'bltitle': title\n",
    "    }\n",
    "    try:\n",
    "        r = requests.get(URL, params=query_params)\n",
    "        r.raise_for_status()\n",
    "        html, r.encoding = r.text, 'gb2312'\n",
    "    except:\n",
    "        logger.error('error backlinks about ' + title)\n",
    "        return list()\n",
    "    text = json.loads(html, encoding='gb2312')\n",
    "    link = list()\n",
    "    try:\n",
    "        link = [obj['title'] for obj in text[\"query\"][\"backlinks\"]]\n",
    "    except:\n",
    "        return link\n",
    "    return link\n",
    "\n",
    "def categories(title = None):\n",
    "    global API_URL\n",
    "    URL = API_URL\n",
    "    query_params = {\n",
    "        'action': 'query',\n",
    "        'prop': 'categories',\n",
    "        'cllimit': 'max',\n",
    "        'clshow': '!hidden',\n",
    "        'format': 'json',\n",
    "        'clcategories': '',\n",
    "        'titles': title\n",
    "    }\n",
    "    try:\n",
    "        r = requests.get(URL, params=query_params)\n",
    "        r.raise_for_status()\n",
    "        html, r.encoding = r.text, 'gb2312'\n",
    "    except:\n",
    "        logger.error('error categories about ' + title)\n",
    "        return list()\n",
    "    text = json.loads(html, encoding='gb2312')\n",
    "    id = list(text[\"query\"][\"pages\"].keys())[0]\n",
    "    category = set()\n",
    "    if id != -1:\n",
    "        try:\n",
    "            category = [obj['title'][9:] for obj in text[\"query\"]['pages'][id][\"categories\"]]\n",
    "        except:\n",
    "            return category\n",
    "    return category\n",
    "\n",
    "def redirects(title=None):\n",
    "    global API_URL\n",
    "    URL = API_URL\n",
    "    query_params = {\n",
    "        'action': 'query',\n",
    "        'prop': 'redirects',\n",
    "        'rdlimit': 'max',\n",
    "        'format': 'json',\n",
    "        'titles': title\n",
    "    }\n",
    "    try:\n",
    "        r = requests.get(URL, params=query_params)\n",
    "        r.raise_for_status()\n",
    "        html, r.encoding = r.text, 'gb2312'\n",
    "    except:\n",
    "        logger.error('error redirects about ' + title)\n",
    "        return list()\n",
    "    text = json.loads(html, encoding='gb2312')\n",
    "    id = list(text[\"query\"][\"pages\"].keys())[0]\n",
    "    redirect = list()\n",
    "    if id != -1:\n",
    "        try:\n",
    "            redirect = [obj['title'] for obj in text[\"query\"]['pages'][id][\"redirects\"]]\n",
    "        except:\n",
    "            return redirect\n",
    "    return redirect\n",
    "\n",
    "\n",
    "\n",
    "def contributors(title=None):\n",
    "    global API_URL\n",
    "    URL = API_URL\n",
    "    query_params = {\n",
    "        'action': 'query',\n",
    "        'prop': 'contributors',\n",
    "        'pclimit': 'max',\n",
    "        'format': 'json',\n",
    "        'titles': title\n",
    "    }\n",
    "    try:\n",
    "        r = requests.get(URL, params=query_params)\n",
    "        r.raise_for_status()\n",
    "        html, r.encoding = r.text, 'gb2312'\n",
    "    except:\n",
    "        logger.error('error linkss about ' + title)\n",
    "        return list()\n",
    "    text = json.loads(html, encoding='gb2312')\n",
    "    id = list(text[\"query\"][\"pages\"].keys())[0]\n",
    "    contributors = list()\n",
    "    try:\n",
    "        for obj in text[\"query\"]['pages'][id][\"contributors\"]:\n",
    "            contributors.append(obj['userid'])\n",
    "    except:\n",
    "        return contributors\n",
    "    return contributors\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'extend'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-21737044f7bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mOut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mredirects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'extend'"
     ]
    }
   ],
   "source": [
    "input_search=[\"linear regression\",\"probability distribution\",\"Hypothesis testing\",\"machine learning\"]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    title = \"Linear regression\"\n",
    "    id = pageid(title, np = 4)\n",
    "    summ = summary(title)\n",
    "    Out = links(title)\n",
    "    Out=Out.extend(redirects(title))\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts = linkss(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "details = []\n",
    "for i in range(len(concepts)):\n",
    "    if __name__ == '__main__':\n",
    "        title = concepts[i]\n",
    "        id = pageid(title, np = 4)\n",
    "        summ = summary(title)\n",
    "        details.append(summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(494, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concepts</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>accelerated failure time model</td>\n",
       "      <td>In the statistical area of survival analysis, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>actuarial science</td>\n",
       "      <td>Actuarial science is the discipline that appli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>adaptive clinical trial</td>\n",
       "      <td>An adaptive clinical trial is a clinical trial...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>adrien-marie legendre</td>\n",
       "      <td>Adrien-Marie Legendre (; French: [adʁiɛ̃ maʁi ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>affine transformation</td>\n",
       "      <td>In Euclidean geometry, an affine transformatio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         concepts  \\\n",
       "0  accelerated failure time model   \n",
       "1               actuarial science   \n",
       "2         adaptive clinical trial   \n",
       "3           adrien-marie legendre   \n",
       "4           affine transformation   \n",
       "\n",
       "                                             summary  \n",
       "0  In the statistical area of survival analysis, ...  \n",
       "1  Actuarial science is the discipline that appli...  \n",
       "2  An adaptive clinical trial is a clinical trial...  \n",
       "3  Adrien-Marie Legendre (; French: [adʁiɛ̃ maʁi ...  \n",
       "4  In Euclidean geometry, an affine transformatio...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concepts=(inner.lower() for inner in concepts)\n",
    "data = {'concepts':concepts, 'summary':details}\n",
    "df = pd.DataFrame(data)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_search=[\"linear regression\",\"probability distribution\",\"Hypothesis testing\",\"machine learning\"]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    title = \"Linear regression\"\n",
    "    id = pageid(title, np = 4)\n",
    "    summ = summary(title)\n",
    "    Out = links(title)\n",
    "    print(summ)\n",
    "    print(Out)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "Related_concept_list=[]\n",
    "for item in Out:\n",
    "    #if __name__==\"main\":\n",
    "        title=item\n",
    "        temp=linkss(item)\n",
    "        Related_concept_list.append(temp)\n",
    "\n",
    "        \n",
    "Related_concept=[]        \n",
    "for i in range(len(Related_concept_list)):\n",
    "    for item in Related_concept_list[i]:\n",
    "        Related_concept.append(item)\n",
    "\n",
    "details_related = []\n",
    "for i in range(len(Related_concept)):\n",
    "    if __name__ == '__main__':\n",
    "        title =Related_concept[i]\n",
    "        id = pageid(title, np = 4)\n",
    "        summ = summary(title)\n",
    "        details_related.append(summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_data = {'concepts':Related_concept, 'summary':details_related}\n",
    "lr_df = pd.DataFrame(lr_data)\n",
    "train_list=[]\n",
    "\n",
    "\n",
    "for i in range(len(lr_df['summary'])) :\n",
    "    li = list(lr_df[\"summary\"][i].split())\n",
    "    train_list.append(li)\n",
    "       \n",
    "for i in range(len(df['summary'])) :\n",
    "    li = list(df[\"summary\"][i].split())\n",
    "    train_list.append(li)\n",
    "\n",
    "for i in range(len(train_list)):\n",
    "    for j in range(len(train_list[i])):\n",
    "        train_list[i][j].lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10689, 100)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concept_model=Word2Vec(window=5,sg=1,min_count=5,negative=10)\n",
    "concept_model.build_vocab(train_list)\n",
    "concept_model.train(train_list,total_examples=concept_model.corpus_count,epochs=concept_model.epochs)\n",
    "Concept_vec=concept_model[concept_model.wv.vocab]\n",
    "Concept_vec.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('combination', 0.6613548398017883),\n",
       " ('flexible', 0.6464951038360596),\n",
       " ('(GLM)', 0.6326786875724792),\n",
       " ('F-test.', 0.6263085603713989),\n",
       " ('regression.', 0.6063989400863647),\n",
       " ('variable.In', 0.6028098464012146),\n",
       " ('(whether', 0.6000233888626099),\n",
       " ('projecting', 0.5972094535827637),\n",
       " ('regression,', 0.5927785038948059),\n",
       " ('monotonic', 0.5909476280212402)]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concept_model.similar_by_vector(\"linear\",topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('(GLM)', 0.6125328540802002),\n",
       " ('Negative', 0.6070998311042786),\n",
       " ('(least', 0.6042513251304626),\n",
       " ('residuals.', 0.6030663251876831),\n",
       " ('flexible', 0.6028972864151001),\n",
       " ('(total', 0.5986204743385315),\n",
       " ('(linear', 0.596278190612793),\n",
       " ('semiparametric', 0.5931200981140137),\n",
       " ('regression)', 0.5923926830291748),\n",
       " ('squares)', 0.5867124795913696)]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concept_model.similar_by_vector(\"regression\",topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('null', 0.6969625353813171),\n",
       " ('one-tailed', 0.6471133828163147),\n",
       " ('\"maximum', 0.6280801296234131),\n",
       " ('eigenvalue\"', 0.6259239912033081),\n",
       " ('F-test', 0.6187523603439331),\n",
       " ('true)', 0.6100216507911682),\n",
       " ('true.', 0.6055892705917358),\n",
       " ('θ0—having', 0.5979104042053223),\n",
       " ('functional)', 0.5970379710197449),\n",
       " ('(alternatively,', 0.5959560871124268),\n",
       " ('testing,', 0.5935155153274536),\n",
       " ('quantile.', 0.5916645526885986),\n",
       " ('two-tailed', 0.5911465287208557),\n",
       " ('r.', 0.5895360708236694),\n",
       " ('noncentrality', 0.5884029865264893),\n",
       " ('testable', 0.588397741317749),\n",
       " ('model.Suppose', 0.5875678658485413),\n",
       " ('estimable', 0.5868659019470215),\n",
       " ('well-established', 0.5868650674819946),\n",
       " ('unlikely', 0.5853275656700134)]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concept_model.most_similar(positive=(\"parameter\",\"hypothesis\"),topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
